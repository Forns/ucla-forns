
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

  <head>
    <title>Andrew Forney - UCLA CS</title>
    <link href="../../../css/bootstrap.min.css" rel="stylesheet" type="text/css">
    <link href="../../../css/magic-bootstrap.css" rel="stylesheet" type="text/css">
    <link href="../../../css/main.css" rel="stylesheet" type="text/css">
    <script src="../../../js/lib/jquery-2.0.3.min.js"></script>
    <script src="../../../js/lib/bootstrap.min.js"></script>
    <script src="../../../js/lib/expanding.js"></script>
    <script src="../../../js/display/general/general-display.js"></script>
    <script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js"></script>
  </head>
  
  <body data-spy="scroll" data-target="#scrollspy">
    
    <!-- BEGIN WRAP -->
    <div id="wrap">
      
      <!-- BEGIN NAVIGATION -->
      <nav class='navbar navbar-default' role='navigation'>
        <div class='nav-accent'></div>
        <div class='container'>
          <div class='row'>
            <div class='col-md-12'>
              <div class='navbar-header'>
                <button class='navbar-toggle' type='button' data-toggle='collapse' data-target='.navbar-main-collapse'>
                  <span class='sr-only'>Toggle Navigation</span>
                  <span class='icon-bar'></span>
                  <span class='icon-bar'></span>
                  <span class='icon-bar'></span>
                </button>
                <a class='navbar-brand' href='/~forns/'>
                  <span id='brand-text'>
                    Andrew Forney
                  </span>
                </a>
              </div>
              
              <div id='nav-main' class='collapse navbar-collapse navbar-main-collapse'>
                <ul class='nav navbar-nav navbar-right'>
                  
                  <li>
                    <a href='/~forns/about.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-user'></span>
                      </div>
                      <p class='text-center'>About</p>
                    </a>
                  </li>
                  
                  <li class='active'>
                    <a href='/~forns/classes.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-book'></span>
                      </div>
                      <p class='text-center'>Classes</p>
                    </a>
                  </li>
                  
                  <li>
                    <a href='/~forns/contact.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-comment'></span>
                      </div>
                      <p class='text-center'>Contact</p>
                    </a>
                  </li>
                  
                  <li>
                    <a href='/~forns/publications.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-file'></span>
                      </div>
                      <p class='text-center'>Publications</p>
                    </a>
                  </li>
                  
                </ul>
              </div>
            </div>
          </div>
        </div>
      </nav>
      <!-- END NAVIGATION -->
      
      <!-- BEGIN MAIN CONTENT -->
      <div id="main-content" class="container">
        <div class="row">
          
          <!-- BEGIN SCROLLSPY -->
          <div class='col-md-2 hidden-sm hidden-xs'>
            <div class="bs-sidebar hidden-print affix" role="complementary">
              <ul id='scrollspy' class="nav bs-sidenav">
              </ul>
            </div>
          </div>
          <!-- END SCROLLSPY -->
          
          <!-- BEGIN PRESENTATION CONTENT -->
          <div class='col-md-10 presentation-content' role='main'>
            
            <ol class="breadcrumb hidden-print">
              <li><a href="../../../classes.html">Classes</a></li>
              <li><a href="./cs-161.html">CS161</a></li>
              <li class="active">Week 2</li>
            </ol>
            
            <div id='states' class='scrollspy-element' scrollspy-title='States &amp; Search'></div>
            <h1>Intro to States &amp; Search</h1>
            <div>
              <p>We perform search in our daily lives all the time...</p>
              <p>We have <strong>problems</strong> that have a certain issue and resolution, and generally some <strong>strategies</strong> to use <strong>actions</strong> to resolve those problems.</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/waldo.jpg' />
              </div>
              <br/>
              <p>The familar Where's Waldo is a classic visual search paradigm where we start in some <strong>state</strong> of not knowing where Waldo is, and perform serial search to try to find Waldo.</p>
              <p class='definition'><strong>Search</strong> is exactly what it sounds like: the process of moving from a starting state of a problem to a goal state of that problem.</p>
              <p class='definition'><strong>States</strong> are a given instantiation of a search problem's variables of interest (possibly an incomplete instantiation).</p>
              <br/>
              <p>Search is therefore problem dependent, such that we need some constraints upon the domain of what we're searching for.</p>
              <p>
                In the case of Where's Waldo, it might be difficult to quantify some notion of states (perhaps we start in the &quot;haven't found Waldo&quot; state and want to move to the 
                &quot;found Waldo&quot; one)
              </p>
              <p class='definition'>A <strong>search domain</strong> is the set of all possible instantiation of variables of interest, i.e. all possible states, for a given problem.</p>
              <br/>
              <p>If we didn't have a search domain, then the concept of search is intractable for both humans and computers.</p>
              <p>
                For Where's Waldo, the search domain is simply a given picture that we know has Waldo somewhere inside of it; we don't go looking in a dictionary for Waldo (the dictionary is not in the 
                search domain!), unless we're really bad at the game...
              </p>
              <p>So, to perform search, we have a couple of rules about states:</p>
              <p class='definition'>An <strong>initial state</strong> is the state that you begin your search operation within.</p>
              <p class='definition'>A <strong>goal state</strong> is a state that satisfies your search criteria, i.e., that completes your search from an initial to a goal state.</p>
              <p class='definition'><strong>Actions</strong> are the legal manipulations of a current state that either (1) return us to the same state, or (2) transition to a new state.</p>
              <br/>
              <p>Actions can also have costs associated with them, e.g., the action of driving through a 10 mile road might have a higher cost than moving through a 1 mile road.</p>
              <p class='definition'>Therefore, the <strong>score</strong> of a particular goal state might be a sum of the costs encountered on the path of actions from the initial to goal state.</p>
              <p>So, in summary, search is the process of moving from some initial state to some goal state through successive applications of legal actions.</p>
            </div>
            <hr/>
            
            
            <br/>
            <div id='uninformedSearch' class='scrollspy-element' scrollspy-title='Uninformed Search'></div>
            <h1>Classical Search</h1>
            <div>
              <p>&quot;I found Beethoven!&quot; you exclaimed excitedly.</p>
              <p>Not that kind of classical search!</p>
              <p class='definition'><strong>Classical search</strong> is the systematic, goal / test oriented search of moving from an initial state to a goal state.</p>
              <br/>
              <h3>Motivating Example</h3>
              <p>Let's talk about the problem of maze pathfinding for our motivating example!</p>
              <p>Here's the problem we're going to discuss:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-0.PNG' />
              </div>
              <br/>
              <p>Our search goal will be to determine if a path exists in the grid from the player's position to the goal state.</p>
              <p class='question' name='search-q0'>What will a state look like in this problem?</p>
              <p class='answer' name='search-q0'>Possibly just the position of the player stored as an (X, Y) tuple.</p>
              <br/>
              <p class='question' name='search-q1'>What will the initial state look like in our example of this problem?</p>
              <p class='answer' name='search-q1'>The initial player position (1, 1)</p>
              <br/>
              <p class='question' name='search-q2'>What will the goal state look like in our example of this problem?</p>
              <p class='answer' name='search-q2'>The goal position (1, 3)</p>
              <br/>
              <p class='question' name='search-q3'>What will actions look like in this problem?</p>
              <p class='answer' name='search-q3'>Any movement from a given state's player position to an adjacent, open or goal tile. This gives us transitions of:<br/>
                <code class='prettyprint'>Up: (0, +1), Down: (0, -1), Right: (+1, 0), Left: (-1, 0)</code>
              </p>
              
              <br/>
              <h3>Building a Search Tree</h3>
              <p>So now that we have our concept of states and transitions in the maze pathfinding example, how do we go about searching for the answer?</p>
              <p>In the <strong>classical search</strong> methodology, we want a systematic way of exploring states so that we can find an answer; to do this, we build a search tree:</p>
              <p class='definition'>
                A <strong>search tree</strong> is a tree with the initial state at the root, transitions along the edges, and nodes corresponding to states that can be reached from one action
                applied from the parent.
              </p>
              <br/>
              <p>So, here is a *non-systematic* (search strategy omitted) search path that corresponds to a successfully discovered goal from our previous problem:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-1.PNG' />
              </div>
              <br/>
              <p>I say it's non-systematic because we didn't really apply any search-strategy; I just kinda picked a path that I saw was available!</p>
              <p class='definition'>A <strong>search strategy</strong> is a systematic means of exploring a search tree, and decides which state we will expand next in the course of our search.</p>
              <br/>
              <p>In this conception, whenever we expand a node, we generate all of the possible moves from that node and then use our search strategy to choose which of the generated nodes to expand next.</p>
              <p class='definition'><strong>Expanding</strong> a node is akin to visiting it on a search traversal, and generating all plausible child nodes.</p>
              <p class='definition'>We <strong>generate</strong> nodes by first expanding a parent node, and then generating all child nodes that could be reached from the parent by taking a legal action.</p>
              <br/>
              <p>Here is a search tree where we expand the root, generating its children (assuming each movement is legal), and then expand the child node corresponding to a taken &quot;right&quot; action:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-2.PNG' />
              </div>
              <br/>
              <p class='question' name='search-q4'>Will this be a valid expansion / generation structure for our maze example at the start of this section?</p>
              <p class='answer' name='search-q4'>Well, no; we generated some children that were not valid nodes because the movements that got us there were invalid!</p>
              <br/>
              <p>So, to rectify this, we would clean the tree up as follows, generating only children who were the product of legal moves:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-3.PNG' />
              </div>
              <br/>
              <p class='definition'>The set of all leaf nodes available for expansion at any given point of a search is called the <strong>frontier (AKA open-list)</strong></p>
              <br/>
              <p>Search strategies give us systematic means of exploring the frontier.</p>
              <p>So, if we wanted some &quot;black-box&quot; (don't care how it's accomplished but know what it's accomplishing) pseudocode for the general idea of search, we might say:</p>
<pre class='prettyprint'>
  ; ...for some expand-node function defined
  (defun BB-SEARCH (frontier)
      (cond
          ; Base case: ran out of nodes, return nil
          ; (failed to find goal state)
          ((null frontier) nil)
          ; Otherwise, more nodes on frontier, so keep
          ; expanding them
          (t (expand-node frontier))
      )
  )
  
  ; ...for some goal-state check and
  ; choose-node functions defined
  (defun EXPAND-NODE (frontier)
      (let* ((next (choose-node frontier)))
          (cond
              ; Base case: found goal; done!
              ((goal-state next) next)
              ; Otherwise, expand and continue
              (t BB-SEARCH (append (remove next frontier))
                                   (successors next)
              )
          )
      )
  )
  ; Pseudo-code credit to Evan Lloyd
</pre>
              
              <br/>
              <h3>Search Strategies</h3>
              <p>We'll start by going over two of the most commonly known search strategies that are known as uninformed searches.</p>
              <p class='definition'>An <strong>uninformed search</strong> is a search strategy that *only* knows how to expand and generate nodes, and detect a goal state.</p>
              <p class='definition'>Uninformed search strategies are therefore distinguished by the <strong>order</strong> in which nodes are expanded along the frontier.</p>
              <br/>
              <p>We'll begin by looking at breadth-first search.</p>
              <p class='toolkit'><strong>Breadth-first search</strong> expands nodes level-by-level, always expanding the *shallowest* node on the fronteir 
                (i.e., always expanding nodes at depth D before expanding *any* nodes at depth (D + 1).
              </p>
              <br/>
              <p>So, take this partial search tree for example, and determine its breadth-first search expansion order:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-4.PNG' />
              </div>
              <br/>
              <p class='question' name='search-q5'>What is the breadth-first expansion order for this search tree (assume an expansion order of Right, Down, Left, Up preference per level)?</p>
              <p class='answer' name='search-q5'>(1, 1), (2, 1), (3, 1), (1, 1), (2, 2), (2, 1), (3, 2), (2, 1), (3, 2), (2, 1), (2, 3)</p>
              <br/>
              <p>We'll need some tools for evaluating a particular search strategy, and we can start by defining some common characteristics of a search tree:</p>
              <p class='definition'>The <strong>branching factor (b)</strong> of a search strategy is the maximum number of children a node can generate when expanded.</p>
              <p class='definition'>The <strong>depth (d)</strong> of a search strategy is the number of nodes expanded from the root to the shallowest goal node.</p>
              <br/>
              <p>So if we expand a few more nodes on our frontier from the previous breadth-first search and then reach a goal (as seen below):</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-5.PNG' />
              </div>
              <br/>
              <p class='question' name='search-q6'>What are the values for b and d in the above tree?</p>
              <p class='answer' name='search-q6'>b = 4 because no node could possibly have more than 4 children<br/>d = 4 because it took 4 expansions from the root to reach the nearest goal state</p>
              <br/>
              <p>Using the notions of branching factor and depth, we can assess four metrics for search strategy performance:</p>
              <p class='definition'><strong>Time complexity</strong> judges how long our search will take to find a solution based on some assymptotic analysis of a problem's search space (often in terms
                of b and d).
              </p>
              <br/>
              <p>In other words, we can think of how many nodes need to be expanded before we find a solution.</p>
              <p>Let's consider the following abstract representation of a complete search tree:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-6.PNG' />
              </div>
              <br/>
              <p>What is the branching factor (b) of this tree (assume it is complete)?</p>
              <p>How many nodes have to be expanded at depth 0?</p>
              <p>How many nodes have to be expanded at depth 1?</p>
              <p>How many nodes have to be expanded at depth 2?</p>
              <p>How many nodes have to be expanded at depth d?</p>
              <p class='question' name='search-q7'>What is the time complexity for breadth first search in big-O notation?</p>
              <p class='answer' name='search-q7'>O(b^0 + b^1 + b^2 + b^3 + ... + b^d) = O(b^d) because b^d is the highest degree in the polynomial</p>
              <br/>
              <p class='definition'><strong>Space complexity</strong> is a measure of how many states we need to keep in memory at any given time for our search strategy.</p>
              <br/>
              <p>So, if we look at our abstract breadth-first search representation again, are we allowed to delete any states / branches from memory while we're exploring the frontier?</p>
              <p>How many nodes have to be kept in memory at depth 0?</p>
              <p>How many nodes have to be kept in memory at depth 1?</p>
              <p>How many nodes have to be kept in memory at depth 2?</p>
              <p>How many nodes have to be kept in memory at depth d?</p>
              <p class='question' name='search-q8'>What is the space complexity for breadth first search in big-O notation?</p>
              <p class='answer' name='search-q8'>O(b + b^2 + b^3 + ... + b^d) = O(b^d) because b^d is the highest degree in the polynomial (it dominates the others)</p>
              <br/>
              <p class='definition'>
                <strong>Completeness</strong> is a property of a search strategy that asks: &quot;If a solution exists in the search space, is this search strategy guaranteed to find it?&quot;
              </p>
              <br/>
              <p>Now consider breadth-first search and how it expands level-by-level...</p>
              <p class='question' name='search-q9'>Is breadth-first search complete?</p>
              <p class='answer' name='search-q9'>Yes! If a solution exists in the search space, then expanding level by level is sure to find it.</p>
              <br/>
              <p class='example'>Prove that breadth-first search is complete.</p>
              <br/>
              <p class='definition'><strong>Optimality</strong> is a measure for a search that asks if it will always find the *optimal* solution to a problem (i.e., the solution with the least cost).</p>
              <p>Let's look at this in a bit...</p>
              
              <br/>
              <p class='definition'><strong>Depth-first search</strong> is a search strategy that expands the deepest node in the current frontier of a search tree.</p>
              <br/>
              <p>So, take this partial search tree for example, and determine its depth-first search expansion order:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-4.PNG' />
              </div>
              <br/>
              <p class='question' name='search-q10'>What is the depth-first expansion order for this search tree (assume an expansion order of Right, Down, Left, Up preference per level)?</p>
              <p class='answer' name='search-q10'>(1, 1), (2, 1), (3, 1), (2, 1), (3, 2), (1, 1), (2, 1), (2, 2), (3, 2), (2, 1), (2, 3)</p>
              <br/>
              <p>Let's return to our maze pathfinding example...</p>
              <p class='debug'>What will happen with standard depth-first search in our partial search tree, pictured below?</p>
              <br/>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-7.PNG' />
              </div>
              <br/>
              <p>...errr, that's kind of an issue... and rather useless in most cases!</p>
              <p>We could keep a list of our repeated states and then remember not to repeat them, but that would grow quickly for large search problems...</p>
              <p class='question' name='search-q11'>What constaint could we place on our depth-first search such that it won't have the infinite expansion?</p>
              <p class='answer' name='search-q11'>Limit the depth that it could go to some level, say, m</p>
              <br/>
              <p>Using this strategy, consider our abstract limited-depth-first search tree representation again:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-8.PNG' />
              </div>
              <br/>
              <p>What is the value of m in this tree?</p>
              <p>What are the number of nodes expanded at depth 0 (worst case)?</p>
              <p>What are the number of nodes expanded at depth 1 (worst case)?</p>
              <p>What are the number of nodes expanded at depth 2 (worst case)?</p>
              <p>What are the number of nodes expanded at depth m (worst case)?</p>
              <p class='question' name='search-q12'>What is the time complexity for limited-depth-first search in terms of b and m?</p>
              <p class='answer' name='search-q12'>O(1 + b + b^2 + ... + b^m) = O(b^m)</p>
              <br/>
              <p>Now, let's examine the space complexity...</p>
              <p>With breadth-first search, we needed to keep our frontier in memory because we were expanding level-by-level... is the same true for depth-first search?</p>
              <p class='example'>What happens when we perform expansion #6 in our tree below?</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-9.PNG' />
              </div>
              <br/>
              <p>Ah, so as soon as we've hit our depth limit m (a leaf) we needn't remember the branch that got us there!</p>
              <p>So, let's think about an upper-bound for the number of nodes we'd need to keep in memory for any depth-first expansion:</p>
              <p class='question' name='search-q13'>What is the space complexity for limited-depth-first search in terms of b and m?</p>
              <p class='answer' name='search-q13'>E.g., with b = 3, O(1 + 3 + 6 + ... + b*m) = O(b*m), the general solution</p>
              <br/>
              <p>So we actually save space by using depth-first search (compared to breadth-first)!</p>
              <br/>
              <p class='debug'>What happens when we don't know the depth (m) to limit our search to?</p>
              <p>What if we chose to limit our search to depth 3 and a goal was at depth 4?</p>
              <p class='question' name='search-q14'>Are either depth-first or limited-depth-first search complete?</p>
              <p class='answer' name='search-q14'>No! If we always knew the value of m, then yes, but that's not true in general / reality.</p>
              <br/>
              <p>Is there a variant of depth-first search to make it complete?</p>
              <p>As it turns out, there is! It was even discovered by our own Dr. Richard Korf!</p>
              <p class='definition'><strong>Iterative-deepening depth-first search</strong> is simply repeated application limited-depth-first search where m = 0, then 1, then 2, etc. etc.</p>
              <br/>
              <p>Pictorially, then, we can think of iterative-deepening depth-first search as looking like the following, with the current depth-limit (l) starting at 1 and incrementing each time:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-10.PNG' />
              </div>
              <br/>
              <p class='question' name='search-q15'>If L is the current maximum depth of an IDDFS iteration, then what are its time and space complexities?</p>
              <p class='answer' name='search-q15'>Time complexity: O(b^L)<br/>Space complexity: O(b*L)<br/>(for same reasons as DFS)</p>
              <br/>
              <p>As it turns out, there isn't a whole lot of overhead for using IDDFS over breadth-first, though there is a bit.</p>
              <p>Notice that the two strategies are assymptotically equivalent in time complexity!</p>
              <p class='question' name='search-q16'>Why would you bother ever using IDDFS over BFS?</p>
              <p class='answer' name='search-q16'>The space complexity of IDDFS is still less than BFS, so if space is a concern, then maybe IDDFS is right for you!</p>
              <br/>
              <p>So now that we see how IDDFS works, we can answer the following question:</p>
              <p class='question' name='search-q17'>Is IDDFS complete?</p>
              <p class='answer' name='search-q17'>Yes! Why? Consider it in comparison to how BFS operates... see the similarity?</p>
              
              <br/>
              <h3>Cost &amp; Optimality</h3>
              <p>Did you know that there's a difference between the words &quot;optimal&quot; and &quot;optimum?&quot;</p>
              <p class='definition'>An <strong>optimal</strong> goal state is one with the lowest cost, that might not be the only best solution.</p>
              <br/>
              <p>For example, observe below that our two goal states have the same distance from the initial state, so their cost is the same; they are both optimal.</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-11.PNG' />
              </div>
              <br/>
              <p>Contrast this with the following situation where we have 3 goal states, but only one of them can be reached with the lowest cost.</p>
              <p class='definition'>An <strong>optimum</strong> goal state is *the* one with the lowest cost; it may have no peer goal states with an equal or lower cost.</p>
              <br/>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-12.PNG' />
              </div>
              <br/>
              <p>Let's consider a maze where we're interested in reaching an optimum goal; we'll consider the cost incurred during movement as 1 unit per move.</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-15.PNG' />
              </div>
              <br/>
              <p>So, this example allows us to ask a few questions about search strategy optimality (is a search strategy guaranteed to return the optimal / optimum solution):</p>
              <p class='question' name='search-q18'>Is DFS optimal? Is BFS? IDDFS?</p>
              <p class='answer' name='search-q18'>DFS is NOT optimal! Observe the goal state it finds in the example above. BFS and IDDFS are, however.</p>
              <br/>
              <p>That said, this whole time we've considered each of our movements in our maze pathfinder to have an equal cost, and so total cost is simply the sum of the number of movements we make.</p>
              <p>But what if we modified our costs by introducing a new element into our mazes?</p>
              <p>Let's consider clear tiles to have a travel cost of 1, and tiles with 'M' (mud) to have a movement cost of 3.</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-13.PNG' />
              </div>
              <br/>
              <p>The search tree for this scenario will look like the following, with costs along the edges:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-14.PNG' />
              </div>
              <br/>
              <p>A couple of observations to make here:</p>
              <ul class='indent-1'>
                <li><p>Is the optimal / optimum solution always just the most shallow in the search tree? When is or isn't it?</p></li>
                <li><p>Will DFS find the optimum solution? Will BFS? IDDFS?</p></li>
              </ul>
              <br/>
              <p>Hmm, so it looks like our uninformed searches aren't sufficiently empowered for optimality when we don't have uniform cost.</p>
              <p>Let's look at some ways to empower our searches!</p>
            </div>
            <hr/>
            
            
            <br/>
            <div id='informedSearch' class='scrollspy-element' scrollspy-title='Informed Search'></div>
            <h1>Informed / Heuristic Search</h1>
            <div>
              <p>
                As we saw with our uninformed search strategies, we don't know about any descendants of a particular state (states reachable through some number of legal movements) without first expanding
                the states in between.
              </p>
              <p>But, just because we don't <em>know</em> what states might be along a certain path, doesn't mean we can't make educated <em>guesses</em>.</p>
              <p class='definition'>A <strong>heuristic</strong> is essentially an estimate of the cost that we might encounter expanding a search tree along a certain path.</p>
              <br/>
              <p>The goal of heuristic search is to look at every node along the frontier, and then expand the one that it thinks will lead to an optimal goal.</p>
              <p>
                To do this, we'll construct an evaluation function, <em>f(n)</em> that takes as input a node in our search tree (a state) and computes an estimated cost of reaching a goal state
                <em>IF we decide to expand that node and follow a path from it.</em>
              </p>
              <p class='definition'><strong>Greedy / Best-first</strong> search is a heuristic search strategy that, instead of having a fixed exploration order, will choose an evaluation function that 
                *only* attempts to minimize cost to a goal.
              </p>
              <br/>
              <p>In this way, we can think of greedy search as ONLY using a heuristic, h(n), where h(n) = the estimated cost of a goal state along the path of node n.</p>
              <p>In other words, for greedy search, our evaluation function f(n) = h(n)</p>
              <p class='question' name='heuristic-q0'>What would be a good heuristic choice for our maze pathfinding example?</p>
              <p class='answer' name='heuristic-q0'>Let's start by considering the <strong>Manhattan distance</strong> between a node and the closest goal.</p>
              <br/>
              <p>So, we might say:</p>
<pre class='prettyprint'>
  Mazefinding Manhattan Distance Heuristic:
  f(n) = h(n)
       = argmin( abs(nX - iGoalX) + abs(nY - iGoalY) )
            i
            
  (where n is the player position in a node in the search tree,
  and i represents one of each goal tile in the maze) 
</pre>
              <br/>
              <p>So, seeing that in action:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-16.PNG' />
              </div>
              <br/>
              <p>Notice, using best-first search, now we found not only the optimum solution, but also didn't bother expanding the left branch, and saved a lot of searching!</p>
              <p>But now, let's throw in our Mud tiles and see how it does:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-17.PNG' />
              </div>
              <br/>
              <p>Why didn't greedy work for us there?</p>
              <p>It didn't remember its history! It stepped on a mud tile (+3 cost) but was only interested in Manhattan distance.</p>
              <p>So how do we remember our choices as well?</p>
              
              <br/>
              <h3>A* Search</h3>
              <p>Greedy may have minimized our heuristic function but it didn't succeed with minimizing the *total* cost.</p>
              <p class='definition'><strong>A* search</strong> defines its evaluation function as a sum of a heuristic function and a &quot;history&quot; function, g(n).<br/>f(n) = g(n) + h(n)</p>
              <br/>
              <p>The history function g(n) keeps a total of the *actual* cost we've encountered along our path so far, and then adds it to our heuristic cost for what's to come.</p>
              <p>This means that we now choose to expand the node on the frontier with the smallest cost that we've already encountered, and also the smallest cost of what's yet to come!</p>
              <p>So, if we maintain our Manhattan distance heuristic from before, we see that A* now successfully navigates our maze into the optimum goal.</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-18.PNG' />
              </div>
              <br/>
              <p class='question' name='heuristic-q1'>In what order will A* expand nodes in the above example (assume a left-to-right child preference for equal cost evaluations)?</p>
              <p class='answer' name='heuristic-q1'>(1, 1), (2, 1), (1, 2), (2, 2), (3, 2)</p>
              <br/>
              <p>Nice! And we managed to cut off a bunch of branches from our search tree!</p>
              <p class='question' name='heuristic-q2'>Is A* complete? Is it optimal?</p>
              <p class='answer' name='heuristic-q2'>Yes, and yes! As long as our heuristic function meets some constraints... which we should talk about now!</p>
              
              <br/>
              <h3>Heuristic Design</h3>
              <p>As we've seen, a heuristic function h(n) returns a problem-specific value that depends only on the input state, n.</p>
              <p>However, in order to be a viable heuristic, it needs to adhere to one central property:</p>
              <p class='definition'><strong>Admissibility</strong> is a heuristic property that states that h(n) will never *overestimate* the cost of reaching a goal from node n.</p>
              <br/>
              <p>If a heuristic is not admissible (for any case), then all of the nice assurances of optimality are lost for A*.</p>
              <p class='question' name='heuristic-q3'>Why would an inadmissible heuristic compromise optimality?</p>
              <p class='answer' name='heuristic-q3'>We might choose a non-optimal path because we believed another one to have a cost higher than it would have actually taken to reach a goal!</p>
              <br/>
              <p class='example'>Which of the following are admissible heuristics for our maze pathfinder?</p>
<pre class='prettyprint'>
  h1(n) = Manhattan distance away from the start
  
  h2(n) = (total # of goal tiles) 
        - (# of goal tiles in the same row as n)
        - (# of goal tiles in the same column as n)
        
  h3(n) = Manhattan distance of n from closest goal
  
  h4(n) = # of Mud tiles surrounding n
</pre>
              <br/>
              <p>From the examples above we see that not all heuristics were created equal!</p>
              <p>We can compare a heuristic (and therefore rank various ones) to the hypothetical *perfect* heuristic, h*(n), that will always return the exact cost of reaching a goal state from node n.</p>
              <p>Through this comparison, we can compute the number of bad expansions our heuristics make in their attempt to guess the best course.</p>
              <p class='question' name='heuristic-q4'>Is it better for a heuristic to make very conservative cost estimates (smaller) or ones that are as large as possible while still being admissible?</p>
              <p class='answer' name='heuristic-q4'>Bigger is better! (see why below)</p>
              <br/>
              <p>Say we have three heuristics, h1, h2, and h3, and we're comparing the costs that they associate with various nodes:</p>
<pre class='prettyprint'>
         n1  n2  n3  n4  n5  n6
  h1(n)   1   2   2   3   2   3
  h2(n)   4   5   5   6   5   6
  h3(n)   4   5   6   7   5   6
</pre>
              <br/>
              <p>Are h1 and h2 going to have different expansion orders? Will one *not* expand anything that the other does?</p>
              <p>What about h3? Will it expand anything that the others don't?</p>
              <br/>
              <p>This gives us a simple illustration that:</p>
              <p class='definition'>
                If h1(n) >= h2(n) for all n, then:<br/>
                - Using h2, we expand at least as many (but usually more) nodes than had we used h1<br/>
                - If we have some set of heuristics, {h1, h2, ..., hN}, then the heuristic h_max(n) = max(h1(n), h2(n), ..., hN(n)) is better than any of the heuristics individually!
              </p>
              <br/>
              <p>So how much work does a good heuristic save us?</p>
              <p class='definition'>
                The <strong>effective branching factor (b*)</strong> is simply an approximation of the branching factor that a uniform tree created by an A* solution would have needed to find a solution at depth
                d.
              </p>
              <p>More formally, b* has the mathematical definition (no closed form) of:</p>
<pre class='prettyprint'>
  Let N be the number of nodes generated by an A* problem solution
  Let d be the depth at which a solution was found to said problem
  Then, b* is computed from the quantity:
  
  N + 1 = 1 + b* + (b*)^2 + ... + (b*)^d
  
  So, for example, if A* expanded 52 nodes and found a solution at
  depth d = 5, then:
  N + 1 = 1 + b* + (b*)^2 + ... + (b*)^d
  52 + 1 = 1 + b* + (b*)^2 + (b*)^3 + (b*)^4 + (b*)^5
                    |
                    | (maths)
                    v
  b* = 1.92
</pre>
              <br/>
              <p>A heuristic that's doing a decent job will have an effective branching factor b* close to 1.</p>
              
              <br/>
              <h3>Summary</h3>
              <p>Search strategy wrap-up:</p>
              <ul class='indent-1'>
                <li><p>b = branching factor</p></li>
                <li><p>d = depth of solution</p></li>
                <li><p>m = max depth for DFS variant</p></li>
                <li><p>L = depth limit for limitted DFS</p></li>
              </ul>
              <table class='table table-bordered table-striped'>
                <thead>
                  <tr>
                    <th><p>Strategy</p></th>
                    <th><p>Complete?</p></th>
                    <th><p>Optimal?</p></th>
                    <th><p>Time Complexity</p></th>
                    <th><p>Space Complexity</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th><p>Breadth-first</p></th>
                    <td><p>Yes</p></td>
                    <td><p>Yes (for uniform cost)</p></td>
                    <td><p>Exponential:<br/>O(b^d)</p></td>
                    <td><p>Exponential:<br/>O(b^d)</p></td>
                  </tr>
                  <tr>
                    <th><p>Depth-(limited)-first</p></th>
                    <td><p>No</p></td>
                    <td><p>No</p></td>
                    <td><p>Exponential:<br/>O(b^m)</p></td>
                    <td><p>Linear:<br/>O(bm)</p></td>
                  </tr>
                  <tr>
                    <th><p>Iterative-Deepening-Depth-first</p></th>
                    <td><p>Yes</p></td>
                    <td><p>Yes (for uniform cost)</p></td>
                    <td><p>Exponential:<br/>O(b^L)</p></td>
                    <td><p>Linear:<br/>O(bL)</p></td>
                  </tr>
                  <tr>
                    <th><p>A*</p></th>
                    <td><p>Yes</p></td>
                    <td><p>Yes</p></td>
                    <td><p>Exponential (worst case), Linear (best case):<br/>Depends on heuristic</p></td>
                    <td><p>Exponential (worst case), Linear (best case):<br/>Depends on heuristic</p></td>
                  </tr>
                </tbody>
              </table>
            </div>
            <hr/>
            
            
            <br/>
            <div id='localSearch' class='scrollspy-element' scrollspy-title='Local Search'></div>
            <h1>Local Search</h1>
            <div>
              <p>Sometimes, our search space is just too massive to tractably search exhaustively using classical search methods.</p>
              <p>So, we can try something akin to intelligent dart-throwing that just kinda gives up on definitely finding an answer.</p>
              <p>Like a lazy mechanic, local search says, &quot;I'll try a few things, but no guarantees.&quot;</p>
              <p class='definition'>
                <strong>Local search</strong> is a search strategy that attempts to guess a correct answer from a random initial state and then movements that attempt to find a goal from that start.
              </p>
              <br/>
              <p>With Local Search, you need to have a few definitions for your problem:</p>
              <ul class='indent-1'>
                <li><p>A <strong>neighborhood</strong> of a state is a set of all states that can be reached from performing one action on the current state.</p></li>
                <li><p>A <strong>state score</strong> is a function that evaluates how close a given state is to a goal state.</p></li>
              </ul>
              <br/>
              <p>With these two elements defined, we'll try to move around from neighbor to neighbor until we find the state with the best score (if we don't need to be perfect) or meeting some threshold.</p>
              <p>Local search, dependent upon the particular implementation, usually has the following general steps:</p>
              <ol class='indent-1'>
                <li><p>Try a random state (variables of interest randomly assigned values inside the search space)</p></li>
                <li><p>Is it the goal state (or does it meet some threshold)? If yes, done!</p></li>
                <li><p>Otherwise, try to massage that random starting position into the goal state.</p></li>
                <li><p>Repeat step 3 some N number of times (don't want to keep going forever), and if you still haven't found a goal, we'll try some other things (discussed later)</p></li>
              </ol>
              <br/>
              <p class='example'>The N-Queens (typically, the 8-Queens) problem asks for an instantiation of some N queens on a chessboard such that no queen is in position to capture another.</p>
              <br/>
              <p>For those unfamiliar with chess, queens can move in any straight-line direction for as many spaces as they want (horizontally, vertically, or diagonally).</p>
              <p>Here's a solution to the 5-queens problem:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-19.PNG' />
              </div>
              <br/>
              <p>These problems aren't so bad for low N, but what about the 100-Queens problem? Ugh...</p>
              <p>So, why don't we try using this example for local search?</p>
              <p>Let's make one improvement at the start and place the restriction that 1 column (or row) has to hold a single queen (or else we're immediately in trouble!)</p>
              <p class='question' name='local-q0'>For our N-Queens local search, what will a state, neighborhood, and score look like?</p>
              <p class='answer' name='local-q0'>
                <strong>State:</strong> instantiation of N queens on an N x N chessboard<br/>
                <strong>Neighborhood:</strong> a set of all states reachable from the current one by moving the ith queen to some other position in column i<br/>
                <strong>Score:</strong> the number of queens that can possibly capture one another (lower is better)
              </p>
              <br/>
              <p>So with those definitions in mind, we could go column by column and determine the number of queens that would be capturable by moving any queen within a given column to some other position.</p>
              <p>
                For example if, in my 5-Queens problem, I got the following random assignment of queens to column positions, then I could view the scores of each neighbor state by determining how many
                queens would be capturable if I decided to move one into a given slot.
              </p>
              <p>Then, we just try to minimize the cost until we (hopefully) find a solution!</p>
              <p class='definition'>This sort of cost optimization local search is called <strong>hill-climbing</strong>, which attempts to find peak solutions.</p>
              <p class='definition'><strong>Peaks</strong> are a term in local search meaning that a solution with cost C has been found but for which all neighbors have some cost greater than C.</p>
              <br/>
              <p>So, if we found a peak, then we might not be able to do better in that particular local exploration, and we don't want to get stuck!</p>
              <p class='question' name='local-q2'>What are some strategies to avoid getting stuck with a bad random initial guess?</p>
              <p class='answer' name='local-q2'>Some of the most common strategies include:<br/><br/>
                <strong>Random Restart:</strong> after some N bad iterations, just call a mulligan and take a new initial state<br/><br/>
                <strong>Sideways Moves:</strong> allow some movements to neighbors that do not strictly have a lesser cost but that might also have an equal cost<br/><br/>
                <strong>Simulated Annealing:</strong> allow local search to move to a bad neighbor (lol) with some percentage of the time that increases the more you fail with your current search<br/><br/>
                <strong>Beam search:</strong> run multiple searches in parallel, exploring the top k new steps from all current points considered simultaneously<br/>
              </p>
              <br/>
              <p>Some of these improvements may seem to be minor but can have a huge impact.</p>
              <p>For example, allowing sideways movements in the 8-queens problem can change success rate from a lousy 14% to a respectable 94%!</p>
              <br/>
              <p class='debug'>Warning: Local search is not appropriate for all problems. Ask your doctor if local search is right for you.</p>
              <p class='question' name='local-q1'>For example, would local search be appropriate for our maze pathfinder?</p>
              <p class='answer' name='local-q1'>No! Local search is not used when we care about the &quot;path&quot; to a solution, since the initial state is random.</p><br/>
              <p class='question' name='local-q3'>Is local search complete? Optimal?</p>
              <p class='answer' name='local-q3'>Not by a long shot... we may not even find a solution if one exists, and if we do, it might just be some lousy one that we stumbled upon!</p>
            </div>
            <hr/>
            
            
            <br/>
            <div id='homework1' class='scrollspy-element' scrollspy-title='Homework 1'></div>
            <h1>Homework 1 Questions</h1>
            <div>
              <p>If you have any HW1 questions, now's the time to ask them!</p>
              <p>Just to reiterate if you haven't looked closely at the homework yet, let's see how to picture frames from a tree perspective:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/hw1-tree.PNG' />
              </div>
              <br/>
              <h3>Tips &amp; Warnings</h3>
              <ul class='indent-1'>
                <li><p>On more complicated functions like UNGAP, you may have helper functions that call the main function, which call the helper function, etc. until a terminating
                  condition is met in both.</p></li>
                <li><p>Beware of bound gaps along the path on PATHC and functions like it!</p></li>
                <li><p>Beware of adding a duplicate slot on the insert functions!</p></li>
              </ul>
            </div>
            <hr/>
            
            
            <a class='btn btn-default pull-right hidden-print' href='javascript:window.print();'>
              <span class='glyphicon glyphicon-print'></span>
              &nbsp; PDF / Print
            </a>
            
          </div>
          <!-- END PRESENTATION CONTENT -->
          
          <!-- MATERIALS FROM CLASS: -->
          
            
        </div>
      </div>
      <!-- END MAIN CONTENT -->
      
      
    </div>
    <!-- END WRAPPER -->
    
    <!-- BEGIN FOOTER -->
    <div id="footer">
      <div class="container">
        <div class="col-md-12 text-center">
          
        </div>
      </div>
    </div>
    <!-- END FOOTER -->
    
  </body>
</html>
