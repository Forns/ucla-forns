
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

  <head>
    <title>Andrew Forney - UCLA CS</title>
    <link href="../../../css/bootstrap.min.css" rel="stylesheet" type="text/css">
    <link href="../../../css/magic-bootstrap.css" rel="stylesheet" type="text/css">
    <link href="../../../css/main.css" rel="stylesheet" type="text/css">
    <script src="../../../js/lib/jquery-2.0.3.min.js"></script>
    <script src="../../../js/lib/bootstrap.min.js"></script>
    <script src="../../../js/lib/expanding.js"></script>
    <script src="../../../js/display/general/general-display.js"></script>
    <script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js"></script>
    <script type="text/javascript" src="../../../js/display/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  </head>
  
  <body data-spy="scroll" data-target="#scrollspy">
    
    <!-- BEGIN WRAP -->
    <div id="wrap">
      
      <!-- BEGIN NAVIGATION -->
      <nav class='navbar navbar-default' role='navigation'>
        <div class='nav-accent'></div>
        <div class='container'>
          <div class='row'>
            <div class='col-md-12'>
              <div class='navbar-header'>
                <button class='navbar-toggle' type='button' data-toggle='collapse' data-target='.navbar-main-collapse'>
                  <span class='sr-only'>Toggle Navigation</span>
                  <span class='icon-bar'></span>
                  <span class='icon-bar'></span>
                  <span class='icon-bar'></span>
                </button>
                <a class='navbar-brand' href='/~forns/'>
                  <span id='brand-text'>
                    Andrew Forney
                  </span>
                </a>
              </div>
              
              <div id='nav-main' class='collapse navbar-collapse navbar-main-collapse'>
                <ul class='nav navbar-nav navbar-right'>
                  
                  <li>
                    <a href='/~forns/about.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-user'></span>
                      </div>
                      <p class='text-center'>About</p>
                    </a>
                  </li>
                  
                  <li class='active'>
                    <a href='/~forns/classes.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-book'></span>
                      </div>
                      <p class='text-center'>Classes</p>
                    </a>
                  </li>
                  
                  <li>
                    <a href='/~forns/contact.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-comment'></span>
                      </div>
                      <p class='text-center'>Contact</p>
                    </a>
                  </li>
                  
                  <li>
                    <a href='/~forns/publications.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-file'></span>
                      </div>
                      <p class='text-center'>Publications</p>
                    </a>
                  </li>
                  
                </ul>
              </div>
            </div>
          </div>
        </div>
      </nav>
      <!-- END NAVIGATION -->
      
      <!-- MathJax CUSTOM DEFS -->
      <div class='hidden'>
        \(\def\independence{\perp\!\!\!\perp}\)
        \(\def\dependence{\perp\!\!\!\!\!/\!\!\!\!\!\perp}\)
      </div>
      
      <!-- BEGIN MAIN CONTENT -->
      <div id="main-content" class="container">
        <div class="row">
          
          <!-- BEGIN SCROLLSPY -->
          <div class='col-md-2 hidden-sm hidden-xs'>
            <div class="bs-sidebar hidden-print affix" role="complementary">
              <ul id='scrollspy' class="nav bs-sidenav">
              </ul>
            </div>
          </div>
          <!-- END SCROLLSPY -->
          
          <!-- BEGIN PRESENTATION CONTENT -->
          <div class='col-md-10 presentation-content' role='main'>
            
            <ol class="breadcrumb hidden-print">
              <li><a href="../../../classes.html">Classes</a></li>
              <li><a href="./cs-161.html">CS161</a></li>
              <li class="active">Week 9</li>
            </ol>
            
            
            <div id='decision' class='scrollspy-element' scrollspy-title='Decision Theory'></div>
            <h1>Decision Theory</h1>
            <div>
              <p>We all take actions to accomplish our goals, and it's our *preferences* that have set those goals to begin with.</p>
              <p>There are a variety of stepping stones that we take on the journey towards our goals, some that are more instrumental to accomplishing our task than others.</p>
              <p class='definition'><strong>Decision theory</strong> is the art of designing rational agents that act based on their utility-based preferences and their perception of the world.</p>
              <br/>
              <p>In the first-order logic we've seen previously, the results of our actions were definitive.</p>
              <p class='toolkit'>Recall that a <strong>state</strong> in first-order logic planning is composed of a set of set of conjoined fluents.</p>
              <p class='toolkit'>The <strong>deterministic outcome</strong> of a given action <code>a</code> from state <code>s0</code> is <code>s1</code>, denoted:<br/>
                $$Result(s0, a) = s1$$
              </p>
<pre class='prettyprint'>
  ; In first order logic planning:
  Action(Eat(Cake)
      Preconditions: Have(Cake)
      Effect: &not;Have(Cake) &and; Eaten(Cake))
  
  ; ...where states:
  s0 = Have(Cake)
  s1 = [&not;Have(Cake) &and;] Eaten(Cake)
  ; s1 need not have &not;Have(Cake) under the
  ; closed world assumption
  
  ; ...and so:
  Result(s0, Eat(Cake)) = s1
  
  ; i.e., the result of being in state s0
  ; and eating your cake is that you *definitely*
  ; arrive in state s1
</pre>
              <br/>
              <p>However, as opposed to some strong assumptions with planning that we made in past chapters, the consequences of our actions might not always be known.</p>
              <p>In this case, where we're dealing with incomplete knowledge or stochastic outcomes, we have to take probabilities into account.</p>
              <br/>
              
              <p class='example'>Forney Industries has been developing a Self-Replicating Cake, such that while eating it, it actually has a chance to make a copy of itself! (OK, you can make the examples if you
                don't like this one)</p>
              <p>This means that eating a cake does *not* necessarily imply: &not;Have(Cake)</p>
              <br/>
              <p class='toolkit'>The <strong>stochastic outcome</strong> of action <code>a</code> from state <code>s0</code> to state <code>s1</code> under evidence <code>e</code> is given by:<br/>
                $$\sum_{s' \in s0} Pr(Result(s0, a) = s1 | a) * Pr(s0 = s' | e)$$
                In other words, sum over all the states of the probability of being in the initial state given the evidence \(Pr(s0 = s' | e)\) times the probability of reaching state <code>s1</code>
                with action <code>a</code>.
              </p>
              <p class='toolkit'>Often, the probability of being in the initial state, <code>s0</code>, is implicit or unimportant for our reasoning, and so we'll sum out <code>s0</code> and write the above as:<br/>
                $$Pr(Result(a) = s1 | a, e)$$
              </p>
<pre class='prettyprint'>
  ; So now, instead of:
  Result(s0, a) = s1
  
  ; ...we have:
  Pr(Result(a) = s1 | a, e)
</pre>
              <br/>
              <p>So now we have some notion of a distribution over possible states that we can reach from taking an action under the current evidence, we need to know *which* action we should take!</p>
              <p>In other words, we need to find an action that has the highest probability of landing us in the most favorable state.</p>
              <p>We do this by defining our agent's Utility function.</p>
              <br/>
              
              <p class='definition'>A <strong>Utility Function</strong> U(s) assigns a single number to score the desirability of a state (the higher the number, the more desirable).</p>
<pre class='prettyprint'>
  ; Cake! Yay!
  U( {Have(Cake)} ) = 100
  
  ; Oh why... why did I eat that
  ; whole cake? ;_;
  U( {Eaten(Cake)} ) = -50
</pre>
              <br/>
              
              <p class='definition'>The <strong>Expected Utility (EU)</strong> of action <code>a</code> given evidence <code>e</code> is simply the average utility value of the possible outcomes weighted 
                by the probability that the outcome occurs, or formally:<br/>
                $$EU(a | e) = \sum_s Pr(Result(a) = s | a, e) * U(s)$$
                In other words, the expected utility leverages the chance of being in a given state times the desirability of that state.
              </p>
              <br/>
              <p>Once we know the expected utility of taking some action, if we have multiple actions to consider, we simply take the one with the highest expected utility!</p>
              <p>Wasn't it The Rolling Stones who said, &quot;You can't always get what you want, but if you try sometimes you might find... maximizing your expected utility to be sufficient?&quot;</p>
              <br/>
              
              <p class='definition'>The <strong>maximum expected utility (MEU)</strong> criterion simply stipulates that an agent will choose an action <code>a</code> (amongst all those that are possible
                <code>A</code>) that has the highest expected utility amongst its choices, written:<br/>
                $$\text{ActionChoice} = \underset{a \in A}{\operatorname{argmax}} EU(a | e)$$
              </p>
              <br/>
              <p>In other words, for all action choices, choose the one that has the highest expected utility given the evidence.</p>
              <br/>
              
              <p class='example'>In the following examples, determine which action will be taken based on the maximum expected utility principle.</p>
              <blockquote>
                <strong>Problem 1:</strong> Andrew is deciding if he should use his day off to go to the beach, or stay inside and watch reruns of Golden Girls...
                Going to the beach would be a lot of fun and staying in would just be OK, but forecasts gave a 60% chance of rain for the day, which would spoil a trip to the beach.
              </blockquote>
              <table class='table table-striped table-bordered'>
                <thead>
                  <tr>
                    <th><p>Action</p></th>
                    <th><p>Weather</p></th>
                    <th><p>U(Action, Weather)</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><p>home</p></td>
                    <td><p>clear</p></td>
                    <td><p>2</p></td>
                  </tr>
                  <tr>
                    <td><p>home</p></td>
                    <td><p>raining</p></td>
                    <td><p>3</p></td>
                  </tr>
                  <tr>
                    <td><p>beach</p></td>
                    <td><p>clear</p></td>
                    <td><p>4</p></td>
                  </tr>
                  <tr>
                    <td><p>beach</p></td>
                    <td><p>raining</p></td>
                    <td><p>1</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p class='question' name='dec-q0'>Compute the expected utilities of each action given by:<br/>
                $$EU(a | e) = \sum_s Pr(Result(a) = s | a, e) * U(s)$$
                ...and determine where Andrew should spend his day off. (Click for solution)
              </p>
              <div class='answer white-bg' name='dec-q0'>
              <div>
                <p>First, determine the expected utility of each action:</p>
                <p>
                  \begin{eqnarray}
                    EU(home) &=& Pr(home, clear | home) * U(home, clear)\\ 
                             &+& Pr(home, raining | home) * U(home, raining)\\
                             &=& 0.4 * 2 + 0.6 * 3\\
                             &=& 2.6
                  \end{eqnarray}
                </p>
                <p>
                  \begin{eqnarray}
                    EU(beach) &=& Pr(beach, clear | beach) * U(beach, clear)\\
                              &+& Pr(beach, raining | beach) * U(beach, raining)\\
                              &=& 0.4 * 4 + 0.6 * 1\\
                              &=& 2.2
                  \end{eqnarray}
                </p>
                <p>Next, using MEU criteria:</p>
                <p>
                  \begin{eqnarray}
                    \text{ActionChoice} &=& \underset{a \in A}{\operatorname{argmax}} EU(a | e)\\
                                        &=& \operatorname{argmax} ({home: 2.6, beach: 2.2})\\
                                        &=& home
                  \end{eqnarray}
                </p>
                <p>Golden Girls it is :(</p>
              </div>
              </div>
              <br/>
              <p class='example'>Recompute the previous example given that we observed <code>Weather = clear</code> outside.</p>
              <p class='question' name='dec-q1'>Click for solution.</p>
              <div class='answer white-bg' name='dec-q1'>
                <div>
                  <p>Our evidence: <code>e = {Weather = clear}</code></p>
                  <p>First, determine the expected utility of each action with the evidence:</p>
                  <p>
                    \begin{eqnarray}
                      EU(home | e) &=& Pr(home, clear | home, e) * U(home, clear)\\
                                   &+& Pr(home, raining | home, e) * U(home, raining)\\
                                   &=& 1 * 2 + 0 * 3\\
                                   &=& 2
                    \end{eqnarray}
                  </p>
                    
                  <p>
                    \begin{eqnarray}
                      EU(beach | e) &=& Pr(beach, clear | beach, e) * U(beach, clear)\\
                                    &+& Pr(beach, raining | beach, e) * U(beach, raining)\\
                                    &=& 1 * 4 + 0 * 1\\
                                    &=& 4
                    \end{eqnarray}
                  </p>
                    
                  <p>Next, using MEU criteria:</p>
                  <p>
                    \begin{eqnarray}
                      \text{ActionChoice} &=& \underset{a \in A}{\operatorname{argmax}} EU(a | e)\\
                                          &=& \operatorname{argmax} ({home: 2, beach: 4})\\
                                          &=& beach
                    \end{eqnarray}
                  </p>
                                
                  <p>Beach time!</p>
                </div>
              </div>
            </div>
            <hr/>
            
            
            <br/>
            <div id='preferences' class='scrollspy-element' scrollspy-title='Preferences'></div>
            <h1>Preferences</h1>
            <div>
              <p>So the next question you might be asking is: where do we get these utility values?</p>
              <p>Do we need to have explicit numerical values mapped to states as indications of their desirability or can these numbers simply be relative and derived?</p>
              <p class='definition'><strong>Preferences</strong> are our agent's rankings of desirable states having considered their relative likelihoods of being reached.</p>
              <br/>
              <p>We can use preference orderings to recover utility functions...</p>
              <p>...but we also have to consider how desirable the state is *in concert with* the probability of reaching it.</p>
              <p>In other words, two actions A1 and A2 might both be capable of reaching a desirable state, but if one of those two actions is more *likely* to reach that state, we will want to choose
                it over the other.</p>
              <p>First, to formalize the possible outcomes of an action, we turn to the notion of a lottery:</p>
              <br/>
              <p class='definition'>A <strong>lottery</strong> is a representation of the possible outcome states from taking some action with the probabilities of reaching each outcome, with the format:
                $$L = [prob_1, outcome_1;  prob_2, outcome_2;  ...;  prob_n, outcome_n]$$
              </p>
              <p class='definition'>An <strong>outcome</strong> is either a primitive state (i.e., a set of fluents) or, recursively, another lottery</p>
              <div class='well'>
                <p>For primitive states such as:</p>
                <p>$$S1 = { Eaten(Cake) }, S2 = ...$$</p>
                <br/>
                
                <p>We might have two lotteries:</p>
                <p>
                  \begin{eqnarray}
                    L1 &=& [0.5, S1; 0.25, S2; 0.25, L2]\\
                    L2 &=& [0.3, S3; 0.6, S1; 0.1, S5]
                  \end{eqnarray}
                </p>
                <br/>
                <p>...indicating that from Lottery L1, we have a 50% chance of transitioning into state S1, etc.</p>
              </div>
              <br/>
              
              <p class='definition'>A lottery is called <strong>complex</strong> if it includes an outcome that is itself another lottery.</p>
              <p>The term &quot;lottery&quot; is intuitive because taking an action associated with a lottery is like buying a Lotto ticket, and hoping that you &quot;win&quot; the state you desired.</p>
              <p>Now that we know how we can model outcomes of taking a particular action, let's talk about preferences.</p>
              <div class='well'>
                <p>For any two lotteries A and B, we can use the following notation to describe preferences:</p>
                <br/>
                <ul class='indent-1'>
                  <li><p>A > B &nbsp;&nbsp;&nbsp; the agent prefers A over B</p></li>
                  <li><p>A ~ B &nbsp;&nbsp;&nbsp; the agent is indifferent between A and B</p></li>
                  <li><p>A &ge; B &nbsp;&nbsp;&nbsp; the agent prefers A over B or is indifferent between them</p></li>
                </ul>
              </div>
              <br/>
              
              <p>The primary goal of utility theory is to determine how preferences between complex lotteries are related to preferences on the primitive states that compose them.</p>
              <p>To do this, we can define some axioms on lotteries that, if violated by our intelligent systems, would lead to irrational behavior.</p>
              <br/>
              <table class='table table-striped table-bordered'>
                <thead>
                  <tr>
                    <th><p>Axiom</p></th>
                    <th><p>Interpretation</p></th>
                    <th><p>Formalism</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th><p>Orderability</p></th>
                    <td><p>An agent cannot avoid deciding between two actions (i.e., either one is preferred or they are equally preferable) 
                      and must assign one of the following relationships to lotteries A and B.</p></td>
                    <td class='col-md-4'><p>(A &gt; B), (B &gt; A), or (A ~ B)</p></td>
                  </tr>
                  <tr>
                    <th><p>Transitivity</p></th>
                    <td><p>If an agent prefers lottery A to lottery B, and also prefers lottery B to lottery C, then it also prefers lottery A to lottery C.</p></td>
                    <td><p>(A &gt; B) &and; (B &gt; C) &rArr; (A &gt; C)</p></td>
                  </tr>
                  <tr>
                    <th><p>Continuity</p></th>
                    <td><p>If A &gt; B &gt; C (i.e., some lottery B is between A and C in preference), then there is some probability p that we could find such that a certain outcome of B would
                      be equally preferrable to an outcome of A with probability p or of C with probability (1 - p)</p></td>
                    <td><p>A &gt; B &gt; C &rArr;<br/> &exist;p [p, A; 1 - p, C] ~ [1, B]</p></td>
                  </tr>
                  <tr>
                    <th><p>Substitutability</p></th>
                    <td><p>If two lotteries are equally preferrable, then you may substitute one in for the other in some other complex lottery.</p></td>
                    <td><p>A ~ B &rArr;<br/>[p, A; 1 - p, C] ~ [p, B; 1 - p, C]</p></td>
                  </tr>
                  <tr>
                    <th><p>Monotonicity</p></th>
                    <td><p>If we prefer outcome A to outcome B, then we must also prefer lotteries that have a higher probability to reach A than B.</p></td>
                    <td><p>A &gt; B &rArr;<br/>(p &gt; q &hArr;<br/>&nbsp;&nbsp;[p, A; 1 - p, B] &gt; [q, A; 1 - q, B]<br/>)</p></td>
                  </tr>
                  <tr>
                    <th><p>Decomposability</p></th>
                    <td><p>We can reduce any number of complex lotteries down to a simpler one simply by the laws of probability.</p></td>
                    <td><p>L1 = [p, A; 1 - p, L2]<br/>L2 = [q, B; 1 - q, C]<br/>L1 ~ [p, A; (1 - p)q, B; (1 - p)(1 - q), C]</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p class='question' name='pref-q0'>Substitutability tells us that two equally preferrable lotteries can be substituted for one another in some other lottery. Does the following also
                hold?<br/>
                $$(A > B) \Rightarrow [p, A; 1 - p, C] > [p, B; 1 - p, C]$$
              </p>
              <p class='answer' name='pref-q0'>Yes! We can think of this as the case of &quot;All other considerations constant,&quot; lotteries containing A will be preferrable to those containing B.</p>
              <br/>
              <p>Alright, so we have lotteries that abide by these axioms... what were we trying to do again?</p>
              <p>Oh yeah... get some notion of what to use for utility functions...</p>
              <p>Well, we have the following two consequences of preferential axioms that can allow us to solve for some (non-unique) utility function:</p>
              <br/>
              <table class='table table-striped table-bordered'>
                <thead>
                  <tr>
                    <th><p>Consequence</p></th>
                    <th><p>Interpretation</p></th>
                    <th><p>Formalism</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th><p>Existence of Utility Function</p></th>
                    <td><p>If an agent's preferences abide by the above axioms, then there exists a utility function such that: <code class='prettyprint'>U(A) &gt; U(B)</code> if and only if
                      A is preferred to B, and <code class='prettyprint'>U(A) = U(B)</code> if and only if the agent is indifferent between A and B.</p></td>
                    <td class='col-md-4'><p>U(A) &gt; U(B) &hArr; A &gt; B<br/>U(A) = U(B) &hArr; A ~ B</p></td>
                  </tr>
                  <tr>
                    <th><p>Expected Utility</p></th>
                    <td><p>The utility of a lottery is the sum of the probability of each outcome times the utility of that outcome.</p></td>
                    <td><p>$$EU([p_1, S_1; ...; p_n, S_n]) =\\\sum_i p_i * U(S_i)$$</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>From these two consequences, we see that, while it *does* matter what numbers we choose for our utility functions (dependent upon the
                scenario and how much more state s0 is desirable compared to state s1), there exists a utility function capable of respecting our preference ordering.</p>
              <br/>
              <p class='example'>Read the following preferences, observe the action lotteries, and then decide which action our intelligent system would choose based on the MEU criterion.</p>
<pre class='prettyprint'>
  ; Our agent has the following preferences,
  ; which abide by the 6 axioms above:
  1. A &gt; B
  2. B ~ C
  3. C &ge; D
  
  ; Utilities:
        A  B  C  D
  U(S)  3  2  2  1
  
  ; Action 1 corresponds to lottery:
  L1 = [0.2, A; 0.3, B; 0.5, L3]
  
  ; Action 2 corresponds to lottery:
  L2 = [0.5, B; 0.3, C; 0.2, L3]
  
  L3 = [0.6, D; 0.4, B]
  
  ; Which action ({1, 2}) should we take?
</pre>
              <p class='question' name='dec-q1'>Click for solution. [Hint: Use decomposition and then compute the expected utility of each lottery]</p>
              <div class='answer white-bg' name='dec-q1'>
                <p>Since Actions 1 and 2 have complex lotteries, we can use decomposition to compress them into simple lotteries:</p>
                <p>
                  \begin{eqnarray}
                    L1 &=& [0.2, A; 0.3, B; 0.5, L3]\\
                       &=& [0.2, A; 0.3, B; 0.5, [0.6, D; 0.4, B]]\\
                       &=& [0.2, A; 0.5, B; 0.3, D]
                  \end{eqnarray}
                </p>
                   
                <p>
                  \begin{eqnarray}
                    L2 &=& [0.5, B; 0.3, C; 0.2, L3]\\
                       &=& [0.5, B; 0.3, C; 0.2, [0.6, D; 0.4, B]]\\
                       &=& [0.58, B; 0.3, C; 0.12, D]
                  \end{eqnarray}
                </p>
                   
                <p>Now compute the utility of each lottery:</p>
                <p>
                  \begin{eqnarray}
                    EU(L1) &=& \sum_i p_i * U(S_i)\\
                          &=& 0.2 * 3 + 0.5 * 2 + 0.3 * 1\\
                          &=& 1.9
                  \end{eqnarray}
                </p>
                      
                <p>
                  \begin{eqnarray}
                    EU(L2) &=& \sum_i p_i * U(S_i)\\
                          &=& 0.58 * 2 + 0.3 * 2 + 0.12 * 1\\
                          &=& 1.88
                  \end{eqnarray}
                </p>
                      
                <p>Whew that was close! But it looks like action 1 is the (slightly) more preferrable</p>
              </div>
              
              <br/>
              <h3>Multi-attribute Utility</h3>
              <p>Previously, we've dealt with states that have had an atomic utility value assigned to them, for example:</p>
              <div class='well'>
                <p>Whole state of being at home and it raining out gets assigned, statically, the utility value of 3</p>
                <p>$$U(home, rainy) = 3$$</p>
                <br/>
                
                <p>BUT, what if I wanted to assess *components* of states and treat them with different utility contributions?</p>
                <p>$$U(home) = x, U(rainy) = y \Rightarrow U(home, rainy) = f(x, y)$$</p>
              </div>
              <br/>
              
              <p class='example'>The following example from the book analyzes variables for planning airport construction.</p>
              <blockquote>
                Siting an airport (i.e., determining where to build one based on analyzed factors) requires a variety of considerations. If we're choosing between some number of land plots on which to build
                our airport, then we can analyze the putative values of some variables of interest based on our possible choices / actions. Let's say we were on the city planning committee of such and determined
                the following variables would comprise our decision criteria:
              </blockquote>
              <ul class='indent-1'>
                <li><p><strong>Cost:</strong> the price of the land required to build upon, plus construction costs, plus the price of legal fees and processing, etc.</p></li>
                <li><p><strong>Noise:</strong> the amount of noise generated by the airport.</p></li>
                <li><p><strong>Deaths:</strong> possible deaths from construction hazards and other airport-related risks (propensities of nearby residents to stand on the runway? IDK...).</p></li>
              </ul>
              <br/>
              
              <p>If we're considering a bunch of construction sites, it might not be feasible to sit down and assign a utility value to each one individually...</p>
              <p>BUT, we might have some data on our variables of interest based on projections from studies and other sources of information, so perhaps we can construct our utility values from these facts.</p>
              <p>That said, we often don't have deterministic information available (e.g., we don't know for certain that construction on site A will incur exactly 3 deaths), so we need to use estimation
                techniques.</p>
              <p>Additionally, we don't always treat all of our variables of interest equally -- some might need to be weighted as more important than others.</p>
              <br/>
              
              <p class='definition'>For some vector of variables \(X = \{X_1, X_2, ..., X_n\}\) and their value functions \((f_1, f_2, ..., f_n)\), where each \(f_i\) corresponds to a variable \(X_i\), the
                utility of a state with variable instantiation: \(x = \{x_1, x_2, ..., x_n\}\) is given by:
                $$U(x_1, x_2, ..., x_n) = F[f_1(x_1), f_2(x_2), ..., f_n(x_n)]$$
                ...where \(F\) is a function that combines the value functions in a meaningful way (usually just the sum).
              </p>
              <p class='definition'>When our metrics of interest exibit <strong>mutual preferential independence</strong>, it means that a change in the value of one variable will not necessarily cause
                a change in the value of another variable.</p>
              <p class='toolkit'>If all of our metrics of interest exibit mutual preferential independence, then our agent's choice boils down to a simple maximimization of:
                $$F[f_1(x_1), f_2(x_2), ..., f_n(x_n)] = \sum_i f_i(x_i)$$
              </p>
              <br/>
              
              <p>Here, F is a simple function like addition that would aggregate all of the individual variable-weighting <strong>value functions</strong> \(f_1, f_2, ..., f_n\) for 
                variable values \(x_1, x_2, ..., x_n\).</p>
              <p>The idea is that we want to condense all of the variable information into a single utility value based on the importance weights of each variable.</p>
              <br/>
              
              <p class='example'>For our airport siting example, let's consider the following two sites and the value functions that provide the proper variable weighting. We'll assume our variables of interest
                exibit mutual preferential independence. Determine, using the mutual preferential independence utility function above with F being simple summation, which is the superior site.</p>
              <br/>
              <table class='table table-bordered table-striped'>
                <thead>
                  <tr>
                    <th><p>Variable</p></th>
                    <th><p>Value Function</p></th>
                    <th><p>Site 1 Value</p></th>
                    <th><p>Site 2 Value</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th><p>Cost</p></th>
                    <td><p>f_cost (x) = -x</p></td>
                    <td><p>$10,000,000</p></td>
                    <td><p>$15,000,000</p></td>
                  </tr>
                  <tr>
                    <th><p>Noise</p></th>
                    <td><p>f_noise (x) = -x * 1000</p></td>
                    <td><p>200dB</p></td>
                    <td><p>180dB</p></td>
                  </tr>
                  <tr>
                    <th><p>Deaths</p></th>
                    <td><p>f_deaths (x) = -x * 10^9</p></td>
                    <td><p>2</p></td>
                    <td><p>1</p></td>
                  </tr>
                </tbody>
              </table>
              <p class='question' name='dec-q2'>Click for solution</p>
              <div class='answer white-bg' name='dec-q2'>
                  <p>Using our definition for mutual preferential independence utility, we assess each site</p>
                  <p>
                    \begin{eqnarray}
                      U(Site1) &=& F[f_{cost}($10,000,000), f_{noise}(200dB), f_{deaths}(2)]\\
                               &=& -10,000,000 + (-200 * 1000) + (-2 * 10^9)\\
                               &=& -2,010,200,000
                    \end{eqnarray}
                  </p>
                  
                  <p>
                    \begin{eqnarray}
                      U(Site2) &=& F[f_{cost}($15,000,000), f_{noise}(180dB), f_{deaths}(1)]\\
                               &=& -15,000,000 + (-180 * 1000) + (-1 * 10^9)\\
                               &=& -1,015,180,000
                    \end{eqnarray}
                  </p>
                  
                  <p>Site2 is the superior site based on our utility definitions!</p>
              </div>
              <br/>
              
              <p class='definition'><strong>Strict dominance</strong> is the case where a state is superior on all metrics.</p>
              <p class='question' name='dom-0'>Site2 was the dominant choice in our example above; is it strictly dominant to Site1?</p>
              <p class='answer' name='dom-0'>No, Site2 is not strictly dominant compared to Site1 because Site1 actually has the lower monetary cost.</p>
              <br/>
              <p>It turns out, however, that the utility cost of our estimated number of deaths far outweighs the monetary cost.</p>
              
              <br/>
              <h3>Decision Networks</h3>
              <p>Often, however, our decisions have factors that are not necessarily the effect of anything we can control.</p>
              <p>To model this uncertainty, we can combine Bayesian Networks, which model our knowledge of the way the world works, with the preference model we've been discussing to assess a utility
                value for possible actions under consideration.</p>
              <p class='definition'>A <strong>decision network</strong> unifies the stochastic modelling capacities of a Bayesian network with the action-choice and utility concepts of preference models
                by adding two additional nodes to the Bayesian network model.</p>
              <p class='definition'>Decision network <strong>chance nodes</strong> (ovals) represent random variables, and illustrate uncertainty about the values of some of our variables (same way that
                Bayesian networks handled this: conditional probability tables).</p>
              <p class='definition'>Decision network <strong>decision nodes</strong> (rectangles) represent points where the agent has a choice of actions.</p>
              <p class='definition'>Decision network <strong>utility nodes</strong> (diamonds) represent the agent's utility function for a given instantiation of decisions and inference.</p>
              <br/>
              <p>Here is the book's example for our airport siting problem, with some additional chance nodes added in to represent our knowledge of the world:</p>
              <br/>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-9/decision-0.PNG' />
              </div>
              <br/>
              <p>BUT, that example uses continuous variables, which we're not used to seeing... let's look at a quick example using discrete variables:</p>
              <p>(here our decision node might have some nondeterministic action consequence)</p>
              <br/>
              
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-9/decision-9.PNG' />
              </div>
              <br/>
              <p>So, we can ask what the utility of a given action is by setting the action, which causes the decision node to act like a &quot;given&quot; chance node:</p>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-9/decision-10.PNG' />
              </div>
              <br/>
              <p class='question' name='d-net-q0'>What is the expected utility of A1 as listed above? (click for solution)</p>
              <div class='answer white-bg' name='d-net-q0'>
                <p>
                  \begin{eqnarray}
                    EU(A1) &=& Pr(Y = 0 | X = 0, D = A1) * Pr(X = 0) * U(Y = 0)\\
                           &+& Pr(Y = 1 | X = 0, D = A1) * Pr(X = 0) * U(Y = 1)\\
                           &+& Pr(Y = 0 | X = 1, D = A1) * Pr(X = 1) * U(Y = 0)\\
                           &+& Pr(Y = 1 | X = 1, D = A1) * Pr(X = 1) * U(Y = 1)\\
                               \\
                           &=& 0.2 * 0.4 * 3\\
                           &+& 0.8 * 0.4 * 1\\
                           &+& 0.7 * 0.6 * 3\\
                           &+& 0.3 * 0.6 * 1\\
                               \\
                           &=& 2
                  \end{eqnarray}
                </p>
              </div>
              <br/>
              <p class='example'>Compute the EU(A2) using the above example.</p>
              <br/>
              <p>So here are some observations to make:</p>
              <ul class='indent-1'>
                <li><p><strong>Purpose of decision networks:</strong> judge which actions from our decision (rectangle) nodes produce the highest utility.</p></li>
                <li><p><strong>Data formats:</strong>
                  <ul class='indent-1'>
                    <li><p>Chance nodes are CPTs just like in Bayesian networks, except parents of chance nodes can be decision nodes as well (which are always given because
                      we're evaluating between action choices)</p></li>
                    <li><p>
                      Decision nodes for a given action simply become given chance nodes.
                    </p></li>
                    <li><p>Utility nodes are an evaluation of its direct causes (parents) based on supplied value functions. In our example above, U = F(f_death(Deaths), f_noise(Noise), f_cost(Cost))</p></li>
                  </ul>
                </p></li>
                <li><p><strong>Inference:</strong> based on the choice for our decision nodes, we can use an inference algorithm (like variable elimination from last week) that gives us the utility 
                  node's parents' posterior probabilities to determine not only the weighted utility of a given variable value, but also accounting for the chance that it will occur based on any evidence.</p></li>
              </ul>
              <br/>
              <p>And that's a decision network in a nutshell!</p>
              <br/>
              <p>Of course, all of this relies on us as humans knowing what data and variables to incorporate into our decision networks...</p>
              <p>Our next topic will be the ability to learn how to construct a decision mechanism from raw data alone!</p>
            </div>
            <hr/>
            
            
            <br/>
            <div id='learning' class='scrollspy-element' scrollspy-title='Learning'></div>
            <h1>Learning</h1>
            <div>
              <p>The art of machine learning is little different from how we as humans acquire new information, and has particularly close analogy to how we teach children.</p>
              <p class='definition'><strong>Machine learning</strong> is the process by which programmers equip intelligent systems to modify their behavior based on training, observations, and other
                features available to the program during operation that might not have been available or convenient for the programmer.</p>
              <br/>
              <p>So one of the first questions you might ask is: why give computers this adaptability at all? Shouldn't our programmers have all the tools necessary to empower their agents when they
                sit down to program them in the first place?</p>
              <p>As it turns out, the answer is: not always; there are a variety of cases in which it is not possible for a programmer to explicitly define an agent's behavior, including:</p>
              <ul class='indent-1'>
                <li><p>Prediction engines that assess changes over time cannot account for all possible future changes at the time of programming and must be adaptive.</p></li>
                <li><p>Navigational systems cannot be programmed with all possible obstacles, mazes, and terrain features at the time of programming and must be adaptive.</p></li>
                <li><p>Programmers may not *know* exactly how to define certain behavior, but can instead pose learning problems to their systems to get a gist for the intended behavior.</p></li>
              </ul>
              <br/>
              <p>Learning algorithms generally suit one of a few different flavors:</p>
              <table class='table table-bordered table-striped'>
                <thead>
                  <tr>
                    <th><p>Learning Class</p></th>
                    <th><p>Description</p></th>
                    <th><p>Example</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th><p>Unsupervised Learning</p></th>
                    <td><p>Unsupervised learning is a type of pattern extrapolation engine whose typical task is <strong>clustering</strong>, the act of finding similarities between various input items
                      (like images, text, etc.) and lumping them into some sort of group. In other words, it attempts to find hidden structures in unlabeled data.</p></td>
                    <td><p>Image Classification: given lots of images of faces and lots of images of computers, an unsupervised learning algorithm should be capable of lumping the faces together apart from
                      the computers.</p></td>
                  </tr>
                  <tr>
                    <th><p>Reinforcement Learning</p></th>
                    <td><p>Just like operant conditioning (for humans), reinforcement learning provides an input for our program, on which it will make some sort of decision and be rewarded (if that was
                      the right decision) or punished (if that was the wrong decision), figuratively speaking.</p></td>
                    <td><p>Animat Modeling: simulation study of animat populations where taking some action (like eating poison berries) has a harmful consequence that discourages that action in the
                      future.</p></td>
                  </tr>
                  <tr>
                    <th><p>Supervised Learning</p></th>
                    <td><p>Just like having an instructor or parent correct your mistakes or reward your triumphs, supervised learning has some &quot;oracle&quot; that will tell you the correct answer
                      on some problem during training so that you'll know what to change when you're wrong and what to keep when you're right. Additionally, labeled input data.</p></td>
                    <td><p>Object Recognition: given lots of images of objects with corresponding labels, e.g. a picture of a chair with label &quot;chair&quot;, object recognition will be able to
                      find other chairs in the future based on the label given *and* know that the particular object represents a chair (unlike unsupervised learning, which can only cluster)</p></td>
                  </tr>
                  <tr>
                    <th><p>Semi-supervised Learning</p></th>
                    <td><p>Almost exactly like supervised learning except that the labels or corrections given to us by our, now imperfect, oracle may not be entirely trustworthy. This creates extra
                      noise that sets semi-supervised learning apart from its accurately supervised counterpart.</p></td>
                    <td><p>User Input Classification: determining age based on images of people and their self-reported age (on which they might have lied).</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>So, the first task of machine learning is to choose a model class that best fits the data that we have available.</p>
              <p>What exactly constitutes our definition of &quot;best&quot; depends on the task at hand...</p>
              <p>Two tools we'll discuss for some applications are decision trees and Bayesian networks.</p>
              
              <br/>
              <h3>Classification</h3>
              <p>One of those most common machine learning tasks involves classifying different items evaluated on their variable settings and sorting them into the proper group.</p>
              <p class='definition'><strong>Classification</strong> attempts to learn a <strong>model</strong> from a collection of examples that will predict a classification given the observable attributes.</p>
              <br/>
              <p>Just what model we should use is dependent on the question we're asking, i.e., the classification we're trying to make, and the data that we have available.</p>
              <p>So, let's look at some example classification problems, and the models appropriate for them!</p>
            </div>
            <hr/>
            
            
            <br/>
            <div id='dtree' class='scrollspy-element' scrollspy-title='Decision Trees'></div>
            <h1>Decision Trees</h1>
            <div>
              <p>One type of classification is making a decision:</p>
              <p>Given some input features about a situation, we have to evaluate whether to make one decision or another.</p>
              <p>To model this task, we might consider a decision tree... but first...</p>
              <br/>
              <p>Looking at a simplified version of the book's example:</p>
              <p class='example'>Dining Dilemma</p>
              <blockquote>
                Forney Industries has <span class='strike'>recently contracted with the NSA</span> decided to branch out into the phone app business, and is debuting with its first app that learns your
                dining habits and can predict whether or not you will wait for a seat at a restaurant based on a number of attributes. The app will therefore generate a &quot;yes&quot; or &quot;no&quot;
                conclusion of whether or not you are going to wait based on the following inputs:
              </blockquote>
              <ul class='indent-1'>
                <li><p><strong>Patrons?</strong> A measure of how many diners are currently at the restaurant, can be: {none, some, many}</p></li>
                <li><p><strong>Hungry?</strong> Whether or not you are currently on the brink of starvation (ok, maybe that's too dramatic... you just haven't eaten since lunch), and can be: {yes, no}</p></li>
                <li><p><strong>Type?</strong> The type of food the restaurant serves, which can be: {French, Italian, Thai, or Burgers} (you know... from the country of Burger?)</p></li>
                <li><p><strong>Fri / Sat?</strong> Whether or not it is a peek-serving day of Friday or Saturday, can be: {yes, no}.</p></li>
              </ul>
              <br/>
              <p>Our app has been learning your habits for awhile now and made the following determinations (expressed here just semantically):</p>
              <ul class='indent-1'>
                <li><p>If there are no patrons you don't trust the restaurant quality and so will not wait, but if it's full, you'll only wait based on other attributes:</p></li>
                <li><p>If you're not really hungry, you're not going to wait, otherwise:</p></li>
                <li><p>You love French food and Burgers, so you'll wait at this point if it's either of those... but if it's Thai...</p></li>
                <li><p>You'll only wait if it's not a peek dining day of Friday or Saturday.</p></li>
              </ul>
              <br/>
              <p>So if that's the plain-English description of our habits, how does an intelligent system predict whether or not we'll wait?</p>
              <p class='definition'>A <strong>decision tree (d-tree)</strong> is a tree structure with nodes as attributes and leaves as deterministic classification outcomes.</p>
              <br/>
              <p>The d-tree of our above observations might look like:</p>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-9/decision-2.PNG' />
              </div>
              <br/>
              <p>It's easy to see what a decision tree will classify for our decision; we simply start at the root and trace the edges until we end up at a leaf!</p>
              <p class='example'>What will our above decision tree give us for the following samples?</p>
              <table class='table table-bordered table-striped'>
                <thead>
                  <tr>
                    <th><p>Patrons?</p></th>
                    <th><p>Hungry?</p></th>
                    <th><p>Type?</p></th>
                    <th><p>Cost?</p></th>
                    <th><p>Fri / Sat?</p></th>
                    <th><p>Wait?</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><p>some</p></td>
                    <td><p>yes</p></td>
                    <td><p>burgers</p></td>
                    <td><p>$$</p></td>
                    <td><p>yes</p></td>
                    <td><p>???</p></td>
                  </tr>
                  <tr>
                    <td><p>full</p></td>
                    <td><p>yes</p></td>
                    <td><p>italian</p></td>
                    <td><p>$$$</p></td>
                    <td><p>no</p></td>
                    <td><p>???</p></td>
                  </tr>
                  <tr>
                    <td><p>full</p></td>
                    <td><p>yes</p></td>
                    <td><p>thai</p></td>
                    <td><p>$</p></td>
                    <td><p>yes</p></td>
                    <td><p>???</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>You might notice: there was another attribute included in our sample data (Cost?) that was never used by our d-tree. That's fine, and sometimes preferrable, as we'll soon find out...</p>
              <p>So now that we see how a d-tree works... let's talk about how to learn them in the first place!</p>
              
              <br/>
              <h3>Learning a D-Tree</h3>
              <p>To talk about learning d-trees, we imagine we have some large amount of data on which we can extract patterns to perform the learning aspect.</p>
              <p class='definition'>A <strong>training set</strong> is a list of input / output pairings where, given the input characteristics of a particular sample, we tell our learning system what the
                expected outcome should be (under the assumption that it will be able to formulate a classification function from lots of examples).</p>
              <br/>
              <p>So let's take a look at an arbitrary training set (Example credit to Evan Lloyd; it was too beautiful not to use)</p>
              <p class='example'>Imagine that we have arbitrary attributes A, B, C, and D. We must make a yes / no decision for some classification X. Observe the following training set over each feature
                and the expected decision for X.</p>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-9/decision-3.PNG' />
              </div>
              <br/>
              <p class='question' name='dec-q3'>What form of learning is this? Unsupervised, semi-supervised, or supervised?</p>
              <p class='answer' name='dec-q3'>Supervised since we're telling our system what classification outcome should be expected for X and have labeled data.</p>
              <br/>
              <p>Now, we want to find some decision tree that accurately gives the correct classification based on attributes A, B, C, and D.</p>
              <p>But which one do we choose?</p>
              <p class='toolkit'>A <strong>single-node split</strong> on a training set tries to maximize the classification accuracy based on a single attribute's values.</p>
              <p class='definition'>In classification, a <strong>hit</strong> occurs whenever our split on attributes agrees with the training set, and a <strong>miss</strong> occurs when it disagrees.</p>
              <br/>
              <p>When we split on a value of a variable, e.g., A = red, we look at how many samples got classified to X = yes and X = no in the training set and try to make the classification that is most
                accurate (most hits / fewest misses) based only on A = red.</p>
              <p>So... let's try splitting on A first. This would give us:</p>
              <table class='table table-striped table-bordered'>
                <thead>
                  <tr>
                    <th><p></p></th>
                    <th><p>A = red</p></th>
                    <th><p>A = blu</p></th>
                    <th><p>A = grn</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th><p>X = yes</p></th>
                    <td><p>7</p></td>
                    <td><p>0</p></td>
                    <td><p>8</p></td>
                  </tr>
                  <tr>
                    <th><p>X = no</p></th>
                    <td><p>0</p></td>
                    <td><p>9</p></td>
                    <td><p>6</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p class='question' name='dtree-q0'>Given this break down, what would be the most accurate classification to give the cases where A = red?</p>
              <p class='answer' name='dtree-q0'>X = yes! It would be a perfect fit, in fact, 100% accurate!</p>
              <br/>
              <p class='question' name='dtree-q1'>Given this break down, what would be the most accurate classification to give the cases where A = blu?</p>
              <p class='answer' name='dtree-q1'>X = no! It would be a perfect fit, in fact, 100% accurate!</p>
              <br/>
              <p class='question' name='dtree-q2'>Given this break down, what would be the most accurate classification to give the cases where A = grn?</p>
              <p class='answer' name='dtree-q2'>X = yes! It would not be a clean fit however...</p>
              <br/>
              <p>This would give us a single node d-tree looking like:</p>
              <br/>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-9/decision-5.PNG' />
              </div>
              <br/>
              <p>So, using this single node split, we would achieve an 80% accuracy since 6 / 30 samples are misclassified by saying that X = yes whenever A = grn</p>
              <br/>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-9/decision-4.PNG' />
              </div>
              <br/>
              <p>Alright, so let's see if doing a single node split on another attribute can do better than 80%.</p>
              <p>Shall we try to split on B?</p>
              <table class='table table-striped table-bordered'>
                <thead>
                  <tr>
                    <th><p></p></th>
                    <th><p>B = sml</p></th>
                    <th><p>B = med</p></th>
                    <th><p>B = lrg</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th><p>X = yes</p></th>
                    <td><p>6</p></td>
                    <td><p>2</p></td>
                    <td><p>7</p></td>
                  </tr>
                  <tr>
                    <th><p>X = no</p></th>
                    <td><p>1</p></td>
                    <td><p>12</p></td>
                    <td><p>2</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>We actually do a bit better with the single var split on B if we choose to again use the plurality rule of classification, giving us only 5 / 30 misclassifications:</p>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-9/decision-6.PNG' />
              </div>
              <br/>
              
              <p>But, as we've seen with our restaurant waiting example, we're not always interested in a single-node split on an attribute, but a multi-node one... why waste all that extra info?!</p>
              <p>Now, of course, we have the question of which node to split on first... it's clear that order matters in deciding the most accurate classification!</p>
              <p class='definition'><strong>Entropy</strong> is a measure of uncertainty of a random variable and is the fundamental unit of information theory. Formally, for variable \(V\), the entropy
                \(H(V)\) is given by:
                $$H(V) = -\sum_{v \in V} Pr(v) * log_2 [Pr(v)]$$
                In other words, the entropy is the sum, for all values of \(V\), of the probability of seeing that value \(v\) times \(log_2\) of that probability.
              </p>
              <br/>
              <p>Some things to note about entropy:</p>
              <ul class='indent-1'>
                <li><p>It's measured in the bits it would require in order to represent all possible equally likely outcomes (manifest in the log_2 in the equation, to follow)</p></li>
                <li><p>A higher entropy means it's closer in value to a uniform distribution, like a coin flip (i.e., the higher the value, the more random)</p></li>
                <li><p>The lower the entropy, the more certain the outcome is.</p></li>
              </ul>
              <br/>
              <p class='question' name='ent-q0'>Find the entropy of a fair coin flip with \(V = \{heads, tails\}\). Click for solution.</p>
              <div class='answer white-bg' name='ent-q0'>
                <p>
                  \begin{eqnarray}
                    H(V) &=& - \sum_{v \in V} Pr(v) * log_2 [Pr(v)]\\
                         &=& - [Pr(heads) * log_2[Pr(heads)] + Pr(tails) * log_2[Pr(tails)]]\\
                         &=& - [0.5 * log_2(0.5) + 0.5 * log_2(0.5)]\\
                         &=& 1
                  \end{eqnarray}
                </p>
              </div>
              <br/>
              <p>So, we see that a coin flip has 1 bit of entropy, since it only takes 1 bit (i.e., a 0 or 1 outcome uniformly distributed) to model the uncertainty.</p>
              <p>Our goal in building decision trees, however, is to MINIMIZE uncertainty.</p>
              <p class='definition'>In the classification problem, <strong>Information gain</strong> is the expected reduction in entropy from splitting our decision at a choice point on some attribute.
                Therefore, to maximize the information gain, we may equivalently minimize the entropy (uncertainty).</p>
              <p class='definition'>We can model this in terms of <strong>expected entropy,</strong> which provides a metric for the entropy reducing contributions of each variable value. Computing
                the expected entropy reduction for splitting on a variable is a two step process described below.</p>
              <div class='well'>
                <p>
                  \begin{eqnarray}
                    \text{Let  } V &=& \text{the variable / feature being analyzed}\\
                                 v &=& \text{a value of V}\\
                                 p &=& \text{the positive classifications (X = YES) for } V = v\\
                                 n &=& \text{the negative classifications (X = NO) for } V = v
                  \end{eqnarray}
                </p>
                <p><strong>Step One:</strong> The entropy for \(V = v\) is given by \(H_v\) and is a function of the positive and negative classifications for that value:</p>
                <p>
                  \begin{eqnarray}
                    H_v(p, n) = &-&\frac{p}{p + n} * log_2 \frac{p}{p + n}\\
                                &-& \frac{n}{p + n} * log_2 \frac{n}{p + n}
                  \end{eqnarray}
                </p>
                <p><strong>Step Two:</strong> The expected entropy for splitting on \(V\) is defined as:</p>
                <p>
                  \begin{eqnarray}
                    EH(V) = \sum_{v \in V} Pr(v) * H_v(p, n)
                  \end{eqnarray}
                </p>
                <p>In other words, for all values v in variable V, sum up their entropies split on positive and negative classifications.</p> 
              </div>
              <br/>
              
              <p class='example'>Compute the expected entropy of splitting on variable A in our example from before (table replicated below for ease):</p>
              <table class='table table-striped table-bordered'>
                <thead>
                  <tr>
                    <th><p>n = 30</p></th>
                    <th><p>A = red</p></th>
                    <th><p>A = blu</p></th>
                    <th><p>A = grn</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th><p>X = yes (p)</p></th>
                    <td><p>7</p></td>
                    <td><p>0</p></td>
                    <td><p>8</p></td>
                  </tr>
                  <tr>
                    <th><p>X = no (n)</p></th>
                    <td><p>0</p></td>
                    <td><p>9</p></td>
                    <td><p>6</p></td>
                  </tr>
                </tbody>
              </table>
              <p class='question' name='ent-s0'><strong>Step Zero - Setup:</strong>
                Compute the probabilities of seeing each variable value from the table and setup what you need to find the entropy. (Click for solution)
              </p>
              <div class='answer white-bg' name='ent-s0'>
                  <p>Expected entropy of A is:</p>
                  <p>
                    \begin{eqnarray}
                      EH(A) &=& \sum_{a \in A} Pr(a) * E_a(p, n)\\
                            &=& Pr(A = red) * H_{red}(p, n)\\
                            &+& Pr(A = blu) * H_{blu}(p, n)\\
                            &+& Pr(A = grn) * H_{grn}(p, n)\\
                    \end{eqnarray}
                  </p>
                  <p>We know from the table that there are 30 samples, so we can find the probabilities:</p>
                  <p>
                    \begin{eqnarray}
                      Pr(A = red) &=& \frac{7}{30}\\
                      Pr(A = blu) &=& \frac{9}{30}\\
                      Pr(A = grn) &=& \frac{14}{30}
                    \end{eqnarray}
                  </p>
              </div>
  
              <p class='question' name='ent-s1'><strong>Step One - Entropies:</strong>
                Compute the entropy of each variable value across the positive and negative trials in our training set. (Click for solution)
              </p>
              <div class='answer white-bg' name='ent-s1'>
                <p>
                  \begin{eqnarray}
                    H_{red}(p, n) &=& H_{red}(7, 0)\\
                                  &=& -\frac{7}{7 + 0} * log_2 \frac{7}{7 + 0}\\
                                  &-&  \frac{0}{7 + 0} * log_2 \frac{0}{7 + 0}\\
                                  &=& 0
                  \end{eqnarray}
                </p>
                  
                <p>
                  \begin{eqnarray}
                    H_{blu}(p, n) &=& H_{blu}(0, 9)\\
                                  &=& -\frac{0}{0 + 9} * log_2 \frac{0}{0 + 9}\\
                                  &-&  \frac{9}{0 + 9} * log_2 \frac{9}{0 + 9}\\
                                  &=& 0
                  \end{eqnarray}
                </p>
                              
                <p>
                  \begin{eqnarray}
                    H_{grn}(p, n) &=& H_{grn}(8, 6)\\
                                  &=& -\frac{8}{8 + 6} * log_2 \frac{8}{8 + 6}\\
                                  &-&  \frac{6}{8 + 6} * log_2 \frac{6}{8 + 6}\\
                                  &\approx& 0.9852
                  \end{eqnarray}
                </p>
              </div>
              
              <p class='question' name='ent-s2'><strong>Step Two - Expected Entropy:</strong> plug in relevant parts from step 1 into our goal from step 0 and solve.</p>
              <div class='answer white-bg' name='ent-s2'>
                <p>
                  \begin{eqnarray}
                      EH(A) &=& Pr(A = red) * H_{red}(p, n)\\
                            &+& Pr(A = blu) * H_{blu}(p, n)\\
                            &+& Pr(A = grn) * H_{grn}(p, n)\\
                            \\
                            &=& (7 / 30) * 0\\
                            &+& (9 / 30) * 0\\
                            &+& (14 / 30) * 0.9852\\
                            \\
                            &\approx& 0.4598 \text{ bits}
                  \end{eqnarray}
                </p>
              </div>
              <br/>
              <p>Whew! What a pain! I hope I never have to do that again...</p>
              <br/>
              <p class='example'>What is the expected entropy for splitting on attribute B in our example? Table replicated below for convenience.</p>
              <table class='table table-striped table-bordered'>
                <thead>
                  <tr>
                    <th><p>n = 30</p></th>
                    <th><p>B = sml</p></th>
                    <th><p>B = med</p></th>
                    <th><p>B = lrg</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th><p>X = yes (p)</p></th>
                    <td><p>6</p></td>
                    <td><p>2</p></td>
                    <td><p>7</p></td>
                  </tr>
                  <tr>
                    <th><p>X = no (n)</p></th>
                    <td><p>1</p></td>
                    <td><p>12</p></td>
                    <td><p>2</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p class='question' name='ent-q1'>Click for brief solution</p>
              <div class='answer' name='ent-q1'>
                <p>
                  \begin{eqnarray}
                    EH(B) &=& \sum_{b \in B} Pr(b) * E_a(p, n)\\ 
                          &=& Pr(B = sml) * H_{sml}(p, n)\\
                          &+& Pr(B = med) * H_{med}(p, n)\\
                          &+& Pr(B = lrg) * H_{lrg}(p, n)\\
                          \\
                          &=& (7 / 30) * H_{sml}(6, 1)\\
                          &+& (14 / 30) * H_{med}(2, 12)\\
                          &+& (9 / 30) * H_{lrg}(7, 2)\\
                          \\
                          &\approx& 0.6434 \text{ bits}
                  \end{eqnarray}
                </p>
              </div>
              <br/>
              <div class='well'>
                <p>Conclusion:</p>
                <p>
                  $$EH(B) = 0.6434 > EH(A) = 0.4598$$
                  $$\text{Accuracy}(B) = 86\% > \text{Accuracy}(A) = 80\%$$
                </p>
                <p>Therefore, we see that splitting on A actually has less uncertainty associated with it than splitting on B, even though A is less accurate by itself.</p>
                <p>(Recall that splitting on A had 6 misses compared to splitting on B which had 5).</p>
              </div>
              <br/>
              
              <p>So how does knowing the expected entropy of A help us? This brings us to the next question:</p>
              <p>Since A is accurate for A = red and A = blu, what if we then split its innacurate component (i.e., A = grn) on B? How would we decide to do that?</p>
              <p class='toolkit'>The <strong>decision tree algorithm</strong> provides means of automating the attribute splitting to minimize expected entropy across multiple variable splits.</p>
              <div class='well'>
                <p><strong>&quot;Naive&quot; Decision Tree Algorithm</strong></p>
                <ol class='indent-1'>
                  <li><p>Pick an attribute A with the minimal expected entropy, and set that as the root</p></li>
                  <li><p>For each <strong>value</strong> of A, if there are attributes remaining on which to split, and classification is not perfect, then recurse on each subtree with A removed.</p></li>
                  <li><p>Otherwise, there were no more attributes on which to split, so simply maximize your accuracy by classifying with a plurality vote (i.e., satisfy the most positives or negatives
                    even though you won't be perfect).</p></li>
                </ol>
              </div>
              <br/>
              <p class='example'>Use the decision tree algorithm on our above example to perform a multi-variable split on the classification task.</p>
              <div class='well'>
                <ol class='indent-1'>
                  <li><p>Choose attribute A for the root since it has the minimal entropy.</p></li>
                  <li><p>We note that for A = red and A = blue, we have perfect accuracy (and therefore need not recurse on those sub-trees), but we are not perfect for A = grn.</p></li>
                  <li><p>Recurse on all of the samples where A = grn and try to find a new attribute to split. In this case, splitting on B gives us a perfect split!</p></li>
                </ol>
              </div>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-9/decision-7.PNG' />
              </div>
              <br/>
              <p>On the 14 remaining cases where A = grn, splitting on B actually gives us a perfect fit! Let's see the decision tree that results:</p>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-9/decision-8.PNG' />
              </div>
              <br/>
              <p>And that's how you do decision trees!</p>
            </div>
            <hr/>
            
            
            <br/>
            <div id='pac' class='scrollspy-element' scrollspy-title='PAC'></div>
            <h1>PAC</h1>
            <div>
              <p>Sometimes, answers are not so clean-cut, and a descent down a decision tree is insufficient for classification.</p>
              <p>So, we'll return to our old friend the Bayesian network in order to help with classification; let's look at our motivating example:</p>
              <br/>
              <p class='example'>GreeterBot 4000</p>
              <blockquote>
                Forney Industries is developing its most ambitious robotic project yet: GreeterBot 4000. GreeterBot 4000 needs to be able to walk around department stores greeting and answering
                questions of the customers. You've been tasked with programming GreeterBot 4000's <span class='strike'>KillAllHumans</span> IdentifyHuman protocol, which must determine if some object
                in its environment is a human or not.<br/><br/>The main issue is that there are a variety of mannequins throughout the store that, although resembling humans, should not be greeted.<br/><br/>
                That said, GreeterBot 4000 is already equipped with input sensors that can assess the following attributes:
              </blockquote>
              <br/>
              <ul class='indent-1'>
                <li><p><strong>Movement (M):</strong> we can assess whether an object is moving at, say, 3 discrete velocities: {not, slow, fast}</p></li>
                <li><p><strong>Height (H):</strong> we can assess the height of an object at, say, 3 discrete heights relative to humans: {sml, med, lrg}</p></li>
                <li><p><strong>Speaking (S):</strong> we have audio sensors capable of determining if some object is speaking a language: {0, 1}</p></li>
                <li><p><strong>Form (F):</strong> we have some scoring algorithm that returns a score from 0 - 5 based on whether an object has human parts like a head, torso, arms, etc.</p></li>
              </ul>
              <br/>
              
              <p>The goal, then, is to determine for each object we encounter assessed with the above 4 attributes, whether or not that object is a human.</p>
              <p>(ignoring, of course, that this is just a re-labelling of the previous example with different semantics...)</p>
              <br/>
              
              <p class='debug'><strong>Complications:</strong> our classification might not be so simple as splitting on a single attribute, some confounding cases might be:</p>
              <ul class='indent-1'>
                <li><p>A loudspeaker might announce the latest deals and be perceived as &quot;speaking&quot; even though it's not a human.</p></li>
                <li><p>A mannequin might have a good human form, and the right height, even though it's not human.</p></li>
                <li><p>A true human might be any height, have good human form (if our visual sensors aren't occluded), and simply not be speaking at a given moment.</p></li>
              </ul>
              <br/>
              <p>So, needless to say, there are a variety of different cases that need to be handled, but different attributes confer some amount of human-ness.</p>
              <p>Because of the amount of noise implicit in this problem, however, we may not be able to get away with clean classification divisions like with decision trees.</p>
              <p class='definition'><strong>Probably Approximately Correct (PAC)</strong> classification strategies are those that, given a sufficiently large training set, can make a decision based on the
                most likely outcome of any witnessed evidence.</p>
              <br/>
              
              <p>Frankly, I think the term &quot;Probably Approximately Correct&quot; sounds like a rationalizing scientist who isn't quite sure of his invention...</p>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-9/profFrink.png' />
              </div>
              <br/>
              
              <p>PAC strategies are good when we don't need a completely accurate answer where all cases are considered, but we have a good gist for when to say &quot;Yes&quot; and when to say &quot;No&quot;</p>
              <p class='definition'>The <strong>Naive Bayes Classifier</strong> is a PAC classification structure where we consider some class C to be the cause of witnessing some number of identifiers, or
                effects, \(F_i\). It is called &quot;Naive&quot; because given the class, we assume that all of the indicators are conditionally independent.</p>
              <br/>
              
              <p>The Naive Bayes Classifier is a Bayesian network where we have a single cause (the class) being the reason for certain indications of that class.</p>
              <p>For our example, the Naive Bayes structure might look like this:</p>
              <br/>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-9/decision-1.PNG' />
              </div>
              <br/>
              
              <p>This classifier says, &quot;If you are a human, then you'll probably exibit some movement, a certain height distribution, the propensity for speech, and some amount of human form.&quot;</p>
              <p>Furthermore, if we know you're human, then the indicators are all conditionally independent of one another because knowing that a human can speak tells us nothing more about a human's ability
                to move.</p>
              <p>Being a Bayesian network, these indicating propensities are all implicit within the network's conditional probability tables.</p>
              <p class='toolkit'>Naive Bayes classification then simply asks: having observed some object with indicators \(F_1, F_2, ..., F_n\), which classification (i.e., value of the class \(C = c\))
                is most likely?</p>
              <br/>
              <p>Let's look at the probability statements of interest:</p>
              <div class='well'>
                <p>We are interested in determining which classification in some set of class variable (C) values is most likely, having witnessed some object with indicators \(F = f\)</p>
                <p>So, we are interested in the table:</p>
                <p>$$Pr(C | f_1, f_2, ..., f_n)$$</p>
                <br/>
                
                <p>...to see which value of C is most probable, we would then find the value for C for which:</p>
                <p>$$\underset{c \in C}{\operatorname{argmax}} Pr(c | f_1, f_2, ..., f_n)$$</p>
              </div>
              <br/>
              <p>So, for our example, let's say we encountered an object exibiting the following:</p>
              <table class='table table-striped table-bordered'>
                <thead>
                  <tr>
                    <th><p>Movement</p></th>
                    <th><p>Height</p></th>
                    <th><p>Speaking</p></th>
                    <th><p>Form</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><p>slow</p></td>
                    <td><p>med</p></td>
                    <td><p>1</p></td>
                    <td><p>4</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>Here we have some object that is moving slowly, of moderate height, that's speaking, and closely resembles the human form.</p>
              <p>Chances are good that it's a human!</p>
              <p>Let's derive what we'll need to make that determination.</p>
              <br/>
              
              <div class='well'>
                <p>Note that \(f_1, f_2, ..., f_n\) is our evidence so we can simply use a familiar theorem to find our answer:</p>
                <p>
                  \begin{eqnarray}
                    Pr(C | f_1, f_2, ..., f_n) = \frac{Pr(f_1, f_2, ..., f_n | C) * Pr(C)}{Pr(f_1, f_2, ..., f_n)}
                  \end{eqnarray}
                </p>
                <br/>
                
                <p>Notice that Pr(C) is the prior on C, and since our evidence is conditionally independent given C, we can write the chain-rule factorization to achieve:</p>
                <p>
                  \begin{eqnarray}
                    = \frac{Pr(f_1 | C) * Pr(f_2 | C) * ... * Pr(f_n | C) * Pr(C)}{Pr(f_1, f_2, ..., f_n)}
                  \end{eqnarray}
                </p>
              </div>
              <br/>
              <p>So, in our example, we would have each indicator \(f_i\) be a different attribute about our witnessed object.</p>
              <p>So our example's quantity of interest is (for Hu? = Human?, M = movement, H = height, S = speech, F = form):</p>
              <br/>
              <div class='well'>
                <p class='text-left'>
                  $$
                    Pr(Hu? | M = slow, H = med, S = 1, F = 4) =\\
                     \frac{Pr(M = slow | Hu?) * Pr(H = med | Hu?) * Pr(S = 1 | Hu?) * Pr(F = 4 | Hu?) * Pr(Hu?)}
                     {Pr(M = slow, H = med, S = 1, F = 4)}
                  $$
                </p>
              </div>
              <br/>
              <p>Almost there! Now we just &quot;need&quot; one more element to complete our classification:</p>
              <br/>
              <div class='well'>
                <p>Now all we need to find out is the \(Pr(f_1, f_2, ..., f_n)\). We have a strategy to do this: Case analysis!</p>
                <p>
                  \begin{eqnarray}
                    Pr(f_1, f_2, ..., f_n) &=& \sum_{c \in C} Pr(f_1, f_2, ..., f_n | c) * Pr(c)\\
                                           &=& \sum_{c \in C} Pr(f_1 | c) * Pr(f_2 | c) * ... * Pr(f_n | c) * Pr(c)
                  \end{eqnarray}
                </p>
                <br/>
                
                <p>And so, substituting back into our original equation:</p>
                <p>
                  $$
                    \frac{Pr(f_1 | c) * Pr(f_2 | c) * ... * Pr(f_n | c) * Pr(C)}{Pr(f_1, f_2, ..., f_n)} =\\
                    \frac{Pr(f_1 | C) * Pr(f_2 | C) * ... * Pr(f_n | C) * Pr(C)}{\sum_{c \in C} Pr(f_1 | c) * Pr(f_2 | c) * ... * Pr(f_n | c) * Pr(c)}
                  $$
                </p>
              </div>
              <br/>
              
              <p>Notice: This gives us a *table* since we have the class variable C in the numerator; we really want to determine *which* value of C is most likely.</p>
              <p>We also notice that we don't need the denominator to make this determination since it's simply the normalizing constant!</p>
              <p>So, we simply compare the different values of C and classify based on whichever is most probable.</p>
              <p>This outcome is defined by:</p>
              <div class='well'>
                <p>The most likely class for all classes in C will be represented by:</p>
                <p>
                  $$\underset{c' \in C}{\operatorname{argmax}} \frac{Pr(f_1 | c') * Pr(f_2 | c') * ... * Pr(fn | c') * Pr(c')}{\sum_{c \in C} Pr(f_1 | c) * Pr(f_2 | c) * ... * Pr(f_n | c) * Pr(c)}$$
                </p>
                <br/>
                
                <p>Noting that the division is simply the normalizing constant:</p>
                <p>
                  $$\propto \underset{c' \in C}{\operatorname{argmax}} Pr(f_1 | c') * Pr(f_2 | c') * ... * Pr(f_n | c') * Pr(c')$$
                </p>
              </div>
              <br/>
              
              <p>...which looks intimidating, but just says, &quot;Choose the value of class variable C that has the highest probability given the witnessed indicator values for the object&quot;</p>
              <p>In our example, we would be choosing between: C = Human? = {yes, no}</p>
              
              <p class='example'>Recall our example Bayesian network from last week (which is actually a Naive Bayes structure with Smoking as the class variable). Suppose that we observe:
                $$\text{Asthma } = 1, \text{Cancer } = 0$$
                Is it more likely that this patient is a smoker or non-smoker?
              </p>
              <div class='fit-pres text-center'>
                <img src="../../../assets/images/winter-2015/cs-161/week-8/bayesnet-1.PNG">
              </div>
              <p class='question' name='naive-bayes-0'>Click for solution.</p>
              <div class='answer white-bg' name='naive-bayes-0'>
                <p>We'll find the solution to:</p>
                <p>$$\underset{s \in Smoking}{\operatorname{argmax}} Pr(Asthma = 1 | s) * Pr(Cancer = 0 | s) * Pr(s)$$</p>
                <p>
                  \begin{eqnarray}
                    Pr(A = 1 | S = 0) * Pr(C = 0 | S = 0) * Pr(S = 0) &=& 0.1 * 0.8 * 0.8\\
                                                                      &=& 0.064
                  \end{eqnarray}
                </p>
                <p>
                  \begin{eqnarray}
                    Pr(A = 1 | S = 1) * Pr(C = 0 | S = 1) * Pr(S = 1) &=& 0.6 * 0.1 * 0.2\\
                                                                      &=& 0.012
                  \end{eqnarray}
                </p>
                <p>Therefore, we conclude that it is more likely for our patient to NOT be a smoker.</p>
              </div>
            </div>
            <hr/>
            

            <br/>
            <div id='final-review' class='scrollspy-element' scrollspy-title='Final'></div>
            <h1>Final</h1>
            <div>
              <p>Your final is fast approaching! Engage panic!</p>
              <p>Just a quick note that I'll be preparing a massive practice final that I hope to be ready by early next week.</p>
              <p>If you want any type of problem to be particularly represented, let me know and I'll be sure to include an instance!</p>
            </div>
            
            
            <a class='btn btn-default pull-right hidden-print' href='javascript:window.print();'>
              <span class='glyphicon glyphicon-print'></span>
              &nbsp; PDF / Print
            </a>
            
          </div>
          <!-- END PRESENTATION CONTENT -->
          
          <!-- MATERIALS FROM CLASS: -->
          
            
        </div>
      </div>
      <!-- END MAIN CONTENT -->
      
      
    </div>
    <!-- END WRAPPER -->
    
    <!-- BEGIN FOOTER -->
    <div id="footer">
      <div class="container">
        <div class="col-md-12 text-center">
          
        </div>
      </div>
    </div>
    <!-- END FOOTER -->
    
  </body>
</html>
