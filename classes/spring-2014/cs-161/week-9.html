
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

  <head>
    <title>Andrew Forney - UCLA CS</title>
    <link href="../../../css/bootstrap.min.css" rel="stylesheet" type="text/css">
    <link href="../../../css/magic-bootstrap.css" rel="stylesheet" type="text/css">
    <link href="../../../css/main.css" rel="stylesheet" type="text/css">
    <script src="../../../js/lib/jquery-2.0.3.min.js"></script>
    <script src="../../../js/lib/bootstrap.min.js"></script>
    <script src="../../../js/lib/expanding.js"></script>
    <script src="../../../js/display/general/general-display.js"></script>
    <script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js"></script>
  </head>
  
  <body data-spy="scroll" data-target="#scrollspy">
    
    <!-- BEGIN WRAP -->
    <div id="wrap">
      
      <!-- BEGIN NAVIGATION -->
      <nav class='navbar navbar-default' role='navigation'>
        <div class='nav-accent'></div>
        <div class='container'>
          <div class='row'>
            <div class='col-md-12'>
              <div class='navbar-header'>
                <button class='navbar-toggle' type='button' data-toggle='collapse' data-target='.navbar-main-collapse'>
                  <span class='sr-only'>Toggle Navigation</span>
                  <span class='icon-bar'></span>
                  <span class='icon-bar'></span>
                  <span class='icon-bar'></span>
                </button>
                <a class='navbar-brand' href='/~forns/'>
                  <span id='brand-text'>
                    Andrew Forney
                  </span>
                </a>
              </div>
              
              <div id='nav-main' class='collapse navbar-collapse navbar-main-collapse'>
                <ul class='nav navbar-nav navbar-right'>
                  
                  <li>
                    <a href='/~forns/about.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-user'></span>
                      </div>
                      <p class='text-center'>About</p>
                    </a>
                  </li>
                  
                  <li class='active'>
                    <a href='/~forns/classes.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-book'></span>
                      </div>
                      <p class='text-center'>Classes</p>
                    </a>
                  </li>
                  
                  <li>
                    <a href='/~forns/contact.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-comment'></span>
                      </div>
                      <p class='text-center'>Contact</p>
                    </a>
                  </li>
                  
                  <li>
                    <a href='/~forns/publications.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-file'></span>
                      </div>
                      <p class='text-center'>Publications</p>
                    </a>
                  </li>
                  
                </ul>
              </div>
            </div>
          </div>
        </div>
      </nav>
      <!-- END NAVIGATION -->
      
      <!-- BEGIN MAIN CONTENT -->
      <div id="main-content" class="container">
        <div class="row">
          
          <!-- BEGIN SCROLLSPY -->
          <div class='col-md-2 hidden-sm hidden-xs'>
            <div class="bs-sidebar hidden-print affix" role="complementary">
              <ul id='scrollspy' class="nav bs-sidenav">
              </ul>
            </div>
          </div>
          <!-- END SCROLLSPY -->
          
          <!-- BEGIN PRESENTATION CONTENT -->
          <div class='col-md-10 presentation-content' role='main'>
            
            <ol class="breadcrumb hidden-print">
              <li><a href="../../../classes.html">Classes</a></li>
              <li><a href="./cs-161.html">Spring14 CS161</a></li>
              <li class="active">Week 9</li>
            </ol>
            
            
            <div id='decision' class='scrollspy-element' scrollspy-title='Decision Theory'></div>
            <h1>Decision Theory</h1>
            <div>
              <p>We all take actions to accomplish our goals, and it's our *preferences* that have set those goals to begin with.</p>
              <p>There are a variety of stepping stones that we take on the journey towards our goals, some that are more instrumental to accomplishing our task than others.</p>
              <p class='definition'><strong>Decision theory</strong> is the art of designing rational agents that act based on their utility-based preferences and their perception of the world.</p>
              <br/>
              <p>However, as opposed to some strong assumptions with planning that we made in past chapters, the consequences of our actions might not always be known.</p>
              <p>In this case, where we're dealing with incomplete knowledge or stochastic outcomes, we have to take probabilities into account.</p>
<pre class='prettyprint'>
  ; In first order logic planning:
  Action(Eat(Cake)
      Preconditions: Have(Cake)
      Effect: &not;Have(Cake) &and; Eaten(Cake))
  
  ; ...where states:
  s0 = Have(Cake)
  s1 = &not;Have(Cake) &and; Eaten(Cake)
  
  ; ...and so:
  Result(s0, Eat(Cake)) = s1
  ; i.e., the result of being in state s0
  ; and eating your cake is that you *definitely*
  ; arrive in state s1
</pre>
              <br/>
              <p>BUT... what if our cake is a Forney Industries Self-Replicating Cake, such that while eating it, it actually has a chance to make a copy of itself!</p>
              <p>This means that eating it does *not* necessarily imply: &not;Have(Cake)</p>
              <p>(OK, you can make the examples if you don't like that one)</p>
              <p class='toolkit'>Instead of representing our action as a definite input-output function <code class='prettyprint'>Result(s0, a) = s1</code>, we'll instead represent it as a random variable:
                <code class='prettyprint'>Result(a) = s1</code> that has the *possibility* of arriving at some state s1 given an action we take and the evidence we observe about our environment.</p>
<pre class='prettyprint'>
  ; So now, instead of:
  Result(s0, a) = s1
  
  ; ...we have:
  Pr(Result(a) = s1 | a, e)
  
  ; ...where:
  ;   a  = the action we are taking (given because
  ;        we are interested in the posterior Pr that
  ;        we did indeed accomplish action a)
  ;   e  = the evidence about the environment
  ;        that we currently know about
  ;   s1 = state we are interested in the
  ;        *probability* of reaching given a, e
  
  ; [!] NOTE: In this new notation, the current state
  ; we are in, s0, is implicit, but could be made
  ; explicit by writing:
    Pr(Result(a) = s1 | a, e)
  = &Sigma;_s Pr(Result(s, a) = s1 | a) * Pr(s0 = s | e)
  
  ; i.e., the sum over all states of the probabilty of being
  ; in the initial state given the evidence (Pr(s0 = s | e))
  ; times the probability of reaching state s1 with action a
</pre>
              <br/>
              <p>So now we have some notion of a distribution over possible states that we can reach from taking an action under the current evidence, we need to know *which* action we should take!</p>
              <p>We do this by defining our agent's Utility function.</p>
              <p class='definition'>A <strong>Utility Function</strong> U(s) assigns a single number to express the desirability of a state (the higher the number, the more desirable).</p>
              <p class='definition'>The <strong>Expected Utility</strong> of an action given the evidence e, <code class='prettyprint'>EU(a | e)</code> is simply the average utility value of the
                outcomes weighted by the probability that the outcome occurs.</p>
<pre class='prettyprint'>
  EU(a | e)
    = &Sigma;_s Pr(Result(a) = s | a, e) * U(s)
    
  ; i.e., the expected utility of taking action a
  ; is the chance that action a will land us in state
  ; s with desirability of that state U(s)
</pre>
              <br/>
              <p>Once we know the expected utility of taking some action, if we have multiple actions to consider, we simply take the one with the highest expected utility!</p>
              <p>Wasn't it The Rolling Stones who said, &quot;You can't always get what you want, but if you try sometimes you might find... maximizing your expected utility to be sufficient?&quot;</p>
              <p class='definition'>The <strong>maximum expected utility (MEU)</strong> criterion simply stipulates that an agent will choose an action (amongst all those that are possible) that has the
                highest expected utility of the choices.</p>
<pre class='prettyprint'>
  action_choice = argmax EU(a | e)
                    a
  
  ; i.e., for all action choices a, choose
  ; the one who has the highest expected
  ; utility given evidence e
</pre>
              <br/>
              <p class='example'>In the following examples, determine which action will be taken based on the maximum expected utility principle.</p>
              <blockquote>
                <strong>Problem 1:</strong> Andrew is deciding if he should use his day off to go to the beach, or stay inside and watch reruns of Golden Girls, because nothing else good is on TV...
                Going to the beach would be a lot of fun and staying in would just be OK, but forecasts gave a 60% chance of rain for the day, which would spoil a trip to the beach. Observe the
                following utility / probability table and decide what Andrew should do.
              </blockquote>
              <table class='table table-striped table-bordered'>
                <thead>
                  <tr>
                    <th><p>Action</p></th>
                    <th><p>Weather</p></th>
                    <th><p>U(Action, Weather)</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><p>AtHome</p></td>
                    <td><p>clear</p></td>
                    <td><p>2</p></td>
                  </tr>
                  <tr>
                    <td><p>AtHome</p></td>
                    <td><p>raining</p></td>
                    <td><p>3</p></td>
                  </tr>
                  <tr>
                    <td><p>AtBeach</p></td>
                    <td><p>clear</p></td>
                    <td><p>4</p></td>
                  </tr>
                  <tr>
                    <td><p>AtBeach</p></td>
                    <td><p>raining</p></td>
                    <td><p>1</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p class='question' name='dec-q0'>Click for solution.</p>
              <div class='answer' name='dec-q0'>
<pre class='prettyprint'>
  ; First, determine the expected utility
  ; of each action:
  EU(AtHome)
    = Pr(AtHome, clear | AtHome) * U(AtHome, clear) +
      Pr(AtHome, raining | AtHome) * U(AtHome, raining)
    = 0.4 * 2 + 0.6 * 3
    = 2.6
    
  EU(AtBeach)
    = Pr(AtBeach, clear | AtBeach) * U(AtBeach, clear) +
      Pr(AtBeach, raining | AtBeach) * U(AtBeach, raining)
    = 0.4 * 4 + 0.6 * 1
    = 2.2
    
  ; Next, using MEU criteria:
  action_chosen = argmax EU(a | e)
                    a
  
                = argmax ({AtHome: 2.6, AtBeach: 2.2})
                = AtHome
                
  ; Golden Girls it is :(
</pre>
              </div>
              <br/>
              <p class='example'>Recompute the previous example given that we observed Weather = clear outside.</p>
              <p class='question' name='dec-q1'>Click for solution.</p>
              <div class='answer' name='dec-q1'>
<pre class='prettyprint'>
  ; Our evidence:
  e = {Weather = clear}
  
  ; First, determine the expected utility
  ; of each action with the evidence:
  EU(AtHome | e)
    = Pr(AtHome, clear | AtHome, e) * U(AtHome, clear) +
      Pr(AtHome, raining | AtHome, e) * U(AtHome, raining)
    = 0.4 * 2 + 0 * 3
    = 0.8
    
  EU(AtBeach)
    = Pr(AtBeach, clear | AtBeach) * U(AtBeach, clear) +
      Pr(AtBeach, raining | AtBeach) * U(AtBeach, raining)
    = 0.4 * 4 + 0 * 1
    = 1.6
    
  ; Next, using MEU criteria:
  action_chosen = argmax EU(a | e)
                    a
  
                = argmax ({AtHome: 0.8, AtBeach: 1.6})
                = AtBeach
                
  ; Beach time!
</pre>
              </div>
            </div>
            <hr/>
            
            
            <br/>
            <div id='preferences' class='scrollspy-element' scrollspy-title='Preferences'></div>
            <h1>Preferences</h1>
            <div>
              <p>So the next question you might be asking is: where do we get these utility values?</p>
              <p>Do we need to have explicit numerical values mapped to states as indications of their desirability or can these numbers simply be relative and derived?</p>
              <p class='definition'><strong>Preferences</strong> are our agent's rankings of desirable states having considered their relative likelihoods of being reached.</p>
              <br/>
              <p>We can use preference orderings to recover utility functions...</p>
              <p>...but we also have to consider how desirable the state is *in concert with* the probability of reaching it.</p>
              <p>In other words, two actions A1 and A2 might both be capable of reaching a desirable state, but if one of those two actions is more *likely* to reach that state, we will want to choose
                it over the other.</p>
              <p>First, to formalize the possible outcomes of an action, we turn to the notion of a lottery:</p>
              <br/>
              <p class='definition'>A <strong>lottery</strong> is a representation of the possible outcome states from taking some action with the probabilities of reaching each outcome.</p>
              <p class='definition'>An <strong>outcome</strong> is either a primitive state or, recursively, another lottery</p>
<pre class='prettyprint'>
  ; For some action, a lottery L
  ; might look like:
  L = [p1, S1; p2, S2; ...; pn Sn]
  
  ; ...where each:
  ;   p_i = the probability of reaching
  ;         S_i by taking that action
  ;   S_i = a possible state outcome OR
  ;         recursively, another lottery
  
  ; Example:
  L1 = [0.5, S1; 0.25, S2; 0.25, L2]
  L2 = [0.3, S3; 0.6, S1; 0.1, S5]
</pre>
              <br/>
              <p class='definition'>A lottery is called <strong>complex</strong> if it includes an outcome that is itself another lottery.</p>
              <p>The term &quot;lottery&quot; is intuitive because taking an action associated with a lottery is like buying a Lotto ticket, and hoping that you &quot;win&quot; the state you desired.</p>
              <p>Now that we know how we can model outcomes of taking a particular action, let's talk about preferences.</p>
              <p>For any two lotteries A and B, we can use the following notation to describe preferences:</p>
              <br/>
              <ul class='indent-1'>
                <li><p>A > B &nbsp;&nbsp;&nbsp; the agent prefers A over B</p></li>
                <li><p>A ~ B &nbsp;&nbsp;&nbsp; the agent is indifferent between A and B</p></li>
                <li><p>A &ge; B &nbsp;&nbsp;&nbsp; the agent prefers A over B or is indifferent between them</p></li>
              </ul>
              <br/>
              <p>The primary goal of utility theory is to determine how preferences between complex lotteries are related to preferences on the primitive states that compose them.</p>
              <p>To do this, we can define some axioms on lotteries that, if violated by our intelligent systems, would lead to irrational behavior.</p>
              <br/>
              <table class='table table-striped table-bordered'>
                <thead>
                  <tr>
                    <th><p>Axiom</p></th>
                    <th><p>Interpretation</p></th>
                    <th><p>Formalism</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th><p>Orderability</p></th>
                    <td><p>An agent cannot avoid deciding between two actions (i.e., either one is preferred or they are equally preferable) 
                      and must assign one of the following relationships to lotteries A and B.</p></td>
                    <td class='col-md-4'><p>(A &gt; B), (B &gt; A), or (A ~ B)</p></td>
                  </tr>
                  <tr>
                    <th><p>Transitivity</p></th>
                    <td><p>If an agent prefers lottery A to lottery B, and also prefers lottery B to lottery C, then it also prefers lottery A to lottery C.</p></td>
                    <td><p>(A &gt; B) &and; (B &gt; C) &rArr; (A &gt; C)</p></td>
                  </tr>
                  <tr>
                    <th><p>Continuity</p></th>
                    <td><p>If A &gt; B &gt; C (i.e., some lottery B is between A and C in preference), then there is some probability p that we could find such that a certain outcome of B would
                      be equally preferrable to an outcome of A with probability p or of C with probability (1 - p)</p></td>
                    <td><p>A &gt; B &gt; C &rArr;<br/> &exist;p [p, A; 1 - p, C] ~ [1, B]</p></td>
                  </tr>
                  <tr>
                    <th><p>Substitutability</p></th>
                    <td><p>If two lotteries are equally preferrable, then you may substitute one in for the other in some other complex lottery.</p></td>
                    <td><p>A ~ B &rArr;<br/>[p A; 1 - p, C] ~ [p, B; 1 - p, C]</p></td>
                  </tr>
                  <tr>
                    <th><p>Monotonicity</p></th>
                    <td><p>If we prefer outcome A to outcome B, then we must also prefer lotteries that have a higher probability to reach A than B.</p></td>
                    <td><p>A &gt; B &rArr;<br/>(p &gt; q &hArr;<br/>&nbsp;&nbsp;[p, A; 1 - p, B] &gt; [q, A; 1 - q, B]<br/>)</p></td>
                  </tr>
                  <tr>
                    <th><p>Decomposability</p></th>
                    <td><p>We can reduce any number of complex lotteries down to a simpler one simply by the laws of probability.</p></td>
                    <td><p>L1 = [p, A; 1 - p, L2]<br/>L2 = [q, B; 1 - q, C]<br/>L1 ~ [p, A; (1 - p)q, B; (1 - p)(1 - q), C]</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p class='question' name='pref-q0'>Substitutability tells us that two equally preferrable lotteries can be substituted for one another in some other complex lottery. Does the following also
                hold?<br/>
                <code class='prettyprint'>(A &gt; B) &rArr; [p, A; 1 - p, C] &gt; [p, B; 1 - p, C]</code>
              </p>
              <p class='answer' name='pref-q0'>Yes! We can think of this as the case of &quot;All other considerations constant,&quot; complex lotteries containing A will be preferrable to those containing B.</p>
              <br/>
              <p>Alright, so we have lotteries that abide by these axioms... what were we trying to do again?</p>
              <p>Oh yeah... get some notion of what to use for utility functions...</p>
              <p>Well, we have the following two consequences of preferential axioms that can allow us to solve for some (non-unique) utility function:</p>
              <br/>
              <table class='table table-striped table-bordered'>
                <thead>
                  <tr>
                    <th><p>Consequence</p></th>
                    <th><p>Interpretation</p></th>
                    <th><p>Formalism</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th><p>Existence of Utility Function</p></th>
                    <td><p>If an agent's preferences abide by the above axioms, then there exists a utility function such that: <code class='prettyprint'>U(A) &gt; U(B)</code> if and only if
                      A is preferred to B, and <code class='prettyprint'>U(A) = U(B)</code> if and only if the agent is indifferent between A and B.</p></td>
                    <td class='col-md-4'><p>U(A) &gt; U(B) &hArr; A &gt; B<br/>U(A) = U(B) &hArr; A ~ B</p></td>
                  </tr>
                  <tr>
                    <th><p>Expected Utility</p></th>
                    <td><p>The utility of a lottery is the sum of the probability of each outcome times the utility of that outcome.</p></td>
                    <td><p>U([p1, S1; ...; pn, Sn]) =<br/>&Sigma;_i p_i * U(S_i)</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>From these two consequences, we see that, while it *does* matter what numbers we choose for our utility functions (dependent upon the
                scenario and how much more state s0 is desirable compared to state s1), there exists a utility function capable of respecting our preference ordering.</p>
              <br/>
              <p class='example'>Read the following preferences, observe the action lotteries, and then decide which action our intelligent system would choose based on the MEU criterion.</p>
<pre class='prettyprint'>
  ; Our agent has the following preferences,
  ; which abide by the 6 axioms above:
  1. A &gt; B
  2. B ~ C
  3. C &ge; D
  
  ; Utilities:
        A  B  C  D
  U(S)  3  2  2  1
  
  ; Action 1 corresponds to lottery:
  L1 = [0.2, A; 0.3, B; 0.5, L3]
  
  ; Action 2 corresponds to lottery:
  L2 = [0.5, B; 0.3, C; 0.2, L3]
  
  L3 = [0.6, D; 0.4, B]
</pre>
              <br/>
              <p class='question' name='dec-q1'>Click for solution.</p>
              <div class='answer' name='dec-q1'>
<pre class='prettyprint'>
  ; Since Actions 1 and 2 have complex
  ; lotteries, we can use decomposition to
  ; compress them into simple lotteries:
  L1 = [0.2, A; 0.3, B; 0.5, L3]
     = [0.2, A; 0.3, B; 0.5, [0.6, D; 0.4, B]]
     = [0.2, A; 0.5, B; 0.3, D]
     
  L2 = [0.5, B; 0.3, C; 0.2, L3]
     = [0.5, B; 0.3, C; 0.2, [0.6, D; 0.4, B]]
     = [0.58, B; 0.3, C; 0.12, D]
     
  ; Now compute the utility of each lottery:
  U(L1) = &Sigma;_i p_i * U(S_i)
        = 0.2 * 3 + 0.5 * 2 + 0.3 * 1
        = 1.9
        
  U(L2) = &Sigma;_i p_i * U(S_i)
        = 0.58 * 2 + 0.3 * 2 + 0.12 * 1
        = 1.88
        
  ; Whew that was close! But it looks like action
  ; 1 is the (slightly) more preferrable
</pre>
              </div>
              
              <br/>
              <h3>Multi-attribute Utility</h3>
              <p>Previously, we've dealt with states that have had an atomic utility value assigned to them, for example:</p>
<pre class='prettyprint'>
  ; Whole state of being at home and it
  ; raining out gets assigned, statically,
  ; the utility value of 3
  U(AtHome, rainy) = 3
</pre>
              <br/>
              <p>BUT, what if I wanted to assess *components* of states and treat them with different utility contributions?</p>
              <p>Take the book example for... example:</p>
              <blockquote>
                Siting (i.e., determining where to build based on analyzed factors) an airport requires a variety of considerations. If we're choosing between some number of land plots on which to build
                our airport, then we can analyze the putative values of some variables of interest based on our possible choices / actions. Let's say we were on the city planning committee of such and determined
                the following variables would comprise our decision criteria:
              </blockquote>
              <ul class='indent-1'>
                <li><p><strong>Cost:</strong> the price of the land required to build upon, plus construction costs, plus the price of legal fees and processing, etc.</p></li>
                <li><p><strong>Noise:</strong> the amount of noise generated by the airport.</p></li>
                <li><p><strong>Deaths:</strong> possible deaths from construction hazards and other airport-related risks.</p></li>
              </ul>
              <br/>
              <p>If we're considering a bunch of construction sites, it might not be feasible to sit down and assign a utility value to each one individually...</p>
              <p>BUT, we might have some data on our variables of interest based on projections from studies and other sources of information, so perhaps we can construct our utility values from these facts.</p>
              <p>That said, we often don't have deterministic information available (e.g., we don't know for certain that construction on site A will incur exactly 3 deaths), so we need to use estimation
                techniques.</p>
              <p>Additionally, we don't always treat all of our variables of interest equally--some might need to be weighted as more important than others.</p>
              <p class='definition'>For some vector of variables <strong>X</strong> = {X1, X2, ..., Xn} and their value functions (f1, f2, ..., fn), where each f_i corresponds to a variable X_i, the
                utility of a state with variable instantiation: x = {x1, x2, ..., xn} is given by:<br/><code class='prettyprint'>U(x1, x2, ..., xn) = F[f1(x1), f2(x2), ..., fn(xn)]</code></p>
              <p class='definition'>When our metrics of interest exibit <strong>mutual preferential independence</strong>, it means that a change in the value of one variable will not necessarily cause
                a change in the value of another variable.</p>
              <p class='toolkit'>If all of our metrics of interest exibit mutual preferential independence, then our agent's choice boils down to a simple maximimization of:<br/>
                <code class='prettyprint'>F[f1(x1), f2(x2), ..., fn(xn)] = &Sigma;_i f_i(x_i)</code>
              </p>
              <br/>
              <p>Here, F is a simple function like addition that would aggregate all of the individual variable-weighting <strong>value functions</strong> f1, f2, ..., fn for variable values x1, x2, ..., xn.</p>
              <p>The idea is that we want to condense all of the variable information into a single utility value based on the importance weights of each variable.</p>
              <p class='example'>For our airport siting example, let's consider the following two sites and the value functions that provide the proper variable weighting. We'll assume our variables of interest
                exibit mutual preferential independence. Determine, using the mutual preferential independence utility function above with F being simple summation, which is the superior site.</p>
              <br/>
              <table class='table table-bordered table-striped'>
                <thead>
                  <tr>
                    <th><p>Variable</p></th>
                    <th><p>Value Function</p></th>
                    <th><p>Site 1 Value</p></th>
                    <th><p>Site 2 Value</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th><p>Cost</p></th>
                    <td><p>f_cost (x) = -x</p></td>
                    <td><p>$10,000,000</p></td>
                    <td><p>$15,000,000</p></td>
                  </tr>
                  <tr>
                    <th><p>Noise</p></th>
                    <td><p>f_noise (x) = -x * 1000</p></td>
                    <td><p>200dB</p></td>
                    <td><p>180dB</p></td>
                  </tr>
                  <tr>
                    <th><p>Deaths</p></th>
                    <td><p>f_deaths (x) = -x * 10^9</p></td>
                    <td><p>2</p></td>
                    <td><p>1</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p class='question' name='dec-q2'>Click for solution</p>
              <div class='answer' name='dec-q2'>
<pre class='prettyprint'>
  ; Using our definition for mutual preferential
  ; independence utility, we assess each site
  U(Site1) = F[f_cost($10,000,000),
               f_noise(200dB),
               f_deaths(2)]
           = -10,000,000 + (-200 * 1000) + (-2 * 10^9)
           = -2,010,200,000
           
  U(Site2) = F[f_cost($15,000,000),
               f_noise(180dB),
               f_deaths(1)]
           = -15,000,000 + (-180 * 1000) + (-1 * 10^9)
           = -1,015,180,000
           
  ; Site2 is the superior site based on our utility
  ; definitions!
</pre>
              </div>
              <p class='definition'><strong>Strict dominance</strong> is the case where a state is superior on all metrics.</p>
              <p>We see that in our example above, Site2 is not strictly dominant compared to Site1 because Site1 actually has the lower monetary cost.</p>
              <p>It turns out, however, that the utility cost of our estimated number of deaths far outweighs the monetary cost.</p>
              
              <br/>
              <h3>Decision Networks</h3>
              <p>Often, however, our decisions have factors that are not necessarily the effect of anything we can control.</p>
              <p>To model this uncertainty, we can combine Bayesian Networks, which model our knowledge of the way the world works, with the preference model we've been discussing to assess a utility
                value for possible actions under consideration.</p>
              <p class='definition'>A <strong>decision network</strong> unifies the stochastic modelling capacities of a Bayesian network with the action-choice and utility concepts of preference models
                by adding two additional nodes to the Bayesian network model.</p>
              <p class='definition'>Decision network <strong>chance nodes</strong> (ovals) represent random variables, and illustrate uncertainty about the values of some of our variables (same way that
                Bayesian networks handled this: conditional probability tables).</p>
              <p class='definition'>Decision network <strong>decision nodes</strong> (rectangles) represent points where the agent has a choice of actions.</p>
              <p class='definition'>Decision network <strong>utility nodes</strong> (diamonds) represent the agent's utility function for a given instantiation of decisions and inference.</p>
              <br/>
              <p>Here is the book's example for our airport siting problem, with some additional chance nodes added in to represent our knowledge of the world:</p>
              <br/>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-9/decision-0.PNG' />
              </div>
              <br/>
              <p>So here are some observations to make:</p>
              <ul class='indent-1'>
                <li><p><strong>Purpose of decision networks:</strong> judge which actions from our decision (rectangle) nodes produce the highest utility.</p></li>
                <li><p><strong>Data formats:</strong>
                  <ul class='indent-1'>
                    <li><p>Chance nodes are CPTs just like in Bayesian networks, except parents of chance nodes can be decision nodes as well (which are always given because
                  we're evaluating between action choices)</p></li>
                    <li><p>
                      Decision nodes are simply a setting of variables to values within a state that a given action would result in. (e.g., choosing Site1 sets cost to the given value of $10,000,000)
                    </p></li>
                    <li><p>Utility nodes are an evaluation of its direct causes (parents) based on supplied value functions. In our example above, U = F(f_death(Deaths), f_noise(Noise), f_cost(Cost))</p></li>
                  </ul>
                </p></li>
                <li><p><strong>Inference:</strong> based on the choice for our decision nodes, we can use an inference algorithm (like variable elimination from last week) that gives us the utility 
                  node's parents' posterior probabilities to determine not only the weighted utility of a given variable value, but also accounting for the chance that it will occur based on any evidence.</p></li>
              </ul>
              <br/>
              <p>And that's a decision network in a nutshell!</p>
              <br/>
              <p>Of course, all of this relies on us as humans knowing what data and variables to incorporate into our decision networks...</p>
              <p>Our next topic will be the ability to learn how to construct a decision mechanism from raw data alone!</p>
            </div>
            <hr/>
            
            
            <br/>
            <div id='learning' class='scrollspy-element' scrollspy-title='Learning'></div>
            <h1>Learning</h1>
            <div>
              <p>The art of machine learning is little different from how we as humans acquire new information, and has particularly close analogy to how we teach children.</p>
              <p class='definition'><strong>Machine learning</strong> is the process by which programmers equip intelligent systems to modify their behavior based on training, observations, and other
                features available to the program during operation that might not have been available or convenient for the programmer.</p>
              <br/>
              <p>So one of the first questions you might ask is: why give computers this adaptability at all? Shouldn't our programmers have all the tools necessary to empower their agents when they
                sit down to program them in the first place?</p>
              <p>As it turns out, the answer is: not always; there are a variety of cases in which it is not possible for a programmer to explicitly define an agent's behavior, including:</p>
              <ul class='indent-1'>
                <li><p>Prediction engines that assess changes over time cannot account for all possible future changes at the time of programming and must be adaptive.</p></li>
                <li><p>Navigational systems cannot be programmed with all possible obstacles, mazes, and terrain features at the time of programming and must be adaptive.</p></li>
                <li><p>Programmers may not *know* exactly how to define certain behavior, but can instead pose learning problems to their systems to get a gist for the intended behavior.</p></li>
              </ul>
              <br/>
              <p>Learning algorithms generally suit one of a few different flavors:</p>
              <table class='table table-bordered table-striped'>
                <thead>
                  <tr>
                    <th><p>Learning Class</p></th>
                    <th><p>Description</p></th>
                    <th><p>Example</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th><p>Unsupervised Learning</p></th>
                    <td><p>Unsupervised learning is a type of pattern extrapolation engine whose typical task is <strong>clustering</strong>, the act of finding similarities between various input items
                      (like images, text, etc.) and lumping them into some sort of group. In other words, it attempts to find hidden structures in unlabeled data.</p></td>
                    <td><p>Image Classification: given lots of images of faces and lots of images of computers, an unsupervised learning algorithm should be capable of lumping the faces together apart from
                      the computers.</p></td>
                  </tr>
                  <tr>
                    <th><p>Reinforcement Learning</p></th>
                    <td><p>Just like operant conditioning (for humans), reinforcement learning provides an input for our program, on which it will make some sort of decision and be rewarded (if that was
                      the right decision) or punished (if that was the wrong decision), figuratively speaking.</p></td>
                    <td><p>Animat Modeling: simulation study of animat populations where taking some action (like eating poison berries) has a harmful consequence that discourages that action in the
                      future.</p></td>
                  </tr>
                  <tr>
                    <th><p>Supervised Learning</p></th>
                    <td><p>Just like having an instructor or parent correct your mistakes or reward your triumphs, supervised learning has some &quot;oracle&quot; that will tell you the correct answer
                      on some problem during training so that you'll know what to change when you're wrong and what to keep when you're right. Additionally, labeled input data.</p></td>
                    <td><p>Object Recognition: given lots of images of objects with corresponding labels, e.g. a picture of a chair with label &quot;chair&quot;, object recognition will be able to
                      find other chairs in the future based on the label given *and* know that the particular object represents a chair (unlike unsupervised learning, which can only cluster)</p></td>
                  </tr>
                  <tr>
                    <th><p>Semi-supervised Learning</p></th>
                    <td><p>Almost exactly like supervised learning except that the labels or corrections given to us by our, now imperfect, oracle may not be entirely trustworthy. This creates extra
                      noise that sets semi-supervised learning apart from its accurately supervised counterpart.</p></td>
                    <td><p>User Input Classification: determining age based on images of people and their self-reported age (on which they might have lied).</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>So, the first task of machine learning is to choose a model class that best fits the data that we have available.</p>
              <p>What exactly constitutes our definition of &quot;best&quot; depends on the task at hand...</p>
              <p>Two tools we'll discuss for some applications are decision trees and Bayesian networks.</p>
              <br/>
              
            </div>
            <hr/>
            
            
            <a class='btn btn-default pull-right hidden-print' href='javascript:window.print();'>
              <span class='glyphicon glyphicon-print'></span>
              &nbsp; PDF / Print
            </a>
            
          </div>
          <!-- END PRESENTATION CONTENT -->
          
          <!-- MATERIALS FROM CLASS: -->
          
            
        </div>
      </div>
      <!-- END MAIN CONTENT -->
      
      
    </div>
    <!-- END WRAPPER -->
    
    <!-- BEGIN FOOTER -->
    <div id="footer">
      <div class="container">
        <div class="col-md-12 text-center">
          
        </div>
      </div>
    </div>
    <!-- END FOOTER -->
    
  </body>
</html>
