
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

  <head>
    <title>Andrew Forney - UCLA CS</title>
    <link href="../../../css/bootstrap.min.css" rel="stylesheet" type="text/css">
    <link href="../../../css/magic-bootstrap.css" rel="stylesheet" type="text/css">
    <link href="../../../css/main.css" rel="stylesheet" type="text/css">
    <script src="../../../js/lib/jquery-2.0.3.min.js"></script>
    <script src="../../../js/lib/bootstrap.min.js"></script>
    <script src="../../../js/lib/expanding.js"></script>
    <script src="../../../js/display/general/general-display.js"></script>
    <script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js"></script>
  </head>
  
  <body data-spy="scroll" data-target="#scrollspy">
    
    <!-- BEGIN WRAP -->
    <div id="wrap">
      
      <!-- BEGIN NAVIGATION -->
      <nav class='navbar navbar-default' role='navigation'>
        <div class='nav-accent'></div>
        <div class='container'>
          <div class='row'>
            <div class='col-md-12'>
              <div class='navbar-header'>
                <button class='navbar-toggle' type='button' data-toggle='collapse' data-target='.navbar-main-collapse'>
                  <span class='sr-only'>Toggle Navigation</span>
                  <span class='icon-bar'></span>
                  <span class='icon-bar'></span>
                  <span class='icon-bar'></span>
                </button>
                <a class='navbar-brand' href='/~forns/'>
                  <span id='brand-text'>
                    Andrew Forney
                  </span>
                </a>
              </div>
              
              <div id='nav-main' class='collapse navbar-collapse navbar-main-collapse'>
                <ul class='nav navbar-nav navbar-right'>
                  
                  <li>
                    <a href='/~forns/about.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-user'></span>
                      </div>
                      <p class='text-center'>About</p>
                    </a>
                  </li>
                  
                  <li class='active'>
                    <a href='/~forns/classes.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-book'></span>
                      </div>
                      <p class='text-center'>Classes</p>
                    </a>
                  </li>
                  
                  <li>
                    <a href='/~forns/contact.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-comment'></span>
                      </div>
                      <p class='text-center'>Contact</p>
                    </a>
                  </li>
                  
                  <li>
                    <a href='/~forns/publications.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-file'></span>
                      </div>
                      <p class='text-center'>Publications</p>
                    </a>
                  </li>
                  
                </ul>
              </div>
            </div>
          </div>
        </div>
      </nav>
      <!-- END NAVIGATION -->
      
      <!-- BEGIN MAIN CONTENT -->
      <div id="main-content" class="container">
        <div class="row">
          
          <!-- BEGIN SCROLLSPY -->
          <div class='col-md-2 hidden-sm hidden-xs'>
            <div class="bs-sidebar hidden-print affix" role="complementary">
              <ul id='scrollspy' class="nav bs-sidenav">
              </ul>
            </div>
          </div>
          <!-- END SCROLLSPY -->
          
          <!-- BEGIN PRESENTATION CONTENT -->
          <div class='col-md-10 presentation-content' role='main'>
            
            <ol class="breadcrumb hidden-print">
              <li><a href="../../../classes.html">Classes</a></li>
              <li><a href="./cs-161.html">Spring14 CS161</a></li>
              <li class="active">Week 2</li>
            </ol>
            
            <div id='states' class='scrollspy-element' scrollspy-title='States &amp; Search'></div>
            <h1>Intro to States &amp; Search</h1>
            <div>
              <p>We perform search in our daily lives all the time...</p>
              <p>We have <strong>problems</strong> that have a certain issue and resolution, and generally some <strong>strategies</strong> to use <strong>actions</strong> to resolve those problems.</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/waldo.jpg' />
              </div>
              <br/>
              <p>The familar Where's Waldo is a classic visual search paradigm where we start in some <strong>state</strong> of not knowing where Waldo is, and perform serial search to try to find Waldo.</p>
              <p class='definition'><strong>Search</strong> is exactly what it sounds like: the process of moving from a starting state of a problem to a goal state of that problem.</p>
              <p class='definition'><strong>States</strong> are a given instantiation of a search problem's variables of interest (possibly an incomplete instantiation).</p>
              <br/>
              <p>Search is therefore problem dependent, such that we need some constraints upon the domain of what we're searching for.</p>
              <p>
                In the case of Where's Waldo, it might be difficult to quantify some notion of states (perhaps we start in the &quot;haven't found Waldo&quot; state and want to move to the 
                &quot;found Waldo&quot; one)
              </p>
              <p class='definition'>A <strong>search domain</strong> is the set of all possible instantiation of variables of interest, i.e. all possible states, for a given problem.</p>
              <br/>
              <p>If we didn't have a search domain, then the concept of search is intractable for both humans and computers.</p>
              <p>
                For Where's Waldo, the search domain is simply a given picture that we know has Waldo somewhere inside of it; we don't go looking in a dictionary for Waldo (the dictionary is not in the 
                search domain!), unless we're really bad at the game...
              </p>
              <p>So, to perform search, we have a couple of rules about states:</p>
              <p class='definition'>An <strong>initial state</strong> is the state that you begin your search operation within.</p>
              <p class='definition'>A <strong>goal state</strong> is a state that satisfies your search criteria, i.e., that completes your search from an initial to a goal state.</p>
              <p class='definition'><strong>Actions</strong> are the legal manipulations of a current state that either (1) return us to the same state, or (2) transition to a new state.</p>
              <br/>
              <p>Actions can also have costs associated with them, e.g., the action of driving through a 10 mile road might have a higher cost than moving through a 1 mile road.</p>
              <p class='definition'>Therefore, the <strong>score</strong> of a particular goal state might be a sum of the costs encountered on the path of actions from the initial to goal state.</p>
              <p>So, in summary, search is the process of moving from some initial state to some goal state through successive applications of legal actions.</p>
            </div>
            <hr/>
            
            
            <br/>
            <div id='uninformedSearch' class='scrollspy-element' scrollspy-title='Uninformed Search'></div>
            <h1>Classical Search</h1>
            <div>
              <p>&quot;I found Beethoven!&quot; you exclaimed excitedly.</p>
              <p>Not that kind of classical search!</p>
              <p class='definition'><strong>Classical search</strong> is the systematic, goal / test oriented search of moving from an initial state to a goal state.</p>
              <br/>
              <h3>Motivating Example</h3>
              <p>Let's talk about the problem of maze pathfinding for our motivating example!</p>
              <p>Here's the problem we're going to discuss:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-0.PNG' />
              </div>
              <br/>
              <p>Our search goal will be to determine if a path exists in the grid from the player's position to the goal state.</p>
              <p class='question' name='search-q0'>What will a state look like in this problem?</p>
              <p class='answer' name='search-q0'>Possibly just the position of the player stored as an (X, Y) tuple.</p>
              <br/>
              <p class='question' name='search-q1'>What will the initial state look like in our example of this problem?</p>
              <p class='answer' name='search-q1'>The initial player position (1, 1)</p>
              <br/>
              <p class='question' name='search-q2'>What will the goal state look like in our example of this problem?</p>
              <p class='answer' name='search-q2'>The goal position (1, 3)</p>
              <br/>
              <p class='question' name='search-q3'>What will actions look like in this problem?</p>
              <p class='answer' name='search-q3'>Any movement from a given state's player position to an adjacent, open or goal tile. This gives us transitions of:<br/>
                <code class='prettyprint'>Up: (0, +1), Down: (0, -1), Right: (+1, 0), Left: (-1, 0)</code>
              </p>
              
              <br/>
              <h3>Building a Search Tree</h3>
              <p>So now that we have our concept of states and transitions in the maze pathfinding example, how do we go about searching for the answer?</p>
              <p>In the <strong>classical search</strong> methodology, we want a systematic way of exploring states so that we can find an answer; to do this, we build a search tree:</p>
              <p class='definition'>
                A <strong>search tree</strong> is a tree with the initial state at the root, transitions along the edges, and nodes corresponding to states that can be reached from one action
                applied from the parent.
              </p>
              <br/>
              <p>So, here is a *non-systematic* (search strategy omitted) search path that corresponds to a successfully discovered goal from our previous problem:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-1.PNG' />
              </div>
              <br/>
              <p>I say it's non-systematic because we didn't really apply any search-strategy; I just kinda picked a path that I saw was available!</p>
              <p class='definition'>A <strong>search strategy</strong> is a systematic means of exploring a search tree, and decides which state we will expand next in the course of our search.</p>
              <br/>
              <p>In this conception, whenever we expand a node, we generate all of the possible moves from that node and then use our search strategy to choose which of the generated nodes to expand next.</p>
              <p class='definition'><strong>Expanding</strong> a node is akin to visiting it on a search traversal, and generating all plausible child nodes.</p>
              <p class='definition'>We <strong>generate</strong> nodes by first expanding a parent node, and then generating all child nodes that could be reached from the parent by taking a legal action.</p>
              <br/>
              <p>Here is a search tree where we expand the root, generating its children (assuming each movement is legal), and then expand the child node corresponding to a taken &quot;right&quot; action:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-2.PNG' />
              </div>
              <br/>
              <p class='question' name='search-q4'>Will this be a valid expansion / generation structure for our maze example at the start of this section?</p>
              <p class='answer' name='search-q4'>Well, no; we generated some children that were not valid nodes because the movements that got us there were invalid!</p>
              <br/>
              <p>So, to rectify this, we would clean the tree up as follows, generating only children who were the product of legal moves:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-3.PNG' />
              </div>
              <br/>
              <p class='definition'>The set of all leaf nodes available for expansion at any given point of a search is called the <strong>frontier (AKA open-list)</strong></p>
              <br/>
              <p>Search strategies give us systematic means of exploring the frontier.</p>
              <p>So, if we wanted some &quot;black-box&quot; (don't care how it's accomplished but know what it's accomplishing) pseudocode for the general idea of search, we might say:</p>
<pre class='prettyprint'>
  ; ...for some expand-node function defined
  (defun BB-SEARCH (frontier)
      (cond
          ; Base case: ran out of nodes, return nil
          ; (failed to find goal state)
          ((null frontier) nil)
          ; Otherwise, more nodes on frontier, so keep
          ; expanding them
          (t (expand-node frontier))
      )
  )
  
  ; ...for some goal-state check and
  ; choose-node functions defined
  (defun EXPAND-NODE (frontier)
      (let* ((next (choose-node frontier)))
          (cond
              ; Base case: found goal; done!
              ((goal-state next) next)
              ; Otherwise, expand and continue
              (t BB-SEARCH (append (remove next frontier))
                                   (successors next)
              )
          )
      )
  )
  ; Pseudo-code credit to Evan Lloyd
</pre>
              
              <br/>
              <h3>Search Strategies</h3>
              <p>We'll start by going over two of the most commonly known search strategies that are known as uninformed searches.</p>
              <p class='definition'>An <strong>uninformed search</strong> is a search strategy that *only* knows how to expand and generate nodes, and detect a goal state.</p>
              <p class='definition'>Uninformed search strategies are therefore distinguished by the <strong>order</strong> in which nodes are expanded along the frontier.</p>
              <br/>
              <p>We'll begin by looking at breadth-first search.</p>
              <p class='toolkit'><strong>Breadth-first search</strong> expands nodes level-by-level, always expanding the *shallowest* node on the fronteir 
                (i.e., always expanding nodes at depth D before expanding *any* nodes at depth (D + 1).
              </p>
              <br/>
              <p>So, take this partial search tree for example, and determine its breadth-first search expansion order:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-4.PNG' />
              </div>
              <br/>
              <p class='question' name='search-q5'>What is the breadth-first expansion order for this search tree (assume an expansion order of Right, Down, Left, Up preference per level)?</p>
              <p class='answer' name='search-q5'>(1, 1), (2, 1), (3, 1), (1, 1), (2, 2), (2, 1), (3, 2), (2, 1), (3, 2), (2, 1), (2, 3)</p>
              <br/>
              <p>We'll need some tools for evaluating a particular search strategy, and we can start by defining some common characteristics of a search tree:</p>
              <p class='definition'>The <strong>branching factor (b)</strong> of a search strategy is the maximum number of children a node can generate when expanded.</p>
              <p class='definition'>The <strong>depth (d)</strong> of a search strategy is the number of nodes expanded from the root to the shallowest goal node.</p>
              <br/>
              <p>So if we expand a few more nodes on our frontier from the previous breadth-first search and then reach a goal (as seen below):</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-5.PNG' />
              </div>
              <br/>
              <p class='question' name='search-q6'>What are the values for b and d in the above tree?</p>
              <p class='answer' name='search-q6'>b = 4 because no node could possibly have more than 4 children<br/>d = 4 because it took 4 expansions from the root to reach the nearest goal state</p>
              <br/>
              <p>Using the notions of branching factor and depth, we can assess four metrics for search strategy performance:</p>
              <p class='definition'><strong>Time complexity</strong> judges how long our search will take to find a solution based on some assymptotic analysis of a problem's search space (often in terms
                of b and d).
              </p>
              <br/>
              <p>In other words, we can think of how many nodes need to be expanded before we find a solution.</p>
              <p>Let's consider the following abstract representation of a complete search tree:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-6.PNG' />
              </div>
              <br/>
              <p>How many nodes have to be expanded at depth 0?</p>
              <p>How many nodes have to be expanded at depth 1?</p>
              <p>How many nodes have to be expanded at depth 2?</p>
              <p>How many nodes have to be expanded at depth d?</p>
              <p class='question' name='search-q7'>What is the time complexity for breadth first search in big-O notation?</p>
              <p class='answer' name='search-q7'>O(1 + b + b^2 + b^3 + ... + b^d) = O(b^d) because b^d is the highest degree in the polynomial</p>
              <br/>
              <p class='definition'><strong>Space complexity</strong> is a measure of how many states we need to keep in memory at any given time for our search strategy.</p>
              <br/>
              <p>So, if we look at our abstract breadth-first search representation again, are we allowed to delete any states / branches from memory while we're exploring the frontier?</p>
              <p>How many nodes have to be generated at depth 0?</p>
              <p>How many nodes have to be generated at depth 1?</p>
              <p>How many nodes have to be generated at depth 2?</p>
              <p>How many nodes have to be generated at depth d?</p>
              <p class='question' name='search-q8'>What is the space complexity for breadth first search in big-O notation?</p>
              <p class='answer' name='search-q8'>O(b + b^2 + b^3 + ... + b^d) = O(b^d) because b^d is the highest degree in the polynomial</p>
              <br/>
              <p class='definition'>
                <strong>Completeness</strong> is a property of a search strategy that asks: &quot;If a solution exists in the search space, is this search strategy guaranteed to find it?&quot;
              </p>
              <br/>
              <p>Now consider breadth-first search and how it expands level-by-level...</p>
              <p class='question' name='search-q9'>Is breadth-first search complete?</p>
              <p class='answer' name='search-q9'>Yes! If a solution exists in the search space, then expanding level by level is sure to find it?</p>
              <br/>
              <p class='example'>Prove that breadth-first search is complete.</p>
              <br/>
              <p class='definition'><strong>Optimality</strong> is a measure for a search that asks if it will always find the *optimal* solution to a problem (i.e., the solution with the least cost).</p>
              <p>Let's look at this in a bit...</p>
              
              <br/>
              <p class='definition'><strong>Depth-first search</strong> is a search strategy that expands the deepest node in the current frontier of a search tree.</p>
              <br/>
              <p>So, take this partial search tree for example, and determine its depth-first search expansion order:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-4.PNG' />
              </div>
              <br/>
              <p class='question' name='search-q10'>What is the depth-first expansion order for this search tree (assume an expansion order of Right, Down, Left, Up preference per level)?</p>
              <p class='answer' name='search-q10'>(1, 1), (2, 1), (3, 1), (2, 1), (3, 2), (1, 1), (2, 1), (2, 2), (3, 2), (2, 1), (2, 3)</p>
              <br/>
              <p>Let's return to our maze pathfinding example...</p>
              <p class='debug'>What will happen with standard breadth-first search in our partial search tree, pictured below?</p>
              <br/>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-7.PNG' />
              </div>
              <br/>
              <p>...errr, that's kind of an issue... and rather useless in most cases!</p>
              <p>We could keep a list of our repeated states and then remember not to repeat them, but that would grow quickly for large search problems...</p>
              <p class='question' name='search-q11'>What constaint could we place on our depth-first search such that it won't have the infinite expansion?</p>
              <p class='answer' name='search-q11'>Limit the depth that it could go to some level, say, m</p>
              <br/>
              <p>Using this strategy, consider our abstract limited-depth-first search tree representation again:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-8.PNG' />
              </div>
              <br/>
              <p>What is the value of m in this tree?</p>
              <p>What are the number of nodes expanded at depth 0?</p>
              <p>What are the number of nodes expanded at depth 1?</p>
              <p>What are the number of nodes expanded at depth 2?</p>
              <p>What are the number of nodes expanded at depth m?</p>
              <p class='question' name='search-q12'>What is the time complexity for limited-depth-first search in terms of b and m?</p>
              <p class='answer' name='search-q12'>O(1 + b + b^2 + ... + b^m) = O(b^m)</p>
              <br/>
              <p>Now, let's examine the space complexity...</p>
              <p>With breadth-first search, we needed to keep our frontier in memory because we were expanding level-by-level... is the same true for depth-first search?</p>
              <p class='example'>What happens when we perform expansion #6 in our tree below?</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-9.PNG' />
              </div>
              <br/>
              <p>Ah, so as soon as we've hit our depth limit m (a leaf) we needn't remember the branch that got us there!</p>
              <p>So, let's think about an upper-bound for the number of nodes we'd need to keep in memory for any depth-first expansion:</p>
              <p class='question' name='search-q13'>What is the space complexity for limited-depth-first search in terms of b and m?</p>
              <p class='answer' name='search-q13'>E.g., with b = 3, O(1 + 3 + 6 + ... + b*m) = O(b*m), the general solution</p>
              <br/>
              <p>So we actually save space by using depth-first search (compared to breadth-first)!</p>
              <br/>
              <p class='debug'>What happens when we don't know the depth (m) to limit our search to?</p>
              <p>What if we chose to limit our search to depth 3 and a goal was at depth 4?</p>
              <p class='question' name='search-q14'>Are either depth-first or limited-depth-first search complete?</p>
              <p class='answer' name='search-q14'>No! If we always knew the value of m, then yes, but that's not true in general / reality.</p>
              <br/>
              <p>Is there a variant of depth-first search to make it complete?</p>
              <p>As it turns out, there is! It was even discovered by our own Dr. Richard Korf!</p>
              <p class='definition'><strong>Iterative-deepening depth-first search</strong> is simply repeated application limited-depth-first search where m = 0, then 1, then 2, etc. etc.</p>
              <br/>
              <p>Pictorially, then, we can think of iterative-deepening depth-first search as looking like the following, with the current depth-limit (l) starting at 1 and incrementing each time:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-10.PNG' />
              </div>
              <br/>
              <p class='question' name='search-q15'>If L is the current maximum depth of an IDDFS iteration, then what are its time and space complexities?</p>
              <p class='answer' name='search-q15'>Time complexity: O(b^L)<br/>Space complexity: O(b*L)<br/>(for same reasons as DFS)</p>
              <br/>
              <p>As it turns out, there isn't a whole lot of overhead for using IDDFS over breadth-first, though there is a bit.</p>
              <p>Notice that the two strategies are assymptotically equivalent in time complexity!</p>
              <p class='question' name='search-q16'>Why would you bother ever using IDDFS over BFS?</p>
              <p class='answer' name='search-q16'>The space complexity of IDDFS is still less than BFS, so if space is a concern, then maybe IDDFS is right for you!</p>
              <br/>
              <p>So now that we see how IDDFS works, we can answer the following question:</p>
              <p class='question' name='search-q17'>Is IDDFS complete?</p>
              <p class='answer' name='search-q17'>Yes! Why? Consider it in comparison to how BFS operates... see the similarity?</p>
              
              <br/>
              <h3>Cost &amp; Optimality</h3>
              <p>Did you know that there's a difference between the words &quot;optimal&quot; and &quot;optimum?&quot;</p>
              <p class='definition'>An <strong>optimal</strong> goal state is one with the lowest cost, that might not be the only best solution.</p>
              <br/>
              <p>For example, observe below that our two goal states have the same distance from the initial state, so their cost is the same; they are both optimal.</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-11.PNG' />
              </div>
              <br/>
              <p>Contrast this with the following situation where we have 3 goal states, but only one of them can be reached with the lowest cost.</p>
              <p class='definition'>An <strong>optimum</strong> goal state is *the* one with the lowest cost; it may have no peer goal states with an equal or lower cost.</p>
              <br/>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-12.PNG' />
              </div>
              <br/>
              <p>Let's consider a maze where we're interested in reaching an optimum goal; we'll consider the cost incurred during movement as 1 unit per move.</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-15.PNG' />
              </div>
              <br/>
              <p>So, this example allows us to ask a few questions about search strategy optimality (is a search strategy guaranteed to return the optimal / optimum solution):</p>
              <p class='question' name='search-q18'>Is DFS optimal? Is BFS? IDDFS?</p>
              <p class='answer' name='search-q18'>DFS is NOT optimal! Observe the goal state it finds in the example above. BFS and IDDFS are, however.</p>
              <br/>
              <p>That said, this whole time we've considered each of our movements in our maze pathfinder to have an equal cost, and so total cost is simply the sum of the number of movements we make.</p>
              <p>But what if we modified our costs by introducing a new element into our mazes?</p>
              <p>Let's consider clear tiles to have a travel cost of 1, and tiles with 'M' (mud) to have a movement cost of 3.</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-13.PNG' />
              </div>
              <br/>
              <p>The search tree for this scenario will look like the following, with costs along the edges:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-14.PNG' />
              </div>
              <br/>
              <p>A couple of observations to make here:</p>
              <ul class='indent-1'>
                <li><p>Is the optimal / optimum solution always just the most shallow in the search tree? When is or isn't it?</p></li>
                <li><p>Will DFS find the optimum solution? Will BFS? IDDFS?</p></li>
              </ul>
              <br/>
              <p>Hmm, so it looks like our uninformed searches aren't sufficiently empowered for optimality when we don't have uniform cost.</p>
              <p>Let's look at some ways to empower our searches!</p>
            </div>
            <hr/>
            
            
            <br/>
            <div id='informedSearch' class='scrollspy-element' scrollspy-title='Informed Search'></div>
            <h1>Informed / Heuristic Search</h1>
            <div>
              <p>
                As we saw with our uninformed search strategies, we don't know about any descendants of a particular state (states reachable through some number of legal movements) without first expanding
                the states in between.
              </p>
              <p>But, just because we don't <em>know</em> what states might be along a certain path, doesn't mean we can't make educated <em>guesses</em>.</p>
              <p class='definition'>A <strong>heuristic</strong> is essentially an estimate of the cost that we might encounter expanding a search tree along a certain path.</p>
              <br/>
              <p>The goal of heuristic search is to look at every node along the frontier, and then expand the one that it thinks will lead to an optimal goal.</p>
              <p>
                To do this, we'll construct an evaluation function, <em>f(n)</em> that takes as input a node in our search tree (a state) and computes an estimated cost of reaching a goal state
                <em>IF we decide to expand that node and follow a path from it.</em>
              </p>
              <p class='definition'><strong>Greedy / Best-first</strong> search is a heuristic search strategy that, instead of having a fixed exploration order, will choose an evaluation function that 
                *only* attempts to minimize cost to a goal.
              </p>
              <br/>
              <p>In this way, we can think of greedy search as ONLY using a heuristic, h(n), where h(n) = the estimated cost of a goal state along the path of node n.</p>
              <p>In other words, for greedy search, our evaluation function f(n) = h(n)</p>
              <p class='question' name='heuristic-q0'>What would be a good heuristic choice for our maze pathfinding example?</p>
              <p class='answer' name='heuristic-q0'>Let's start by considering the <strong>Manhattan distance</strong> between a node and the closest goal.</p>
              <br/>
              <p>So, we might say:</p>
<pre class='prettyprint'>
  Mazefinding Manhattan Distance Heuristic:
  f(n) = h(n)
       = argmin( abs(nX - iGoalX) + abs(nY - iGoalY) )
            i
            
  (where n is the player position in a node in the search tree,
  and i represents one of each goal tile in the maze) 
</pre>
              <br/>
              <p>So, seeing that in action:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-16.PNG' />
              </div>
              <br/>
              <p>Notice, using best-first search, now we found not only the optimum solution, but also didn't bother expanding the left branch, and saved a lot of searching!</p>
              <p>But now, let's throw in our Mud tiles and see how it does:</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-17.PNG' />
              </div>
              <br/>
              <p>Why didn't greedy work for us there?</p>
              <p>It didn't remember its history! It stepped on a mud tile (+3 cost) but was only interested in Manhattan distance.</p>
              <p>So how do we remember our choices as well?</p>
              
              <br/>
              <h3>A* Search</h3>
              <p>Greedy may have minimized our heuristic function but it didn't succeed with minimizing the *total* cost.</p>
              <p class='definition'><strong>A* search</strong> defines its evaluation function as a sum of a heuristic function and a &quot;history&quot; function, g(n).<br/>f(n) = g(n) + h(n)</p>
              <br/>
              <p>The history function g(n) keeps a total of the *actual* cost we've encountered along our path so far, and then adds it to our heuristic cost for what's to come.</p>
              <p>This means that we now choose to expand the node on the frontier with the smallest cost that we've already encountered, and also the smallest cost of what's yet to come!</p>
              <p>So, if we maintain our Manhattan distance heuristic from before, we see that A* now successfully navigates our maze into the optimum goal.</p>
              <div class='text-center fit-pres'>
                <img src='../../../assets/images/spring-2014/cs-161/week-2/search-18.PNG' />
              </div>
              <br/>
              <p class='question' name='heuristic-q1'>In what order will A* expand nodes in the above example (assume a left-to-right child preference for equal cost evaluations)?</p>
              <p class='answer' name='heuristic-q1'>(1, 1), (2, 1), (1, 2), (2, 2), (3, 2)</p>
              <br/>
              <p>Nice! And we managed to cut off a bunch of branches from our search tree!</p>
            </div>
            <hr/>
            
            
            <a class='btn btn-default pull-right hidden-print' href='javascript:window.print();'>
              <span class='glyphicon glyphicon-print'></span>
              &nbsp; PDF / Print
            </a>
            
          </div>
          <!-- END PRESENTATION CONTENT -->
          
          <!-- MATERIALS FROM CLASS: -->
          
            <!-- TODO: State based knowledge and operations -->
              <!-- TODO: A state is just a description of some objects in the environment -->
              <!-- TODO: A goal state is one with objects in the environment that have particular sets of features (get better definition) -->
              <!-- TODO: Getting to a state with the shortest path / least cost will be discussed -->
            <!-- TODO: Architecture made up of components that are all implemented in different possible ways -->
            <!-- TODO: Go over search spaces -->
            <!-- TODO: Go over branching factor (max number of possible moves from a given state) -->
            <!-- TODO: Search strategies -->
              <!-- TODO: NOTE: Definition difference between "expanding" a Node and "generating" a Node -->
                <!-- TODO: Expansion = generating a Node's children -->
              <!-- TODO: Depth first -->
                <!-- TODO: Not complete if the depth isn't limited -->
                <!-- TODO: Not optimal if stopping after finding a goal state -->
              <!-- TODO: Breadth first -->
                <!-- TODO: Starting with the start state, expand level-by-level -->
              <!-- TODO: Performance metrics -->
                <!-- TODO: Time -->
                <!-- TODO: Space -->
                <!-- TODO: Completeness -->
                <!-- TODO: Optimality -->
            
            
        </div>
      </div>
      <!-- END MAIN CONTENT -->
      
      
    </div>
    <!-- END WRAPPER -->
    
    <!-- BEGIN FOOTER -->
    <div id="footer">
      <div class="container">
        <div class="col-md-12 text-center">
          
        </div>
      </div>
    </div>
    <!-- END FOOTER -->
    
  </body>
</html>
