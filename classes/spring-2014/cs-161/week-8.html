
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

  <head>
    <title>Andrew Forney - UCLA CS</title>
    <link href="../../../css/bootstrap.min.css" rel="stylesheet" type="text/css">
    <link href="../../../css/magic-bootstrap.css" rel="stylesheet" type="text/css">
    <link href="../../../css/main.css" rel="stylesheet" type="text/css">
    <script src="../../../js/lib/jquery-2.0.3.min.js"></script>
    <script src="../../../js/lib/bootstrap.min.js"></script>
    <script src="../../../js/lib/expanding.js"></script>
    <script src="../../../js/display/general/general-display.js"></script>
    <script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js"></script>
  </head>
  
  <body data-spy="scroll" data-target="#scrollspy">
    
    <!-- BEGIN WRAP -->
    <div id="wrap">
      
      <!-- BEGIN NAVIGATION -->
      <nav class='navbar navbar-default' role='navigation'>
        <div class='nav-accent'></div>
        <div class='container'>
          <div class='row'>
            <div class='col-md-12'>
              <div class='navbar-header'>
                <button class='navbar-toggle' type='button' data-toggle='collapse' data-target='.navbar-main-collapse'>
                  <span class='sr-only'>Toggle Navigation</span>
                  <span class='icon-bar'></span>
                  <span class='icon-bar'></span>
                  <span class='icon-bar'></span>
                </button>
                <a class='navbar-brand' href='/~forns/'>
                  <span id='brand-text'>
                    Andrew Forney
                  </span>
                </a>
              </div>
              
              <div id='nav-main' class='collapse navbar-collapse navbar-main-collapse'>
                <ul class='nav navbar-nav navbar-right'>
                  
                  <li>
                    <a href='/~forns/about.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-user'></span>
                      </div>
                      <p class='text-center'>About</p>
                    </a>
                  </li>
                  
                  <li class='active'>
                    <a href='/~forns/classes.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-book'></span>
                      </div>
                      <p class='text-center'>Classes</p>
                    </a>
                  </li>
                  
                  <li>
                    <a href='/~forns/contact.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-comment'></span>
                      </div>
                      <p class='text-center'>Contact</p>
                    </a>
                  </li>
                  
                  <li>
                    <a href='/~forns/publications.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-file'></span>
                      </div>
                      <p class='text-center'>Publications</p>
                    </a>
                  </li>
                  
                </ul>
              </div>
            </div>
          </div>
        </div>
      </nav>
      <!-- END NAVIGATION -->
      
      <!-- BEGIN MAIN CONTENT -->
      <div id="main-content" class="container">
        <div class="row">
          
          <!-- BEGIN SCROLLSPY -->
          <div class='col-md-2 hidden-sm hidden-xs'>
            <div class="bs-sidebar hidden-print affix" role="complementary">
              <ul id='scrollspy' class="nav bs-sidenav">
              </ul>
            </div>
          </div>
          <!-- END SCROLLSPY -->
          
          <!-- BEGIN PRESENTATION CONTENT -->
          <div class='col-md-10 presentation-content' role='main'>
            
            <ol class="breadcrumb hidden-print">
              <li><a href="../../../classes.html">Classes</a></li>
              <li><a href="./cs-161.html">Spring14 CS161</a></li>
              <li class="active">Week 8</li>
            </ol>
            
            
            <div id='bayesTheorem' class='scrollspy-element' scrollspy-title='Applying Bayes&#39; Theorem'></div>
            <h1>Applying Bayes' Theorem</h1>
            <div>
              <p>Last week we just saw the tip of the iceberg as far as what Bayes' Theorem can accomplish for us.</p>
              <p>We've learned the probabilistic mechanisms that we'll use in the following lecture, so let's see how it all fits together.</p>
              <p>Shall we start with a motivating example? Don't feel bad if you don't get this at first, about 85% of medical doctors asked the same question get it wrong.</p>
              <br/>
              <p class='example'>Read the following problem description, and for each probability mentioned, formalize it into a Pr(x | y) statement (for example) and then solve for the correct quantity.
                Assume two binary variables: Test and Disease, as described by the problem.</p>
              <blockquote>
                A very rare condition, Schistosoforneymiosis, is found in about 1/1000 of those tested for it; sufferers experience a consistently wet left foot and sentient freckles.<br/><br/>
                The test is an elaborate procedure involving multiple probes, and returns an end result that is either positive (Test) or negative (&not;Test). The problem is that the tests are not perfect, but 
                95% of people who have the disease will test positive, and 2% of people who do *not* have the disease will test positive.<br/><br/>
                <strong>If a patient tests positive, what is the probability that they have the disease?</strong>
              </blockquote>
              <br/>
              <p>Let's dissect this problem and determine the proper quantities for each component to lead to our solution.</p>
              <br/>
              <p class='question' name='bayes-q0'>Translate the sentence into a Pr statement: &quot;A very rare condition, Schistosoforneymiosis, is found in about 1/1000 of those tested for it.&quot;</p>
              <p class='answer' name='bayes-q0'>Pr(Disease) = 0.001</p>
              <br/>
              <p>The above represents the <strong>prior</strong> probability that we discussed from last time, i.e., the chance that someone has the condition before accounting for any evidence.</p>
              <br/>
              <p class='question' name='bayes-q1'>Translate the sentence into a Pr statement: &quot;95% of people who have the disease will test positive.&quot;</p>
              <p class='answer' name='bayes-q1'>Pr(Test | Disease) = 0.95</p>
              <br/>
              <p>This value represents the case of a <strong>true positive</strong> since the test is positive and so is the disease.</p>
              <br/>
              <p class='question' name='bayes-q2'>Translate the sentence into a Pr statement: &quot;2% of people who do *not* have the disease will test positive.&quot;</p>
              <p class='answer' name='bayes-q2'>Pr(Test | &not;Disease) = 0.02</p>
              <br/>
              <p>This value represents the case of a <strong>false positive</strong> since the test is positive but the disease is not.</p>
              <br/>
              <p class='question' name='bayes-q3'>Translate the sentence into a Pr statement: &quot;If a patient tests positive, what is the probability that they have the disease?&quot;</p>
              <p class='answer' name='bayes-q3'>Pr(Disease | Test) = ??? (this is what we're trying to find out!)</p>
              <br/>
              <p>Alright, off to a good start! Let's write out everything that we have so far:</p>
<pre class='prettyprint'>
  Pr(Disease) = 0.001
  Pr(Test | Disease) = 0.95
  Pr(Test | &not;Disease) = 0.02
  
  Pr(Disease | Test) = ???
</pre>
              <br/>
              <p>So, if it wasn't obvious before, we need to use Bayes' Theorem in order to solve for our target quantity! Let's write out what we'll need for that:</p>
<pre class='prettyprint'>
  Pr(Disease | Test) = (Pr(Test | Disease) * Pr(Disease)) / Pr(Test)
</pre>
              <br/>
              <p>Do we have everything that we need to solve for Pr(Disease | Test)?</p>
              <p>Well, yes and no; we have everything we need to calculate what we need, explicitly. In particular, we have everything on the RHS but Pr(Test).</p>
              <p>Do we have a means of calculating the Pr(Test)?</p>
              <br/>
              <p class='question' name='bayes-q4'>Click here if you need a hint on how to calculate Pr(Test)...</p>
              <p class='answer' name='bayes-q4'>Why, the Law of Total Probability of course!<br/><code class='prettyprint'>Pr(&alpha;) = &Sigma;_i Pr(&alpha;, &beta;_i)</code>, 
                for mutually exclusive and exhaustive &beta;_i</p>
              <br/>
              <p>&quot;But Andrew, we don't have things in terms of Pr(&alpha;, &beta;), only <strong>conditions</strong>!&quot;</p>
              <p>You're correct! You even said the strategy we need to use! Let's take a look:</p>
<pre class='prettyprint'>
  ; I claim that the choice for &beta; is a partition
  ; in which each &beta;_i is mutually exclusive, do you agree?
  &beta; = {{Disease}, {&not;Disease}}
  
  ; So, by the Law of Total Probability:
  Pr(&alpha;) = &Sigma;_i Pr(&alpha;, &beta;_i)
  
  Pr(Test) = &Sigma;_i Pr(Test, &beta;_i)
           = Pr(Test, Disease) + Pr(Test, &not;Disease)
  
  ; Now, we can condition in order to get:
  Pr(Test) = Pr(Test | Disease) * Pr(Disease) +
             Pr(Test | &not;Disease) * Pr(&not;Disease)
             
  ; We do not *explicitly* have one of these quantities above,
  ; BUT, obvserve:
  Pr(&not;Disease) = 1 - Pr(Disease)
               = 1 - 0.001
               = 0.999
               
  ; Therefore:
  Pr(Test) = 0.95 * 0.001 + 0.02 * 0.999
           = 0.02093
           
  ; And now, plugging back into our original:
  Pr(Disease | Test) = (Pr(Test | Disease) * Pr(Disease)) / Pr(Test)
                     = (0.95 * 0.001) / 0.02093
                     = 0.045
</pre>
              <br/>
              <p>Wow! That's a really small chance given that our incredibly accurate test was still positive!</p>
              <p>Understanding Bayes' Theorem gives us a lot of power to detect non-obvious consequences like the above.</p>
            </div>
            <hr/>
            
            
            <br/>
            <div id='multivariate' class='scrollspy-element' scrollspy-title='Multivariate Distributions'></div>
            <h1>Multivariate Distributions</h1>
            <div>
              <p>In the previous lecture, we dealt only with two variables, even though one of them was not binary.</p>
              <p>Let's take a look at a multi-variate distribution taking a (not large) step to three variables.</p>
              <p>For this example, let's just assume we have 3 binary variables X, Y, and Z, with a joint probability table that looks like:</p>
              <table class='table table-bordered table-striped'>
                <thead>
                  <tr>
                    <th><p>X</p></th>
                    <th><p>Y</p></th>
                    <th><p>Z</p></th>
                    <th><p>Pr(X, Y, Z)</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0.2</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0.2</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0.05</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0.05</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0.05</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0.05</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0.2</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0.2</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>Well that's... err... pretty perfect... such neat probabilities! (we'd never get something so clean in the real world, of course, but play along for now...)</p>
              <p>Let's talk about a couple of observations we can make on multivariate distributions.</p>
              <p class='definition'>We can <strong>sum out (marginalize)</strong> a variable by &quot;collapsing&quot; the distribution on the remaining variables we're not summing out.
                This is formally defined as:<br/>
                P(Y) = &Sigma;_z&in;Z P(Y, z)
              </p>
              <br/>
              <p>
                Less formally, for example, if we wanted to sum out X in the above distribution, leaving a joint <strong>marginal</strong> on Y and Z, 
                we simply look for every row where Y and Z *agree* and then sum over the possible values for X
              </p>
              <p>Let's look at that in action; take a look at the following two pairs of rows, color-coded for your convenience:</p>
              <table class='table table-bordered'>
                <thead>
                  <tr>
                    <th><p>X</p></th>
                    <th><p>Y</p></th>
                    <th><p>Z</p></th>
                    <th><p>Pr(X, Y, Z)</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr class='danger'>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0.2</p></td>
                  </tr>
                  <tr class='warning'>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0.2</p></td>
                  </tr>
                  <tr class='success'>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0.05</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0.05</p></td>
                  </tr>
                  <tr class='danger'>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0.05</p></td>
                  </tr>
                  <tr class='warning'>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0.05</p></td>
                  </tr>
                  <tr class='success'>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0.2</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0.2</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>(the individual colors don't mean anything but observe the corresponding rows)</p>
              <p>These corresponding rows are where Y and Z agree (i.e., in both rows 1 and 4 Y has value y and Z has value z)</p>
              <p>So, to sum out X, we simply collapse across the two rows! It's as though X were removed entirely from the equation, just leaving us with a distribution over Y and Z.</p>
              <p class='definition'>The distribution that results from summing out a variable from a joint is called a <strong>joint marginal</strong> distribution, to illustrate that a variable was
                summed out of the original joint.</p>
              <br/>
              <p>This amounts to the following joint marginal on Y and Z with X summed out (typically written: <code class='prettyprint'>&Sigma;_X Pr(Y, Z)</code>):</p>
              <table class='table table-bordered'>
                <thead>
                  <tr>
                    <th><p>Y</p></th>
                    <th><p>Z</p></th>
                    <th><p>&Sigma;_X Pr(Y, Z)</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr class='danger'>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0.25</p></td>
                  </tr>
                  <tr class='warning'>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0.25</p></td>
                  </tr>
                  <tr class='success'>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0.25</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0.25</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>(Aside: that table looks really rasta)</p>
              <p>Any who, observe how we now have a joint distribution on Y and Z.</p>
              
              <br/>
              <h3>Independence</h3>
              <p>Let's turn our attention to independence relationships, remembering that our definition of independence was that:</p>
<pre class='prettyprint'>
  ; If Y is independent from Z, written:
  ; Y &perp; Z
  ; ...then:
  
  Pr(Y | Z) = Pr(Y)
  
  ; ...&forall; y, z
</pre>
              <br/>
              <p>So let's assess that above (if it isn't obvious)...</p>
              <p>Using Bayes' Conditioning, we see that:</p>
<pre class='prettyprint'>
  Pr(Y | Z) = Pr(Y, Z) / Pr(Z)
  
  ; Using our new joint on Y and Z, it is
  ; easy to see that:
  Pr(Y = 1, Z = 1) = 0.25
  Pr(Y = 1, Z = 0) = 0.25
  Pr(Z = 1) = Pr(Z = 0) = 0.5
  
  ; ...so:
  Pr(Y = 1 | Z = 1) = 0.25 / 0.5
                    = 0.5
                    = Pr(Y = 1)
  
  ; ...and we can also show that:
  Pr(Y = 1 | Z = 0) = Pr(Y = 1 | Z = 1)
                    = Pr(Y = 1)
  
  &there4; Y &perp; Z
</pre>
              <br/>
              <p>Neat! So this means that knowing something about Z tells me nothing about the state of Y...</p>
              <p>E.g., knowing that it's Tuesday tells me nothing about my chances of winning the lottery.</p>
              <br/>
              <p>If we were so inclined, we could repeat the process of summing out Z from the original joint distribution to get:</p>
              <table class='table table-bordered'>
                <thead>
                  <tr>
                    <th><p>X</p></th>
                    <th><p>Y</p></th>
                    <th><p>&Sigma;_Z Pr(X, Y)</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0.4</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0.1</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0.1</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0.4</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>Performing the same test for independence, we would find that:</p>
<pre class='prettyprint'>
  Pr(Y | X) = Pr(Y, X) / Pr(X)
  
  Pr(Y = 1 | X = 1) = 0.8
                    &ne; Pr(Y = 1)
                    
  &there4; Y is dependent on X
</pre>
              
              <br/>
              <h3>Conditional Independence</h3>
              <p>A topic we haven't talked about yet is a peculiar phenomenon known as conditional independence.</p>
              <p>As it turns out, it's possible for two variables X and Y to be independent ONLY after we've observed (conditioned upon) some other variable(s) Z.</p>
              <p>To compare:</p>
              <table class='table table-bordered table-striped'>
                <thead>
                  <tr>
                    <th><p>Relationship</p></th>
                    <th><p>Description</p></th>
                    <th><p>Written</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th><p>Independence</p></th>
                    <td><p>If we have knowledge that X occurred, and that tells us nothing about whether or not Y occured, then X is independent of Y and vice versa.</p></td>
                    <td><p>X &perp; Y</p></td>
                  </tr>
                  <tr>
                    <th><p>Conditional Independence</p></th>
                    <td><p>X and Y are conditionally independent if and only if, given information about Z, having knowledge about X tells us nothing about whether or not Y occured; i.e., X and Y may
                      not be independent until *after* conditioning on a third variable / set of variables Z.</p></td>
                    <td class='col-md-2'><p>X &perp; Y | Z</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>So you might be curious... what's an intuitive interpretation of conditional independence?</p>
              <p>Here are some good good scenarios that explain it, using dice, because every statistician loves dice for some reason:</p>
              <table class='table table-bordered table-striped'>
                <thead>
                  <tr>
                    <th><p>Concept</p></th>
                    <th><p>Description</p></th>
                    <th class='col-md-3'><p>Written</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><p>Independent</p></td>
                    <td>You roll two dice: A and B. Knowing the outcome of A tells you nothing about the outcome of B.</td>
                    <td>A &perp; B</td>
                  </tr>
                  <tr>
                    <td><p>Independent, but Conditionally Dependent</p></td>
                    <td><p>You roll two dice: A and B. If I tell you that the sum (S) of the two dice's totals is even, then knowing the value of either A or B actually *does* tell me something about the other,
                      even though A and B are independent on their own.</p></td>
                    <td><p>A /&perp; B | S<br/>(that's meant to be a little line through the &perp; symbol but I can't find that in unicode so... use your imagination)</p></td>
                  </tr>
                  <tr>
                    <td><p>Independent, and Conditionally Independent</p></td>
                    <td><p>You roll two dice: A and B. If I tell you that the result (R) of A is not 3 and the result of B is not 2, I learn new information about each, but nothing that connects the
                      two outcomes. So, the dice rolls are independent AND conditionally independent based on the new information.</p></td>
                    <td><p>A &perp; B<br/>A &perp; B | R</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p class='definition'>Two other, equivalent, ways to think about conditional independence are that:<br/><code class='prettyprint'>Pr(X | Y, Z) = Pr(X | Z)</code> if X &perp; Y | Z<br/><br/>
                <code class='prettyprint'>Pr(X, Y | Z) = Pr(X | Z) * Pr(Y | Z)</code> if X &perp; Y | Z
              </p>
              <br/>
              <p>So, let's take a look at a probability distribution that might elicit conditional independence relationships.</p>
              <p>In the book, the example given is for observing relationships between three dental-exam variables:</p>
              <ul class='indent-1'>
                <li><p><strong>Toothache:</strong> subjective patient reported tooth pain; typically a sign of a cavity</p></li>
                <li><p><strong>Cavity:</strong> whether the dentist found a cavity</p></li>
                <li><p><strong>Catch:</strong> whether the dentist felt their metal teeth cleaner get caught on a tooth of interest; typically a sign of a cavity</p></li>
              </ul>
              <br/>
              <p>...an example, which, frankly sounds a bit odd, but we'll run with it because it's the numbers we're interested in.</p>
              <p>Let's look at that distribution:</p>
              <table class='table table-bordered table-striped'>
                <thead>
                  <tr>
                    <th><p>Toothache</p></th>
                    <th><p>Cavity</p></th>
                    <th><p>Catch</p></th>
                    <th><p>Pr(Toothache, Cavity, Catch)</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0.576</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0.144</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0.008</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0.072</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0.064</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0.016</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0.012</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0.108</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>We can see that the instances of toothache are usually indicative of a cavity, which is also a cause of catches.</p>
              <p>Now, the example makes the following observations (by example design, of course):</p>
              <ul class='indent-1'>
                <li><p>We hypothesize that cavities cause toothaches and catches.</p></li>
                <li><p>If this is the case, then knowing that someone has a cavity immediately tells us that they are likely to have a toothache and catch.</p></li>
                <li><p>Furthermore, if we knew someone had a cavity, does knowing that they also have a toothache tell us more than we already know? No!</p></li>
              </ul>
              <br/>
              <p class='question' name='condind-q0'>What conditional independence relationship are the above observations making?</p>
              <p class='answer' name='condind-q0'>That a toothaches and catches are independent once we know whether or not a tooth has a cavity!</p>
              <br/>
              <p class='question' name='condind-q1'>Rationalize: why is this a conditional independence relationship and not an absolute independence relationship?</p>
              <p class='answer' name='condind-q1'>Because *without* knowing whether or not there is a cavity, we *do* know more about the probability of a catch given that there is a toothache, and vice versa;
                we say that <strong>information flows</strong> from one variable to the other without the conditioning on Cavity.</p>
              <br/>
              <p class='question' name='condind-q2'>Express this conditional independence in probability notation; how will we go about illustrating this independence relationship from the distribution?</p>
              <p class='answer' name='condind-q2'>Pr(Toothache | Catch, Cavity) = Pr(Toothache | Cavity); we'll solve this below!</p>
              <br/>
              <p>Alright, now that we have our query in mind, let's observe the following:</p>
<pre class='prettyprint'>
  ; Goal:
  Pr(Toothache | Catch, Cavity) = Pr(Toothache | Cavity)
  
  ; We have neither of those tables, but we can compute them!
  ; Let's use Bayes' Conditioning and marginalization!
    Pr(Toothache | Catch, Cavity)
  = Pr(Toothache, Catch, Cavity) / Pr(Catch, Cavity)
  
    Pr(Toothache | Cavity)
  = Pr(Toothache, Cavity) / Pr(Cavity)
</pre>
              <br/>
              <p>Marginalizing from the joint distribution, we can get our two joint-marginals on Pr(Catch, Cavity) and Pr(Toothache, Cavity):</p>
              <div class='row'>
                <div class='col-md-6'>
                  <table class='table table-bordered table-striped'>
                    <thead>
                      <tr>
                        <th><p>Cavity</p></th>
                        <th><p>Catch</p></th>
                        <th><p>Pr(Cavity, Catch)</p></th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td><p>0</p></td>
                        <td><p>0</p></td>
                        <td><p>0.64</p></td>
                      </tr>
                      <tr>
                        <td><p>0</p></td>
                        <td><p>1</p></td>
                        <td><p>0.16</p></td>
                      </tr>
                      <tr>
                        <td><p>1</p></td>
                        <td><p>0</p></td>
                        <td><p>0.02</p></td>
                      </tr>
                      <tr>
                        <td><p>1</p></td>
                        <td><p>1</p></td>
                        <td><p>0.18</p></td>
                      </tr>
                    </tbody>
                  </table>
                </div>
                <div class='col-md-6'>
                  <table class='table table-bordered table-striped'>
                    <thead>
                      <tr>
                        <th><p>Toothache</p></th>
                        <th><p>Cavity</p></th>
                        <th><p>Pr(Toothache, Cavity)</p></th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td><p>0</p></td>
                        <td><p>0</p></td>
                        <td><p>0.72</p></td>
                      </tr>
                      <tr>
                        <td><p>0</p></td>
                        <td><p>1</p></td>
                        <td><p>0.08</p></td>
                      </tr>
                      <tr>
                        <td><p>1</p></td>
                        <td><p>0</p></td>
                        <td><p>0.08</p></td>
                      </tr>
                      <tr>
                        <td><p>1</p></td>
                        <td><p>1</p></td>
                        <td><p>0.12</p></td>
                      </tr>
                    </tbody>
                  </table>
                </div>
              </div>
              <br/>
              <p>As a final ingredient for our proof of conditional independence, I'll save you the meager effort and tell you that: Pr(Cavity) = 0.2</p>
              <p>Now, we just need to compute the two conditional distributions; we'll start with the Pr(Toothache | Catch, Cavity) from the joint:</p>
              <table class='table table-bordered table-striped'>
                <thead>
                  <tr>
                    <th><p>Toothache</p></th>
                    <th><p>Cavity</p></th>
                    <th><p>Catch</p></th>
                    <th><p>Pr(Toothache, Cavity, Catch)</p></th>
                    <th><p>Pr(Toothache | Cavity, Catch)</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0.576</p></td>
                    <td><p>0.9</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0.144</p></td>
                    <td><p>0.9</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0.008</p></td>
                    <td><p>0.4</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0.072</p></td>
                    <td><p>0.4</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0.064</p></td>
                    <td><p>0.1</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0.016</p></td>
                    <td><p>0.1</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0.012</p></td>
                    <td><p>0.6</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0.108</p></td>
                    <td><p>0.6</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>Interesting, looks like we have some flavor of uniformity going on here... hmmm... Let's compute Pr(Toothache | Cavity):</p>
              <table class='table table-bordered table-striped'>
                <thead>
                  <tr>
                    <th><p>Toothache</p></th>
                    <th><p>Cavity</p></th>
                    <th><p>Pr(Toothache | Cavity)</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0.9</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0.4</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0.1</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0.6</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>Aha! We see that the two are in fact equivalent, regardless of what we know about a Catch! Taking a closer look:</p>
<pre class='prettyprint'>
    Pr(&not;Toothache | &not;Catch, &not;Cavity)
  = Pr(&not;Toothache | Catch, &not;Cavity)
  = Pr(&not;Toothache | &not;Cavity)
  = 0.9
  
    Pr(&not;Toothache | &not;Catch, Cavity)
  = Pr(&not;Toothache | Catch, Cavity)
  = Pr(&not;Toothache | Cavity)
  = 0.4
  
    Pr(Toothache | &not;Catch, &not;Cavity)
  = Pr(Toothache | Catch, &not;Cavity)
  = Pr(Toothache | &not;Cavity)
  = 0.1
  
    Pr(Toothache | &not;Catch, Cavity)
  = Pr(Toothache | Catch, Cavity)
  = Pr(Toothache | Cavity)
  = 0.6
  
  &there4; Pr(Toothache | Catch, Cavity) = Pr(Toothache | Cavity)
  &there4; Toothache &perp; Catch | Cavity
</pre>
              <br/>
              <p>Whew! That was a lot of work! But we'll see in a moment that this is all worth it...</p>
            </div>
            <hr/>
            
            
            <br/>
            <div id='bayesnet' class='scrollspy-element' scrollspy-title='Bayesian Networks'></div>
            <h1>Bayesian Networks</h1>
            <div>
              <p>Finally, we get to Bayesian Networks! I know you paid for your whole seat this discussion but you're only going to need the edge!</p>
              <p>To commemorate the occasion, I've created an image deserving of the day's grandeur that I've wanted to make for some time now.</p>
              <br/>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-8/bayeswatch.png' />
              </div>
              <br/>
              <p>That, of course, is a picture of Father Thomas Bayes, the one responsible for the eponymous theorem, photoshopped onto David Hasselhoff's body from the hit drama series Baywatch from 1989.</p>
              <p>I haven't had any means of working this joke into conversation, and the previous part of this lecture's been dull, so here we are...</p>
              <br/>
              <p>You awake again? OK...</p>
              <p>In the last section we derived that <code class='prettyprint'>Toothache &perp; Catch | Cavity</code> from our dentist example.</p>
              <p>Let's look at an interesting consequence:</p>
              <p class='question' name='bayesnet-q0'>Give the chain-rule factorization of the joint probability table: <code class='prettyprint'>Pr(Toothache, Catch, Cavity)</code></p>
              <p class='answer' name='bayesnet-q0'>Pr(Toothache, Catch, Cavity) = Pr(Toothache | Catch, Cavity) * Pr(Catch | Cavity) * Pr(Cavity)</p>
              <br/>
              <p>OK, and based on our conditional independence relation found from the previous section, can we reduce this to anything simpler?</p>
              <br/>
<pre class='prettyprint'>
  ; Yes! Since:
  Toothache &perp; Catch | Cavity
  
  ; Then:
    Pr(Toothache | Catch, Cavity)
  = Pr(Toothache | Cavity)
  
  ; And so our factorization:
    Pr(Toothache, Catch, Cavity)
  = Pr(Toothache | Catch, Cavity) * Pr(Catch | Cavity) * Pr(Cavity)
  = Pr(Toothache | Cavity) * Pr(Catch | Cavity) * Pr(Cavity)
</pre>
              <br/>
              <p>Hmm, interesting... so we took a big joint probability table (Pr(Toothache, Catch, Cavity)) and broke it down into 3 smaller tables!</p>
              <p>There's something else interesting about our factorization...</p>
              <p>Remembering that we presumed that both toothaches and catches were indicators of cavities:</p>
              <p class='question' name='bayesnet-q1'>Thinking about cause-effect relationships, what's interesting about the tables: Pr(Toothache | Cavity) and Pr(Catch | Cavity)</p>
              <p class='answer' name='bayesnet-q1'>They are tables structured according to a possible effect *given* that effect's possible causes!</p>
              <br/>
              <p>Alright, we're almost ready to hit the punchline... one final example.</p>
              <p class='example'>Using the following distribution properties, compare the full joint probability distribution's size (over all 5 variables) to its chain-rule factorization:</p>
<pre class='prettyprint'>
  ; Say we have 5 binary variables:
  A, B, C, D, E
  
  ; They elicit a joint probability distrbution:
  Pr(A, B, C, D, E)
  
  ; [?] How many rows are in this table?
  
  ; Now, say we had independence relationships
  ; that allowed us to factor that joint distribution
  ; into the following:
    Pr(A, B, C, D, E)
  = Pr(A) * Pr(B) * Pr(C) * Pr(D) * Pr(E)
  
  ; [?] How many rows are in each of these smaller
  ; tables?
  
  ; [?] Do we witness a difference between the number
  ; of rows in the full joint vs. the factorization?
</pre>
              <br/>
              <p>Yes! Big savings, in fact!</p>
              <p>There are 32 rows in the full joint, but only 10 rows in the individual, factored tables!</p>
              <p class='definition'><strong>Bayesian Networks</strong> attempt to exploit independence relationships to reduce massive joint probability distributions to smaller tables, all while
                capturing the intuitive notions of cause and effect to structure the independences.</p>
              <br/>
              <p>In the words of the great Judea Pearl, who was instrumental in their development, Bayesian networks are, &quot;A parsimonious representation&quot; of the joint distribution.</p>
              <p>They're just really intuitive data structures!</p>
              <p>Here's a simple Bayesian network representing our dentist's problem:</p>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-8/bayesnet-0.PNG' />
              </div>
              <br/>
              <p>There's a lot to note about Bayesian networks:</p>
              <p class='definition'>Bayesian networks are a class of graphs called <strong>directed, acyclic graphs (DAGs)</strong>, meaning that the edges between nodes are directed and they form 
                no cycles (derp).</p>
              <p class='definition'>The network's <strong>nodes</strong> are the variables in our distributions.</p>
              <p class='definition'>The network's <strong>edges</strong> represent <strong>dependence</strong> relationships, i.e., two connected nodes are dependent upon one another.</p>
              <p class='definition'>Edges illustrate only a dependence; effects can have multiple causes and causes can have multiple effects, any one of which illustrates an influence that may be negative 
                OR positive (correlationally).</p>
              <p class='definition'>The <strong>edge directions</strong> are merely tools to represent the independence relationships, but are structured with potential causes pointing to potential effects.</p>
              <br/>
              <p>Why do we say &quot;potential causes&quot; and &quot;potential effects?&quot;</p>
              <p>Because our data is correlational!</p>
              <a href='http://gizmodo.com/5977989/internet-explorer-vs-murder-rate-will-be-your-favorite-chart-today' target='_blank'>
                <div class='fit-pres text-center'>
                  <img src='../../../assets/images/spring-2014/cs-161/week-8/correlation.jpg' />
                </div>
              </a>
              <br/>
              <p>We would need stronger tools to claim confidence in a true causal relation, but for the purposes of Bayesian networks, it is convenient and intuitive to draw arrows from causes to effects.</p>
              <p>The reason being the same as in our dentist example: if we know that someone has a cavity (the cause), then knowing that they also have a toothache doesn't tell us more about their chance
                of having a catch.</p>
              <p>That is, as soon as we know the state of the causes, we don't get any new information about the effects!</p>
              <p>Thus, we glean some notion of the <strong>semantics / meaning</strong> implicit within Bayesian networks:</p>
              <div class='definition'>
<pre class='prettyprint'>
  ; The joint distribution:
    Pr(X1, X2, ..., Xn)
  
  ; ...can be factored using independence
  ; relationships to:
  = Pr(X1 | Parents(X1)) * Pr(X2 | Parents(X2)) * ...
  
  ; ...which semantically represents the putative
  ; relationship between causes and effects:
  = Pr(Effect1 | Causes(Effect1)) * (Effect2 | Causes(Effect2)) * ...
  
  = &Pi;_X&in;Vars Pr(X | Parents(X))
</pre>
              </div>
              <br/>
              <p class='definition'>This factorization allows us to express the large joint distribution in terms of <strong>conditional probability tables (CPTs)</strong> of an effect given its parents.</p>
              <br/>
              <p>For our dentist example, the CPTs would look like:</p>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-8/bayesnet-1.PNG' />
              </div>
              <br/>
              <p class='question' name='bayesnet-q2'>Using the above CPTs, compute: Pr(Cavity = 0, Toothache = 1, Catch = 0)</p>
              <div class='answer' name='bayesnet-q2'>
<pre class='prettyprint'>
    Pr(Cavity = 0, Toothache = 1, Catch = 0)
  = Pr(Toothache = 1 | Cavity = 0) * Pr(Catch = 0 | Cavity = 0) * Pr(Cavity = 0)
  = 0.1 * 0.8 * 0.8
  = 0.064
</pre>
              </div>
              <br/>
              <p>Since our CPTs describe a world in which the effects can be screened off from other portions of the network by knowing about their causes, we say that Bayesian networks make the
                Markovian assumption:</p>
              <p class='definition'>The <strong>Markovian assumption</strong> states that every node is independent of its non-descendents given its parents.</p>
              <br/>
              <p>(non-descendents of node X are defined as any node that you could not reach taking a directed path starting at X)</p>
              <p>You should repeat that a couple times... make it your mantra.</p>
              <p class='example'>What are the Markovian assumptions implicit in the following Bayesian network?</p>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-8/bayesnet-2.PNG' />
              </div>
              <p class='question' name='bayesnet-q3'>Click for answer.</p>
              <div class='answer' name='bayesnet-q3'>
<pre class='prettyprint'>
  Earthquake &perp; Burglary
  
  Radio &perp; Alarm | Earthquake
  Radio &perp; Call | Earthquake
  Radio &perp; Burglary | Earthquake
  
  Burglary &perp; Radio
  
  Alarm &perp; Radio | Earthquake, Burglary
  
  Call &perp; Radio | Alarm
  Call &perp; Earthquake | Alarm
  Call &perp; Burglary | Alarm
</pre>
              </div>
              <br/>
              <p>We notice a couple things about our Markovian assumptions:</p>
              <ol class='indent-1'>
                <li><p>There are some redundancies (e.g., Burglary &perp; Radio, but we also have that Radio &perp; Burglary | Earthquake)</p></li>
                <li><p>There are some independence relations that they miss...</p></li>
              </ol>
              <br/>
              <p>Let's look at a formalization of independences implicit within our Bayesian Network next.</p>
            </div>
            <hr/>
            
            
            <br/>
            <div id='dsep' class='scrollspy-element' scrollspy-title='D-Separation'></div>
            <h1>D-Separation</h1>
            <div>
              <p>Let's start off by considering 3 different simple Bayesian networks and observe how we can generalize their characteristics.</p>
              <p>We'll examine Bayesian networks as a plumbing network where:</p>
              <ol class='indent-1'>
                <li><p>Nodes are &quot;valves&quot; that allow water (information) to flow through</p></li>
                <li><p>Edges are &quot;pipes&quot; that connect the valves</p></li>
                <li><p>Dependence is the process of determining whether water (information) could flow from some set of valves (variables) X to some other set of valves Y, accounting for whether or not
                  any valves along any pipe-path are closed or not (our evidence, some set of variables Z)</p></li>
              </ol>
              <br/>
              <p>We'll start with a familar problem:</p>
              <p class='definition'>A valve Z is <strong>divergent / a fork</strong> along some path if it is a common cause of two effects, X and Y.<br/>
                <code class='prettyprint'>X &larr; Z &rarr; Y</code></p>
              <br/>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-8/bayesnet-0.PNG' />
              </div>
              <br/>
              <p>Divergent nodes are sometimes referred to as common causes. Here, we see that if we know whether or not someone has a cavity, then information does NOT flow from toothache to catch.</p>
              <p>So, the rule for divergent valves along some path is that when we have a triplet X &larr; Z &rarr; Y, given Z blocks information flow from X to Y.</p>
              <br/>
              <p class='definition'>A valve Z is <strong>sequential / a chain</strong> along some path if some other variable X is its cause and Z has some effect Y.<br/>
                <code class='prettyprint'>X &rarr; Z &rarr; Y</code> OR <code class='prettyprint'>Y &larr; Z &larr; X</code></p>
              <br/>
              <p>Here's an example path of a chain:</p>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-8/bayesnet-3.PNG' />
              </div>
              <br/>
              <p>Here, knowing that someone has tar in their lungs means that we no longer get information flowing from any knowledge that the person smoked to whether they'll have lung cancer.</p>
              <p>So, the rule for chain valves along some path is that for triple X &rarr; Z &rarr; Y, given Z blocks information flow from X to Y.</p>
              <br/>
              <p class='definition'>A valve Z is <strong>convergent / a sink</strong> along some path if it is the common effect of two causes, X and Y.<br/>
                <code class='prettyprint'>X &rarr; Z &larr; Y</code>
              </p>
              <br/>
              <p>Convergent nodes are a special beast, so let's look at the following scenario:</p>
              <blockquote>
                Consider the scenario where a bell rings if and only if the outcome of two coin flips (from two separate coins) are identical (i.e., both heads or both tails).
              </blockquote>
              <br/>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-8/bayesnet-4.PNG' />
              </div>
              <br/>
              <p class='question' name='bayesnet-q4'>If I know whether or not the bell rings, does information flow from coinflip 1 to coinflip 2?</p>
              <p class='answer' name='bayesnet-q4'>Yes! If I know the bell rang, then knowing that coinflip 1 came up heads tells me exactly that coinflip 2 must have been heads!</p>
              <br/>
              <p class='question' name='bayesnet-q5'>If I DO NOT know whether or not the bell rang, does information flow from coinflip 1 to coinflip 2?</p>
              <p class='answer' name='bayesnet-q4'>No! Just by knowing that coinflip 1 came up heads tells me nothing about coinflip 2.</p>
              <br/>
              <p>Now consider the following scenario:</p>
              <blockquote>
                A bell rings if and only if the outcome of two coin flips (from two separate coins) are identical (i.e., both heads or both tails). Additionally, if the bell rings, our elderly butler,
                Jeeves, has an 80% chance of bringing tea to our room.
              </blockquote>
              <br/>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-8/bayesnet-5.PNG' />
              </div>
              <br/>
              <p class='question' name='bayesnet-q5'>If I know whether or not Jeeves brings tea, does information flow from coinflip 1 to coinflip 2?</p>
              <p class='answer' name='bayesnet-q5'>Yes! If I know that Jeeves brought tea, then I know that the bell *probably* rang! As such, I gain information about coinflip 2 based on whatever coinflip 1 was.</p>
              <br/>
              <p class='question' name='bayesnet-q6'>If I DO NOT know whether or not the bell rang AND I DO NOT know whether Jeeves brought tea, does information flow from coinflip 1 to coinflip 2?</p>
              <p class='answer' name='bayesnet-q6'>No!</p>
              <br/>
              <p>So, the rule for convergent valves is the following: for common effect Z in configuration: X &rarr; Z &larr; Y, then Z is blocked if neither it NOR any of its descendants are given!</p>
              
              <br/>
              <h3>d-Separation</h3>
              <p>Now, we can think about these three cases of valves as being elements along a path in a Bayesian network.</p>
              <p class='definition'><strong>d-separation</strong> (directional separation) allows us to determine all independence relations implicit from the graph just by looking at its structure.</p>
              <br/>
              <p>The rules of d-separation are as follows:</p>
<pre class='prettyprint'>
  ; To determine if some set of nodes X is
  ; d-separated from some set of nodes Y by
  ; some (possibly empty) set of nodes Z:
  
  trace ALL undirected paths from each X to each Y
      for each valve V on the current path
          if V is a fork and given (i.e., V&in;Z)
              then this path is blocked
          if V is a chain and given
              then this path is blocked
          if V is a sink and neither it NOR its
            descendents are given
              then this path is blocked
      
  if ALL paths blocked from each X to each Y given Z
      then X is d-separated from Y given Z
  else there was an open path
      then X is NOT d-separated from Y given Z
</pre>
              <br/>
              <p>It might look complicated, but the fact that it's a strictly graphical criterion for independence makes it easy to trace.</p>
              <p class='definition'>We use the following notation to establish d-separation relationships: 
                <code class='prettyprint'>d-sep(X, Z, Y) &equiv; X &perp; Y | Z</code></p>
              <br/>
              <p>Let's do a bunch of examples:</p>
              <br/>
              <p class='example'>Use the following Bayesian network to determine if each variable set X is separated from Y given Z. If it is not, show the open path.</p>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-8/bayesnet-6.PNG' />
              </div>
              <br/>
              <p class='question' name='bayesnet-q7'>Is <code class='prettyprint'>d-sep( {V}, {}, {T} )</code></p>
              <p class='answer' name='bayesnet-q7'>No! They're directly connected ya dingus! The open path is simply: V &rarr; T</p>
              <br/>
              <p class='question' name='bayesnet-q8'>Is <code class='prettyprint'>d-sep( {V}, {T}, {O} )</code></p>
              <p class='answer' name='bayesnet-q8'>Yes! The only path between V and O is: V &rarr; T &rarr; O, in which T is a chain that is given, so that path is blocked.</p>
              <br/>
              <p class='question' name='bayesnet-q9'>Is <code class='prettyprint'>d-sep( {T}, {O}, {S} )</code></p>
              <p class='answer' name='bayesnet-q9'>No! There are two paths from T to S: one that goes through O to D (and is blocked), but the other which goes through O and up to C (which is open because
                O is a sink and is given). So, the open path is: T &rarr; O &larr; C &larr; S</p>
              <br/>
              <p class='question' name='bayesnet-q10'>Is <code class='prettyprint'>d-sep( {T}, {D}, {S} )</code></p>
              <p class='answer' name='bayesnet-q10'>No! In fact, now *both* paths from T to S are open. They are: <br/>T &rarr; O &larr; C &larr; S and additionally,<br/>T &rarr; O &rarr; D &larr; B &larr; S</p>
              <br/>
              <p class='question' name='bayesnet-q11'>Is <code class='prettyprint'>d-sep( {T}, {}, {S} )</code></p>
              <p class='answer' name='bayesnet-q11'>Yes! Now all paths are blocked</p>
              <br/>
              <p class='question' name='bayesnet-q12'>Is <code class='prettyprint'>d-sep( {T, X}, {B}, {S} )</code></p>
              <p class='answer' name='bayesnet-q12'>No! A path is open from: X &larr; O &larr; C &larr; S</p>
              <br/>
              <p>And now, one final, conceptual question:</p>
              <p class='question' name='bayesnet-q13'>Are the independence relationships given by the Markovian assumptions implied by d-separation? Is the reverse true?</p>
              <p class='answer' name='bayesnet-q13'>The Markovian independence relationships are implied by d-separation.</p>
            </div>
            <hr/>
            
            
            <br/>
            <div id='inference' class='scrollspy-element' scrollspy-title='Inference'></div>
            <h1>Inference</h1>
            <div>
              <p>So now that we have our Bayesian networks defined and we know what independence relationships they claim... what do we do with them?</p>
              <p>Well, we can ask them questions of course!</p>
              <p>For our networks, those questions will be of the form: &quot;What's the probability of witnessing events Q given that I've seen evidence e?&quot; (where Q is a set of variables and e is an
                instantiation of evidence)</p>
              <br/>
              <p>This problem would be pretty easy if we had our joint distribution... but we don't with Bayesian networks! We just have CPTs.</p>
              <p>Furthermore, we want to avoid reconstructing the joint because it might be massive!</p>
              <p class='definition'><strong>Variable elimination</strong> is an inference strategy designed to use only our CPTs to pose queries on our network.</p>
              <p class='debug'>WARNING: The topic of Bayesian network inference is massive (there's a whole class on it, 262a, which I genuinely suggest taking!) and so this section is very hand-wavy.</p>
              <br/>
              <p>The idea of variable elimination is that, with some query and evidence variables, we want to find Pr(Q | e).</p>
              <p>For example, we might be interested in Pr(Cancer | Smoking = true)</p>
              <p>Above, Q = {Smoking} and e = {Smoking = true}</p>
              <p>Let's use this as our example! (example credit to Dr. Darwiche, and my notebook for surviving this long to be useful)</p>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-8/bayesnet-7.PNG' />
              </div>
              <br/>
              <p><strong>Step One:</strong> Zero out CPT rows that are inconsistent with the evidence (these inconsistent rows are often just omitted entirely)</p>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-8/bayesnet-8.PNG' />
              </div>
              <p><strong>Step Two:</strong> Choose an &quot;elimination order&quot;</p>
              <p>Our goal is to get ONE CPT at the end that only mentions variables in Q; we do this by successively multiplying table values and summing out irrelevant variables.</p>
              <p>The order in which you do this matters greatly for computational complexity, but that discussion is omitted herein; for now, let's just choose an order:</p>
<pre class='prettyprint'>
  ; &Pi; is used to designate an ordering
  &Pi; = {A, B, C}
  
  ; Meaning A first, then B, etc.
</pre>
              <br/>
              <p>Since A is up first, this means we multiply all the CPTs mentioning A and get back a <strong>factor</strong>, which is simply some function mapping variables to some positive numbers.</p>
              <p>Factors are NOT distributions, but simply the intermediary computational elements of variable elimination.</p>
              <p>So, the two CPTs mentioning A in our example are: Pr(A) and Pr(B | A) so:</p>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-8/bayesnet-9.PNG' />
              </div>
              <br/>
              <p>Since we've now dealt with all of the tables mentioning A, we can <strong>eliminate</strong> it by summing out, giving us:</p>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-8/bayesnet-10.PNG' />
              </div>
              <br/>
              <p>Our next variable to eliminate is B, which is mentioned in both our current factor (&Sigma;_A Pr(B | A) * Pr(A)) and Pr(C | B), so do the same thing there!</p>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-8/bayesnet-11.PNG' />
              </div>
              <br/>
              <p>Almost there! But now B is irrelevant, so we can sum it out to get:</p>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-8/bayesnet-12.PNG' />
              </div>
              <br/>
              <p>So close! But let's look at what we have so far:</p>
<pre class='prettyprint'>
  ; The term currently in our table:
  &Sigma;_B Pr(B | C) * &Sigma;_A Pr(B | A) * Pr(A)
  
  ; ...is really just the joint with B and A summed out!
  ; (after accounting for evidence A = true, of course)
  
  = &Sigma;_B &Sigma;_A Pr(B | C) * Pr(B | A) * Pr(A)
  = &Sigma;_B &Sigma;_A Pr(A, B, C)
</pre>
              <br/>
              <p>So, since our final factor has only C remaining, and accounted for A = true, the values it represents are a marginal on C and A = true!</p>
<pre class='prettyprint'>
  ; So, from our final factor:
  Pr(C = true, A = true) = 0.192
  Pr(C = false, A = true) = 0.408
  
  &there4; Pr(A = true) = 0.600
  
  ; Now we have everything we need to compute
  ; our query:
  ; Pr(Q | e) = Pr(C | A = true)
  
  ; ...using Bayes conditioning:
  Pr(C = true | A = true) = Pr(C = true, A = true) / Pr(A = true)
                          = 0.192 / 0.600
                          = 0.32
                          
  Pr(C = false | A = true) = Pr(C = false, A = true) / Pr(A = true)
                           = 0.408 / 0.600
                           = 0.68
</pre>
              <br/>
              <p>And that's that!</p>
              <p>Like I say, there's a whole class on Bayesian network inference, so this is but a small example and one strategy that can be used for such.</p>
              <p>Thus concludes our brief tour of Bayesian networks... hope you enjoyed the ride!</p>
            </div>
            <hr/>
            
            
            <br/>
            <div id='homework4' class='scrollspy-element' scrollspy-title='Homework 4'></div>
            <h1>Homework 4</h1>
            <div>
              <p>Now that I've had a chance to look through the homework and do some solutions, I have a few hints that might be helpful in crafting your own!</p>
              <table class='table table-striped table-bordered'>
                <thead>
                  <tr>
                    <th><p>Problem</p></th>
                    <th><p>Tip</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><p>1. SF-UNIFY</p></td>
                    <td><p>Remember, to watch out for ordering in frames and lists of frames! The book algorithm attempts to unify slots in frames, and frames in lists, *in the same order*, but
                      our frames don't care about that!</p><br/><p>As such, you might find it useful to have multiple helper functions to handle the cases mentioned by the book, accounting
                      for frames that might successfully unify but that simply aren't ordered the same.</p></td>
                  </tr>
                  <tr>
                    <td><p>2. SF-SUBST</p></td>
                    <td><p>As you might have already seen, the solution to this one looks a lot like a couple of other functions we've done in the past that have to replace fillers with other values...</p></td>
                  </tr>
                  <tr>
                    <td><p>3. SF-INFER</p></td>
                    <td><p>Fairly straightforward once you have SF-UNIFY working, since it does most of the heavy lifting...</p></td>
                  </tr>
                  <tr>
                    <td><p>4. FORWARD1</p></td>
                    <td><p>Be careful to avoid infinite loops with the facts that you learn! Hmm, how to screen the results of SF-INFER?</p></td>
                  </tr>
                  <tr>
                    <td><p>5. BACKWARD1</p></td>
                    <td><p>Now much easier since we're only doing the first step of backward chaining, this one too is reduced to a search problem on the first consequent that successfully unifies to the
                      query.</p></td>
                  </tr>
                  <tr>
                    <td><p>6. C-GEN</p></td>
                    <td><p>Deceptively intricate; make sure you read through the examples and piece together a plan of attack. I suggest dividing the algorithm into several steps: (1) isoltaing the
                      pattern to use, (2) picking apart each pattern component, and (3) recursing as necessary.</p></td>
                  </tr>
                  <tr>
                    <td><p>7. C-GENS</p></td>
                    <td><p>Trivial once Problem 6 is solved.</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>Hope those tips help a bit!</p>
            </div>
            <hr/>
            
            
            <a class='btn btn-default pull-right hidden-print' href='javascript:window.print();'>
              <span class='glyphicon glyphicon-print'></span>
              &nbsp; PDF / Print
            </a>
            
          </div>
          <!-- END PRESENTATION CONTENT -->
          
          <!-- MATERIALS FROM CLASS: -->
          
            
        </div>
      </div>
      <!-- END MAIN CONTENT -->
      
      
    </div>
    <!-- END WRAPPER -->
    
    <!-- BEGIN FOOTER -->
    <div id="footer">
      <div class="container">
        <div class="col-md-12 text-center">
          
        </div>
      </div>
    </div>
    <!-- END FOOTER -->
    
  </body>
</html>
