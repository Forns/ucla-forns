
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

  <head>
    <title>Andrew Forney - UCLA CS</title>
    <link href="../../../css/bootstrap.min.css" rel="stylesheet" type="text/css">
    <link href="../../../css/magic-bootstrap.css" rel="stylesheet" type="text/css">
    <link href="../../../css/main.css" rel="stylesheet" type="text/css">
    <script src="../../../js/lib/jquery-2.0.3.min.js"></script>
    <script src="../../../js/lib/bootstrap.min.js"></script>
    <script src="../../../js/lib/expanding.js"></script>
    <script src="../../../js/display/general/general-display.js"></script>
    <script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js"></script>
    <script type="text/javascript" src="../../../js/display/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  </head>
  
  <body data-spy="scroll" data-target="#scrollspy">
    
    <!-- BEGIN WRAP -->
    <div id="wrap">
      
      <!-- BEGIN NAVIGATION -->
      <nav class='navbar navbar-default' role='navigation'>
        <div class='nav-accent'></div>
        <div class='container'>
          <div class='row'>
            <div class='col-md-12'>
              <div class='navbar-header'>
                <button class='navbar-toggle' type='button' data-toggle='collapse' data-target='.navbar-main-collapse'>
                  <span class='sr-only'>Toggle Navigation</span>
                  <span class='icon-bar'></span>
                  <span class='icon-bar'></span>
                  <span class='icon-bar'></span>
                </button>
                <a class='navbar-brand' href='/~forns/'>
                  <span id='brand-text'>
                    Andrew Forney
                  </span>
                </a>
              </div>
              
              <div id='nav-main' class='collapse navbar-collapse navbar-main-collapse'>
                <ul class='nav navbar-nav navbar-right'>
                  
                  <li>
                    <a href='/~forns/about.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-user'></span>
                      </div>
                      <p class='text-center'>About</p>
                    </a>
                  </li>
                  
                  <li class='active'>
                    <a href='/~forns/classes.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-book'></span>
                      </div>
                      <p class='text-center'>Classes</p>
                    </a>
                  </li>
                  
                  <li>
                    <a href='/~forns/contact.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-comment'></span>
                      </div>
                      <p class='text-center'>Contact</p>
                    </a>
                  </li>
                  
                  <li>
                    <a href='/~forns/publications.html'>
                      <div class='text-center'>
                        <span class='glyphicon glyphicon-file'></span>
                      </div>
                      <p class='text-center'>Publications</p>
                    </a>
                  </li>
                  
                </ul>
              </div>
            </div>
          </div>
        </div>
      </nav>
      <!-- END NAVIGATION -->
      
      <!-- MathJax CUSTOM DEFS -->
      <div class='hidden'>
        \(\def\independence{\perp\!\!\!\perp}\)
        \(\def\dependence{\perp\!\!\!\!\!/\!\!\!\!\!\perp}\)
      </div>
      
      <!-- BEGIN MAIN CONTENT -->
      <div id="main-content" class="container">
        <div class="row">
          
          <!-- BEGIN SCROLLSPY -->
          <div class='col-md-2 hidden-sm hidden-xs'>
            <div class="bs-sidebar hidden-print affix" role="complementary">
              <ul id='scrollspy' class="nav bs-sidenav">
              </ul>
            </div>
          </div>
          <!-- END SCROLLSPY -->
          
          
          <!-- BEGIN PRESENTATION CONTENT -->
          <div class='col-md-10 presentation-content' role='main'>
            
            <ol class="breadcrumb hidden-print">
              <li><a href="../../../classes.html">Classes</a></li>
              <li><a href="./cs-161.html">CS161</a></li>
              <li class="active">Week 8</li>
            </ol>
            
            
            <div id='multivariate' class='scrollspy-element' scrollspy-title='Multivariate Distributions'></div>
            <h1>Multivariate Distributions</h1>
            <div>
              <p>In the previous lecture, we dealt only with two variables, even though one of them was not binary.</p>
              <p>Let's take a look at a multi-variate distribution taking a (not large) step to three variables.</p>
              <p>For this example, let's just assume we have 3 binary variables X, Y, and Z, with a joint probability table that looks like:</p>
              <table class='table table-bordered table-striped'>
                <thead>
                  <tr>
                    <th><p>X</p></th>
                    <th><p>Y</p></th>
                    <th><p>Z</p></th>
                    <th><p>Pr(X, Y, Z)</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0.030</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0.120</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0.105</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0.245</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0.105</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0.045</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0.280</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0.070</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>Let's talk about a couple of observations we can make on multivariate distributions.</p>
              <p>Suppose we didn't want the entire joint distribution, but wanted to look at the co-occurences of only some subset of variables.</p>
              <p class='definition'>We can <strong>sum out (marginalize)</strong> a variable by &quot;collapsing&quot; the distribution on the remaining variables we're not summing out. Formally,
                for variables of interest \(\alpha\) and variables we're summing out \(\beta\) with \(\alpha \cap \beta = \varnothing\)<br/>
                $$P(\alpha) = \sum_{\beta_i \in \beta} P(\alpha, \beta_i)$$
              </p>
              <p class='definition'>The resulting distribution over variables \(\alpha\) that have not been summed out is called the <strong>joint marginal</strong> distribution over \(P(\alpha)\).</p>
              <br/>
              <p>So, when we marginalize, we get a new table, or distribution, whose rows are the sums of probability mass over the variables that were summed out.</p>
              <p class='question' name='marg-0'>
                Marginalize Z in the above distribution, leaving a joint marginal on X and Y. To do so, simply look for every row where X and Y *agree* and then sum over the possible values for Z.
              </p>
              <div class='answer white-bg' name='marg-0'>
                <p>Let's look at that in action; take a look at the following pairs of rows, color-coded for your convenience:</p>
                <table class='table table-bordered'>
                  <thead>
                    <tr>
                      <th><p>X</p></th>
                      <th><p>Y</p></th>
                      <th><p>Z</p></th>
                      <th><p>Pr(X, Y, Z)</p></th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr class='success'>
                      <td><p>0</p></td>
                      <td><p>0</p></td>
                      <td><p>0</p></td>
                      <td><p>0.030</p></td>
                    </tr>
                    <tr class='success'>
                      <td><p>0</p></td>
                      <td><p>0</p></td>
                      <td><p>1</p></td>
                      <td><p>0.120</p></td>
                    </tr>
                    <tr class='danger'>
                      <td><p>0</p></td>
                      <td><p>1</p></td>
                      <td><p>0</p></td>
                      <td><p>0.105</p></td>
                    </tr>
                    <tr class='danger'>
                      <td><p>0</p></td>
                      <td><p>1</p></td>
                      <td><p>1</p></td>
                      <td><p>0.245</p></td>
                    </tr>
                    <tr class='warning'>
                      <td><p>1</p></td>
                      <td><p>0</p></td>
                      <td><p>0</p></td>
                      <td><p>0.105</p></td>
                    </tr>
                    <tr class='warning'>
                      <td><p>1</p></td>
                      <td><p>0</p></td>
                      <td><p>1</p></td>
                      <td><p>0.045</p></td>
                    </tr>
                    <tr class='white-bg'>
                      <td><p>1</p></td>
                      <td><p>1</p></td>
                      <td><p>0</p></td>
                      <td><p>0.280</p></td>
                    </tr>
                    <tr class='white-bg'>
                      <td><p>1</p></td>
                      <td><p>1</p></td>
                      <td><p>1</p></td>
                      <td><p>0.070</p></td>
                    </tr>
                  </tbody>
                </table>
              </div>
              <br/>
              <p>(the individual colors don't mean anything but observe the corresponding rows)</p>
              <p>These corresponding rows are where X and Y agree (i.e., in both rows 1 and 4 Y has value y and X has value x)</p>
              <p>So, to sum out Z, we simply collapse across the two rows! It's as though X were removed entirely from the equation, just leaving us with a distribution over X and Y.</p>
              <br/>
              <p>This amounts to the following joint marginal on X and Y with Z summed out, written:<br/>
                $$\sum_Z Pr(X, Y, Z)$$
              </p>
              <table class='table table-bordered'>
                <thead>
                  <tr>
                    <th><p>X</p></th>
                    <th><p>Y</p></th>
                    <th><p>\(\sum_Z Pr(X, Y, Z) = Pr(X, Y)\)</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr class='success'>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0.15</p></td>
                  </tr>
                  <tr class='warning'>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0.35</p></td>
                  </tr>
                  <tr class='danger'>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0.15</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0.35</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>(Aside: that table looks really rasta)</p>
              <p>Any who, observe how we now have a joint distribution on Y and Z.</p>
              
              <br/>
              <h3>Independence</h3>
              <p>Let's turn our attention to independence relationships, remembering that our definition of independence was that:</p>
              <div class='well'>
                <p>If X is independent from Y, written</p>
                <p>$$X \independence Y$$</p>
                <p>...then:</p>
                <p>$$Pr(X | Y) = Pr(X), \forall x, y$$</p>
              </div>
              <br/>
              <p>So using our joint marginal over X and Y above, let's test them for independence.</p>
              <p>Repeating our joint marginal from before:</p>
              <table class='table table-bordered'>
                <thead>
                  <tr>
                    <th><p>X</p></th>
                    <th><p>Y</p></th>
                    <th><p>\(\sum_Z Pr(X, Y, Z) = Pr(X, Y)\)</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0.15</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0.35</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0.15</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0.35</p></td>
                  </tr>
                </tbody>
              </table>
              
              <p class='question' name='inde-0'>Using the joint marginal above, determine whether or not \(X \independence Y\). Click for solution.</p>
              <div class='answer' name='inde-0'>
                <p>Using Bayes' Conditioning, we remember that:</p>
                <p>$$Pr(X | Y) = \frac{Pr(X, Y)}{Pr(Y)}$$</p>
                
                <p>Using our marginal on X and Y, it is easy to see that:</p>
                <p>
                  \begin{eqnarray}
                    Pr(X = 0, Y = 0) &=& 0.15\\
                    Pr(X = 0, Y = 1) &=& 0.35\\
                    Pr(X = 0) &=& Pr(X = 1)\\
                              &=& 0.5
                  \end{eqnarray}
                </p>
                <p>
                  \begin{eqnarray}
                    Pr(X = 0, Y = 0) &=& 0.15\\
                    Pr(X = 1, Y = 0) &=& 0.15\\
                    Pr(Y = 0) &=& 0.30\\
                    Pr(Y = 1) &=& 0.70
                  \end{eqnarray}
                </p>
                <p>Now, to compute the conditional:</p>
                <p>
                  \begin{eqnarray}
                    Pr(X = 0 | Y = 0) &=& \frac{Pr(X = 0, Y = 0)}{Pr(Y = 0)}\\
                                      &=& \frac{0.15}{0.30}\\
                                      &=& 0.5\\
                                      &=& Pr(X = 0)
                  \end{eqnarray}
                </p>
                <p>We would also show this for the other conditional values of \(Pr(X | Y)\), but in short:</p>
                <p>
                  \begin{eqnarray}
                  Pr(X = 0 | Y = 0) &=& Pr(X = 0 | Y = 1)\\
                                    &=& Pr(X = 1 | Y = 0)\\
                                    &=& Pr(X = 1 | Y = 1)\\
                                    &=& Pr(X = 0)\\
                                    &=& Pr(X = 1)\\
                  \end{eqnarray}
                </p>
                <p>As such we conclude:<br/>$$X \independence Y$$</p>
              </div>
              <br/>
              
              <p>Neat! So this means that knowing something about X tells me nothing about the state of Y and vice versa...</p>
              <p>E.g., knowing that it's Tuesday tells me nothing about my chances of winning the lottery.</p>
              <br/>
              <p>If we were so inclined, we could repeat the process of summing out X from the original joint distribution to get:</p>
              <table class='table table-bordered'>
                <thead>
                  <tr>
                    <th><p>Y</p></th>
                    <th><p>Z</p></th>
                    <th><p>\(\sum_X Pr(X, Y, Z)\)</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0.135</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0.165</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0.385</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0.315</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>Performing the same test for independence, we would find that:</p>
              <div class='well'>
                <p>
                  \begin{eqnarray}
                    Pr(Y = 0 | Z = 0) &=& \frac{Pr(Y = 0, Z = 0)}{Pr(Z = 0)}\\
                                      &=& \frac{0.135}{0.52}\\
                                      &\approx& 0.260\\
                                      &\ne& Pr(Y = 0)\\
                                      
                  \therefore Y \dependence Z
                  \end{eqnarray}
                </p>
              </div>
              
              <br/>
              <h3>Conditional Independence</h3>
              <p>A topic we haven't talked about yet is a peculiar phenomenon known as conditional independence.</p>
              <p>As it turns out, it's possible for two variables X and Y to be independent ONLY after we've observed (conditioned upon) some other variable(s) Z.</p>
              <p>To compare:</p>
              <table class='table table-bordered table-striped'>
                <thead>
                  <tr>
                    <th><p>Relationship</p></th>
                    <th><p>Description</p></th>
                    <th><p>Written</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th><p>Independence</p></th>
                    <td><p>If we have knowledge that X occurred, and that tells us nothing about whether or not Y occured, then X is independent of Y and vice versa.</p></td>
                    <td><p>$$X \independence Y$$</p></td>
                  </tr>
                  <tr>
                    <th><p>Conditional Independence</p></th>
                    <td><p>X and Y are conditionally independent if and only if, given information about Z, having knowledge about X tells us nothing about whether or not Y occured; i.e., X and Y may
                      not be independent until *after* conditioning on a third variable / set of variables Z.</p></td>
                    <td class='col-md-2'><p>$$X \independence Y | Z$$</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>So you might be curious... what's an intuitive interpretation of conditional independence?</p>
              <p>Here are some good good scenarios that explain it, using dice, because every statistician loves dice for some reason:</p>
              <table class='table table-bordered table-striped'>
                <thead>
                  <tr>
                    <th><p>Concept</p></th>
                    <th><p>Description</p></th>
                    <th class='col-md-3'><p>Written</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><p>Independent</p></td>
                    <td><p>You roll two dice: A and B. Knowing the outcome of A tells you nothing about the outcome of B.</p></td>
                    <td><p>$$A \independence B$$</p></td>
                  </tr>
                  <tr>
                    <td><p>Independent, but Conditionally Dependent</p></td>
                    <td><p>You roll two dice: A and B. If I tell you that the sum (S) of the two dice's totals is even, then knowing the value of either A or B actually *does* tell me something about the other,
                      even though A and B are independent on their own.</p></td>
                    <td><p>$$A \dependence B | S$$</p></td>
                  </tr>
                  <tr>
                    <td><p>Independent, and Conditionally Independent</p></td>
                    <td><p>You roll two dice: A and B. If I tell you that the result (R) of A is not 3 and the result of B is not 2, I learn new information about each, but nothing that connects the
                      two outcomes. So, the dice rolls are independent AND conditionally independent based on the new information.</p></td>
                    <td><p>$$A \independence B$$<br/>$$A \independence B | R$$</p></td>
                  </tr>
                  <tr>
                    <td><p>Dependent, but Conditionally Independent</p></td>
                    <td><p>This one's a bit trickier and we'll need to develop a new toolkit to think about it, so let's hold off on it for now...</p></td>
                    <td><p>$$A \dependence B$$<br/>$$A \independence B | C$$</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p class='definition'>Two other, equivalent, ways to think about conditional independence are that:<br/>
                \begin{eqnarray}
                  Pr(X | Y, Z) = Pr(X | Z) &\Leftrightarrow& X \independence Y | Z\\
                  Pr(X, Y | Z) = Pr(X | Z) Pr(Y | Z) &\Leftrightarrow& X \independence Y | Z
                \end{eqnarray}
              </p>
              <br/>
              <p>So, let's take a look at a probability distribution that might elicit conditional independence relationships.</p>
              <p class='example'>Consider the following example relating three variables in a (fictitious) study on health effects of smoking.</p>
              <ul class='indent-1'>
                <li><p><strong>Smoking:</strong> whether or not an individual smokes</p></li>
                <li><p><strong>Cancer:</strong> whether or not an individual has cancer</p></li>
                <li><p><strong>Asthma:</strong> whether or not an individual has asthma</p></li>
              </ul>
              <br/>
              <p>Let's look at a made-up distribution:</p>
              <table class='table table-bordered table-striped'>
                <thead>
                  <tr>
                    <th><p>Asthma</p></th>
                    <th><p>Smoking</p></th>
                    <th><p>Cancer</p></th>
                    <th><p>Pr(Asthma, Smoking, Cancer)</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0.576</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0.144</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0.008</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0.072</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0.064</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0.016</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0.012</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0.108</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>We can see that the instances of smoking are usually indicative of both cancer and asthma.</p>
              <p>Now, we make the following observations:</p>
              <ol class='indent-1'>
                <li><p>We hypothesize that smoking causes cancer and asthma.</p></li>
                <li><p>If this is the case, then knowing that someone smokes immediately tells us that they are likely to develop cancer and asthma.</p></li>
                <li><p>Because of (1) and (2) above, we observe that IF we know someone smokes, then knowing that they have asthma tells us nothing more about them having cancer.</p></li>
              </ol>
              <br/>
              <p class='question' name='condind-q0'>What conditional independence relationship are the above observations making?</p>
              <p class='answer' name='condind-q0'>That cancer and asthma are independent once we know whether or not someone smokes!<br/>$$Asthma \independence Cancer | Smoking$$</p>
              <br/>
              <p class='question' name='condind-q1'>Rationalize: why is this a conditional independence relationship and not an absolute independence relationship?</p>
              <p class='answer' name='condind-q1'>Because *without* knowing whether or not someone smokes, we *do* know more about the probability of cancer given that someone has asthma, and vice versa;
                we say that <strong>information flows</strong> from one variable to the other without the conditioning on variable Smoking.</p>
              <br/>
              <p class='question' name='condind-q2'>Express this conditional independence in probability notation involving all three variables;
                how will we go about illustrating this independence relationship from the distribution?</p>
              <p class='answer' name='condind-q2'>Either of the two probability statements will suffice to illustrate this conditional independence relation:
                \begin{eqnarray}
                  Pr(Asthma | Cancer, Smoking) &=& Pr(Asthma | Smoking)\\
                  Pr(Cancer | Asthma, Smoking) &=& Pr(Cancer | Smoking)
                \end{eqnarray}
              </p>
              <br/>
              <p>Alright, now that we have our query in mind, let's observe the following:</p>
              <div class='well'>
                <p>Goal:</p>
                <p>$$Pr(Asthma | Cancer, Smoking) = Pr(Asthma | Smoking)$$</p>
                
                <p>We have neither of those tables, but we can compute them! Let's use Bayes' Conditioning and marginalization!</p>
                <p>
                  $$Pr(Asthma | Cancer, Smoking) = \frac{Pr(Asthma, Cancer, Smoking)}{Pr(Cancer, Smoking)}$$
                </p>
                <p>
                  $$Pr(Asthma | Smoking) = \frac{Pr(Asthma, Smoking)}{Pr(Smoking)}$$
                </p>
              </div>
              <br/>
              <p>Marginalizing from the joint distribution, we can get our two joint-marginals on \(Pr(Cancer, Smoking)\) and \(Pr(Asthma, Smoking)\):</p>
              <div class='row'>
                <div class='col-md-6'>
                  <table class='table table-bordered table-striped'>
                    <thead>
                      <tr>
                        <th><p>Smoking</p></th>
                        <th><p>Cancer</p></th>
                        <th><p>Pr(Smoking, Cancer)</p></th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td><p>0</p></td>
                        <td><p>0</p></td>
                        <td><p>0.64</p></td>
                      </tr>
                      <tr>
                        <td><p>0</p></td>
                        <td><p>1</p></td>
                        <td><p>0.16</p></td>
                      </tr>
                      <tr>
                        <td><p>1</p></td>
                        <td><p>0</p></td>
                        <td><p>0.02</p></td>
                      </tr>
                      <tr>
                        <td><p>1</p></td>
                        <td><p>1</p></td>
                        <td><p>0.18</p></td>
                      </tr>
                    </tbody>
                  </table>
                </div>
                <div class='col-md-6'>
                  <table class='table table-bordered table-striped'>
                    <thead>
                      <tr>
                        <th><p>Smoking</p></th>
                        <th><p>Asthma</p></th>
                        <th><p>Pr(Smoking, Asthma)</p></th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td><p>0</p></td>
                        <td><p>0</p></td>
                        <td><p>0.72</p></td>
                      </tr>
                      <tr>
                        <td><p>0</p></td>
                        <td><p>1</p></td>
                        <td><p>0.08</p></td>
                      </tr>
                      <tr>
                        <td><p>1</p></td>
                        <td><p>0</p></td>
                        <td><p>0.08</p></td>
                      </tr>
                      <tr>
                        <td><p>1</p></td>
                        <td><p>1</p></td>
                        <td><p>0.12</p></td>
                      </tr>
                    </tbody>
                  </table>
                </div>
              </div>
              <br/>
              <p>As a final ingredient for our proof of conditional independence, I'll save you the meager effort and tell you that:<br/>$$Pr(Smoking = 1) = 0.2$$</p>
              <p>Now, we just need to compute the two conditional distributions; we'll start with the Pr(Asthma | Cancer, Smoking) from the joint:<br/>
                $$Pr(Asthma | Cancer, Smoking) = \frac{Pr(Asthma, Cancer, Smoking)}{Pr(Cancer, Smoking)}$$
              </p>
              <table class='table table-bordered table-striped'>
                <thead>
                  <tr>
                    <th><p>Asthma</p></th>
                    <th><p>Smoking</p></th>
                    <th><p>Cancer</p></th>
                    <th><p>Pr(Asthma, Smoking, Cancer)</p></th>
                    <th><p>Pr(Asthma | Smoking, Cancer)</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0.576</p></td>
                    <td><p>0.9</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0.144</p></td>
                    <td><p>0.9</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0.008</p></td>
                    <td><p>0.4</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0.072</p></td>
                    <td><p>0.4</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0.064</p></td>
                    <td><p>0.1</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0.016</p></td>
                    <td><p>0.1</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0.012</p></td>
                    <td><p>0.6</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0.108</p></td>
                    <td><p>0.6</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>Interesting, looks like we have some flavor of uniformity going on here... hmmm... Let's compute Pr(Asthma | Smoking):<br/>
                $$Pr(Asthma | Smoking) = \frac{Pr(Asthma, Smoking)}{Pr(Smoking)}$$
              </p>
              <table class='table table-bordered table-striped'>
                <thead>
                  <tr>
                    <th><p>Smoking</p></th>
                    <th><p>Asthma</p></th>
                    <th><p>Pr(Asthma | Smoking)</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>0</p></td>
                    <td><p>0.9</p></td>
                  </tr>
                  <tr>
                    <td><p>0</p></td>
                    <td><p>1</p></td>
                    <td><p>0.1</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>0</p></td>
                    <td><p>0.4</p></td>
                  </tr>
                  <tr>
                    <td><p>1</p></td>
                    <td><p>1</p></td>
                    <td><p>0.6</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>Aha! We see that the two are in fact equivalent, regardless of what we know about whether or not a person has cancer! For brevity's sake, consider the positive variables below to be = 1, and 
                the negated ones to be = 0:</p>
              <div class='well'>
                <p>
                  \begin{eqnarray}
                    Pr(&not;Asthma | &not;Cancer, &not;Smoking) &=& Pr(&not;Asthma | Cancer, &not;Smoking)\\
                                                                &=& Pr(&not;Asthma | &not;Smoking)\\
                                                                &=& 0.9
                  \end{eqnarray}
                  
                  \begin{eqnarray}
                    Pr(&not;Asthma | &not;Cancer, Smoking) &=& Pr(&not;Asthma | Cancer, Smoking)\\
                                                           &=& Pr(&not;Asthma | Smoking)\\
                                                           &=& 0.4
                  \end{eqnarray}
                  
                  \begin{eqnarray}
                    Pr(Asthma | &not;Cancer, &not;Smoking) &=& Pr(Asthma | Cancer, &not;Smoking)\\
                                                           &=& Pr(Asthma | &not;Smoking)\\
                                                           &=& 0.1
                  \end{eqnarray}
                  
                  \begin{eqnarray}
                    Pr(Asthma | &not;Cancer, Smoking) &=& Pr(Asthma | Cancer, Smoking)\\
                                                      &=& Pr(Asthma | Smoking)\\
                                                      &=& 0.6
                  \end{eqnarray}
                  
                  $$\therefore Pr(Asthma | Cancer, Smoking) = Pr(Asthma | Smoking)$$
                  $$\therefore Asthma \independence Cancer | Smoking$$
                </p>
              </div>
              <br/>
              <p>Whew! That was a lot of work! But we'll see in a moment that this is all worth it...</p>
            </div>
            <hr/>
            
            
            <br/>
            <div id='bayesnet' class='scrollspy-element' scrollspy-title='Bayesian Networks'></div>
            <h1>Bayesian Networks</h1>
            <div>
              <p>Finally, we get to Bayesian Networks! I know you paid for your whole seat this discussion but you're only going to need the edge!</p>
              <p>To commemorate the occasion, I've created an image deserving of the day's grandeur that I've wanted to make for some time now.</p>
              <br/>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-8/bayeswatch.png' />
              </div>
              <br/>
              <p>That, of course, is a picture of Father Thomas Bayes, the one responsible for the eponymous theorem, photoshopped onto David Hasselhoff's body from the hit drama series Baywatch from 1989.</p>
              <p>I haven't had any means of working this joke into conversation, and the previous part of this lecture's been dull, so here we are...</p>
              <br/>
              <p>You awake again? OK...</p>
              <p>In the last section we derived that \(Asthma \independence Cancer | Smoking\) from our smoking example.</p>
              <p>Let's look at an interesting consequence:</p>
              <p class='question' name='bayesnet-q0'>Give a chain-rule factorization of the joint probability table: <code class='prettyprint'>Pr(Asthma, Cancer, Smoking)</code></p>
              <p class='answer' name='bayesnet-q0'>\(Pr(Asthma, Cancer, Smoking)\\= Pr(Asthma | Cancer, Smoking) * Pr(Cancer | Smoking) * Pr(Smoking)\)</p>
              <p class='question' name='bayes-q00'>Based on our conditional independence relation found from the previous section \((Asthma \independence Cancer | Smoking)\), can we reduce this to anything simpler?</p>
              <div class='answer' name='bayes-q00'>
                <p>Yes! Since based on our independence relation:<br/>$$Pr(Asthma | Cancer, Smoking) = Pr(Asthma | Smoking)$$</p>
                <p>So, our factorization reduces to:</p>
                <p class='text-center'>
                  \(
                    Pr(Asthma, Cancer, Smoking)\\
                    = Pr(Asthma | Cancer, Smoking) * Pr(Cancer | Smoking) * Pr(Smoking)\\
                    = Pr(Asthma | Smoking) * Pr(Cancer | Smoking) * Pr(Smoking)
                  \)
                </p>
              </div>
              <br/>
              
              <p>Hmm, interesting... so we took a big joint probability table (Pr(Asthma, Cancer, Smoking)) and broke it down into 3 smaller tables!</p>
              <p>There's something else interesting about our factorization...</p>
              <p>Remembering that we presumed that both Asthma and Cancer were indicators of Smoking:</p>
              
              <p class='question' name='bayesnet-q1'>Observation 1: Thinking about cause-effect relationships, what's interesting about the tables: Pr(Asthma | Smoking) and Pr(Cancer | Smoking)?</p>
              <p class='answer' name='bayesnet-q1'>They are tables structured according to a possible effect *given* that effect's possible causes!</p>
              <br/>
              <p>Alright, we're almost ready to hit the punchline... one final example.</p>
              <p class='example'>Suppose we have 5 binary variables: A, B, C, D, and E. Each of these variables are pairwise independent.</p>
              <p class='question' name='bayes-ex-0'>How many worlds are in the table / distribution:<br/>$$Pr(A, B, C, D, E)$$</p>
              <p class='answer' name='bayes-ex-0'>Since each variable is binary, there are:<br/>$$2^n = 2^5 = 32$$</p>
              <p class='question' name='bayes-ex-1'>Use the chain rule to factor this table. What is the resulting factorization?</p>
              <p class='answer' name='bayes-ex-1'>\(Pr(A, B, C, D, E)\\ = Pr(A | B, C, D, E) * Pr(B | C, D, E) * Pr(C | D, E) * Pr(D | E) * Pr(E)\\ = Pr(A) * Pr(B) * Pr(C) * Pr(D) * Pr(E)\)</p>
              <p class='question' name='bayes-ex-2'>How many worlds are there in each of these factored tables? How many rows total?</p>
              <p class='answer' name='bayes-ex-2'>Each new table is simply the prior on every variable, which are binary, so there are only 2 worlds per 5 table, for a total of 10 worlds.</p>
              <br/>
              
              <p class='definition'>Observation 2: The more independence relationships we're able to make on our distribution, the more compact we make the factored joint.</p>
              <p>There are 32 rows in the full joint, but only 10 rows in the individual, factored tables!</p>
              <p>Observations 1 and 2 lead us to the beauty of Bayesian networks.</p>
              <br/>
              
              <p class='definition'><strong>Bayesian Networks</strong> attempt to exploit independence relationships to reduce massive joint probability distributions to smaller tables, all while
                capturing the intuitive notions of cause and effect to structure the independences.</p>
              <br/>
              
              <p>In the words of the great Judea Pearl, who was instrumental in their development, Bayesian networks are, &quot;A parsimonious representation&quot; of the joint distribution.</p>
              <p>They're just really intuitive data structures!</p>
              <p>Here's a simple Bayesian network representing our smoking problem:</p>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/winter-2015/cs-161/week-8/bayesnet-0.PNG' />
              </div>
              <hr/>
              <br/>
              
              <h3>Bayesian Network Properties</h3>
              <p class='definition'>Bayesian networks belong to a class of graphs called <strong>directed, acyclic graphs (DAGs)</strong>, meaning that the edges between nodes are directed and they form 
                no cycles (derp).</p>
              <p class='definition'>The network's <strong>nodes</strong> represent the variables in our distributions.</p>
              <p class='definition'>The network's <strong>edges</strong> represent <strong>dependence</strong> relationships, i.e., two connected nodes are dependent upon one another.</p>
              <p class='definition'>Edges illustrate only a dependence; effects can have multiple causes and causes can have multiple effects, any one of which illustrates an influence that may be negative 
                OR positive (correlationally).</p>
              <p class='definition'>The <strong>edge directions</strong> are merely tools to represent the independence relationships, but are structured with potential causes pointing to potential effects.</p>
              <br/>
              <p>Why do we say &quot;potential causes&quot; and &quot;potential effects?&quot;</p>
              <p>Because our data is correlational!</p>
              <a href='http://gizmodo.com/5977989/internet-explorer-vs-murder-rate-will-be-your-favorite-chart-today' target='_blank'>
                <div class='fit-pres text-center'>
                  <img src='../../../assets/images/spring-2014/cs-161/week-8/correlation.jpg' />
                </div>
              </a>
              <br/>
              
              <p>We would need stronger tools to claim confidence in a true causal relation, but for the purposes of Bayesian networks, it is convenient and intuitive to draw arrows from causes to effects.</p>
              <p>The reason being the same as in our smoking example: if we know that someone has a Smoking (the cause), then knowing that they also have a Asthma doesn't tell us more about their chance
                of having a Cancer.</p>
              <p>That is, as soon as we know the state of the causes, we don't get any new information about the effects!</p>
              <p>Thus, we glean some notion of the <strong>semantics / meaning</strong> implicit within Bayesian networks:</p>
              <p class='definition'>The <strong>Markovian factorization</strong> of a Bayesian network says that we can use the chain rule to phrase the joint distribution as a product of "family" factors,
                composed of a node given its parents.</p>
              <div class='well'>
                <p>The joint distribution can be factored using independence relationships to:</p>
                <p>$$Pr(X_1, X_2, ..., X_n) = Pr(X_1 | Parents(X_1)) * Pr(X_2 | Parents(X_2)) * ...$$</p>
                
                <p>...which semantically represents the putative relationship between causes and effects:</p>
                <p class='text-center'>
                  \(
                    Pr(X_1 | Parents(X_1)) * Pr(X_2 | Parents(X_2)) * ...\\
                    = Pr(Effect_1 | Causes(Effect_1)) * (Effect_2 | Causes(Effect_2)) * ...\\
                    = \prod_{X_i \in Vars} Pr(X_i | Parents(X_i))
                  \)
                </p>
              </div>
              <br/>
              <p class='definition'>This factorization allows us to express the large joint distribution in terms of <strong>conditional probability tables (CPTs)</strong> of an effect given its parents.</p>
              <br/>
              <p>For our smoking example, the CPTs would look like:</p>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/winter-2015/cs-161/week-8/bayesnet-1.PNG' />
              </div>
              <br/>
              <p class='question' name='bayesnet-q2'>Using the above CPTs, compute: Pr(Smoking = 0, Asthma = 1, Cancer = 0)</p>
              <div class='answer' name='bayesnet-q2'>
                <p>
                  \(
                    Pr(Smoking = 0, Asthma = 1, Cancer = 0)\\
                    = Pr(Asthma = 1 | Smoking = 0) * Pr(Cancer = 0 | Smoking = 0) * Pr(Smoking = 0)\\
                    = 0.1 * 0.8 * 0.8\\
                    = 0.064
                  \)
                </p>
              </div>
              <br/>
              
              <p>Since our CPTs describe a world in which the effects can be screened off from other portions of the network by knowing about their causes, we say that Bayesian networks make the
                Markovian assumption:</p>
              <p class='definition'>The <strong>Markovian assumption</strong> states that every node is <strong>independent</strong> of its non-descendents <strong>given</strong> its parents, or formally,
                for node X, we write:
                $$X \independence NonDescendants(X) | Parents(X)$$
              </p>
              <p class='definition'><strong>Non-descendants</strong> of node X are any node that can NOT be reached by taking a directed path starting at X.</p>
              <br/>
              
              <p>You should repeat that a couple times... make it your mantra.</p>
              <p>The Markovian assumptions are intuitive because they convey the "screening off" of extraneous effects once we know all of the causes for a given variable.</p>
              <p class='example'>What are the Markovian assumptions implicit in the following Bayesian network?</p>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-8/bayesnet-2.PNG' />
              </div>
              <p class='question' name='bayesnet-q3'>Click for answer.</p>
              <div class='answer' name='bayesnet-q3'>
                <div class='well'>
                  <p>The Markovian assumptions implicit in the network are:</p>
                  <p>Earthquake \(\independence\) Burglary</p>
                  <br/>
                  
                  <p>Radio \(\independence\) Alarm | Earthquake</p>
                  <p>Radio \(\independence\) Call | Earthquake</p>
                  <p>Radio \(\independence\) Burglary | Earthquake</p>
                  <br/>
                  
                  <p>Burglary \(\independence\) Radio</p>
                  <br/>
                  
                  <p>Alarm \(\independence\) Radio | Earthquake, Burglary</p>
                  <br/>
                  
                  <p>Call \(\independence\) Radio | Alarm</p>
                  <p>Call \(\independence\) Earthquake | Alarm</p>
                  <p>Call \(\independence\) Burglary | Alarm</p>
                </div>
              </div>
              <br/>
              
              <p>Markovian assumptions are great and can give us some off-the-top independence relationships encoded by our networks...</p>
              <p>...however, they don't give us a clean way of asking arbitrary claims of independence between nodes given other nodes.</p>
              <p>To help us with such queries, we turn to the notion of d-separation.</p>
            </div>
            <hr/>
            
            
            <br/>
            <div id='dsep' class='scrollspy-element' scrollspy-title='D-Separation'></div>
            <h1>D-Separation</h1>
            <div>
              <p>Let's start off by considering 3 different simple Bayesian networks and observe how we can generalize their characteristics.</p>
              <p>Consider each of the following 3-node Bayesian networks as a plumbing network where:</p>
              <ol class='indent-1'>
                <li><p>Nodes are &quot;valves&quot; that allow water (information) to flow through</p></li>
                <li><p>Edges are &quot;pipes&quot; that connect the valves</p></li>
                <li><p>Dependence is the process of determining whether water (information) could flow from some set of valves (variables) X to some other set of valves Y, accounting for whether or not
                  any valves along any pipe-path are closed or not (our evidence, some set of variables Z)</p></li>
              </ol>
              <br/>
              
              <p>We'll start with a familar problem:</p>
              <p class='definition'>A valve Z is <strong>divergent / a fork</strong> along some path if it is a common cause of two effects, X and Y. Moreover, whenever we're given Z, X is independent
                from Y.<br/>
                $$X \leftarrow Z \rightarrow Y \Rightarrow X \independence Y | Z$$
              </p>
              <br/>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/winter-2015/cs-161/week-8/bayesnet-0.PNG' />
              </div>
              <p class='question' name='fork-q0'>Intuit: why does knowing whether or not someone Smokes make knowing whether or not they have Asthma tell us nothing more about their likelihood of having Cancer?</p>
              <p class='answer' name='fork-q0'>Because once we know the state of the single cause of Cancer (in this model, Smoking), knowing any of that cause's indicators doesn't tell us anything more than
                already knowing the state of the cause.</p>
              <br/>
              <p>Divergent nodes are sometimes referred to as common causes. Here, we see that if we know whether or not someone has a Smoking, then information does NOT flow from Asthma to Cancer.</p>
              <p>So, the rule for divergent valves along some path is that when we have a triplet X &larr; Z &rarr; Y, given Z blocks information flow from X to Y.</p>
              <hr/>
              <br/>
              
              <p class='definition'>A valve Z is <strong>sequential / a chain</strong> along some path if some other variable X is its cause and Z has some effect Y.<br/>
                $$X \rightarrow Z \rightarrow Y \Rightarrow X \independence Y | Z$$
                $$Y \leftarrow Z \leftarrow X \Rightarrow X \independence Y | Z$$</p>
              <br/>
              <p>Here's an example path of a chain:</p>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-8/bayesnet-3.PNG' />
              </div>
              <p class='question' name='chain-q0'>Intuit: why does knowing whether or not someone has Lung Tar make knowing whether or not they Smoke tell us nothing more about their likelihood of having Cancer?</p>
              <p class='answer' name='chain-q0'>Because once we know the state of the single cause of Cancer (in this model, Lung Tar), knowing any of that cause's causes doesn't tell us anything more than
                already knowing the state of the cause.</p>
              <br/>
              <p>Here, knowing that someone has tar in their lungs means that we no longer get information flowing from any knowledge that the person smoked to whether they'll have lung cancer.</p>
              <p>"If Lung Tar (the medical term, of course) is the true cause of Lung Cancer, then it's irrelevant how the tar got there in the first place to whether or not you have Lung Cancer."</p>
              <p>So, the rule for chain valves along some path is that for triple X &rarr; Z &rarr; Y, given Z blocks information flow from X to Y.</p>
              <hr/>
              <br/>
              
              <p class='definition'>A valve Z is <strong>convergent / a sink</strong> along some path if it is the common effect of two causes, X and Y.<br/>
                \begin{eqnarray}
                  X \rightarrow Z \leftarrow Y &\Rightarrow& X \dependence Y | Z\\
                                               &\Rightarrow& X \dependence Y | Descendants(Z)
                \end{eqnarray}
              </p>
              <br/>
              <p>Convergent nodes are a special beast, so let's look at the following scenario:</p>
              <blockquote>
                Consider the scenario where a bell rings if and only if the outcome of two coin flips (from two separate coins) are identical (i.e., both heads or both tails).
              </blockquote>
              <br/>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-8/bayesnet-4.PNG' />
              </div>
              <br/>
              <p class='question' name='bayesnet-q41'>If I know whether or not <strong>the bell rings</strong>, does information flow from knowing about coinflip 1 to coinflip 2?</p>
              <p class='answer' name='bayesnet-q41'>Yes! If I know the bell rang, then knowing that coinflip 1 came up heads tells me exactly that coinflip 2 must have been heads!</p>
              <p class='question' name='bayesnet-q42'>If I DO NOT know whether or not the bell rang, does information flow from knowing about coinflip 1 to coinflip 2?</p>
              <p class='answer' name='bayesnet-q42'>No! Just by knowing that coinflip 1 came up heads tells me nothing about coinflip 2.</p>
              <br/>
              <p>Now consider the following scenario:</p>
              <blockquote>
                A bell rings if and only if the outcome of two coin flips (from two separate coins) are identical (i.e., both heads or both tails). Additionally, if the bell rings, our elderly butler,
                Jeeves, has an 80% chance of bringing tea to our room.
              </blockquote>
              <br/>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-8/bayesnet-5.PNG' />
              </div>
              <br/>
              <p class='question' name='bayesnet-q5'>If I know whether or not <strong>Jeeves brings tea</strong>, does information flow from knowing about coinflip 1 to coinflip 2?</p>
              <p class='answer' name='bayesnet-q5'>Yes! If I know that Jeeves brought tea, then I know that the bell *probably* rang! As such, I gain information about coinflip 2 based on whatever coinflip 1 was.</p>
              <p class='question' name='bayesnet-q6'>If I DO NOT know whether or not the bell rang AND I DO NOT know whether Jeeves brought tea, does information flow from knowing about coinflip 1 to coinflip 2?</p>
              <p class='answer' name='bayesnet-q6'>No!</p>
              <br/>
              <p>So, the rule for convergent valves is the following: for common effect Z in configuration: X &rarr; Z &larr; Y, then Z is blocked if neither it NOR any of its descendants are given!</p>
              
              <br/>
              <h3>d-Separation</h3>
              <p>Now, we can think about these three cases of valves as being elements along a path in a Bayesian network.</p>
              <p class='definition'><strong>d-separation</strong> (directional separation) allows us to determine all independence relations implicit from the graph just by looking at its structure.</p>
              <p class='definition'>We use the following notation to pose d-separation queries and assert independence through d-separation between variables:<br/> 
                $$dsep(X, Z, Y) \Leftrightarrow X \independence Y | Z$$</p>
              <br/>
              <p>The rules of d-separation are as follows:</p>
<pre class='prettyprint'>
  ; To determine if some set of nodes X is
  ; d-separated from some set of nodes Y by
  ; some (possibly empty) set of nodes Z:
  
  trace ALL undirected paths from each X to each Y
      for each valve V on the current path
          if V is a fork and given (i.e., V&in;Z)
              then this path is blocked
          if V is a chain and given
              then this path is blocked
          if V is a sink and neither it NOR its
            descendents are given
              then this path is blocked
      
  if ALL paths blocked from each X to each Y given Z
      then X is d-separated from Y given Z
  else there was an open path
      then X is NOT d-separated from Y given Z
</pre>
              <br/>
              <p>Here's that algorithm at-a-glance:</p>
              <table class='table table-bordered table-striped'>
                <thead>
                  <tr>
                    <th><p>Valve Type</p></th>
                    <th><p>Example (Z is the valve)</p></th>
                    <th><p>Rule for being blocked</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th><p>Fork</p></th>
                    <td><p>X &larr; Z &rarr; Y</p></td>
                    <td><p>Path is blocked if Z is <strong>given</strong></p></td>
                  </tr>
                  <tr>
                    <th><p>Sequence</p></th>
                    <td><p>X &rarr; Z &rarr; Y</p></td>
                    <td><p>Path is blocked if Z is <strong>given</strong></p></td>
                  </tr>
                  <tr>
                    <th><p>Collider</p></th>
                    <td><p>X &rarr; Z &larr; Y</p></td>
                    <td><p>Path is blocked if <strong>NEITHER Z NOR ANY OF ITS DESCENDANTS</strong> are given</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>It might look complicated, but the fact that it's a strictly graphical criterion for independence makes it easy to trace.</p>
              <p>Let's do a bunch of examples:</p>
              <br/>
              <p class='example'>Use the following Bayesian network to determine if each variable set X is separated from Y given Z. If it is not, show the open path.</p>
              <div class='fit-pres text-center'>
                <img src='../../../assets/images/spring-2014/cs-161/week-8/bayesnet-6.PNG' />
              </div>
              <table class='table table-bordered table-striped'>
                <thead>
                  <tr>
                    <th><p>Query</p></th>
                    <th><p>Answer</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><p class='question' name='bayesnet-q7'>\(dsep( \{V\}, \{\}, \{T\} )\)</p></td>
                    <td><p class='answer' name='bayesnet-q7'>No! V is directly connected to T! The open path is simply: \(V \rightarrow T\)</p></td>
                  </tr>
                  <tr>
                    <td><p class='question' name='bayesnet-q8'>\(dsep( \{V\}, \{T\}, \{O\} )\)</p></td>
                    <td><p class='answer' name='bayesnet-q8'>Yes! The only path between V and O is: \(V \rightarrow T \rightarrow O\), in which T is a chain that is given, so that path is blocked.</p></td>
                  </tr>
                  <tr>
                    <td><p class='question' name='bayesnet-q9'>\(dsep( \{T\}, \{O\}, \{S\} )\)</p></td>
                    <td><p class='answer' name='bayesnet-q9'>No! There are two paths from T to S: one that goes through O to D (and is blocked), but the other which goes through O and up to C (which is open because
                      O is a sink and is given). So, the open path is: \(T \rightarrow O \leftarrow C \leftarrow S\)</p></td>
                  </tr>
                  <tr>
                    <td><p class='question' name='bayesnet-q10'>\(dsep( \{T\}, \{D\}, \{S\} )\)</p></td>
                    <td><p class='answer' name='bayesnet-q10'>No! In fact, now *both* paths from T to S are open. They are:<br/>\(T \rightarrow O \leftarrow C \leftarrow S\)
                      <br/>\(T \rightarrow O \rightarrow D \leftarrow B \leftarrow S\)</p></td>
                  </tr>
                  <tr>
                    <td><p class='question' name='bayesnet-q11'>\(dsep( \{T\}, \{\}, \{S\} )\)</p></td>
                    <td><p class='answer' name='bayesnet-q11'>Yes! Now all paths are blocked</p></td>
                  </tr>
                  <tr>
                    <td><p class='question' name='bayesnet-q12'>\(dsep( \{T, X\}, \{B\}, \{S\} )\)</p></td>
                    <td><p class='answer' name='bayesnet-q12'>No! A path is open from: \(X \leftarrow O \leftarrow C \leftarrow S\)</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>And now, one final, conceptual question:</p>
              <p class='question' name='bayesnet-q13'>Are the independence relationships given by the Markovian assumptions implied by d-separation? Is the reverse true?</p>
              <p class='answer' name='bayesnet-q13'>The Markovian independence relationships are implied by d-separation. Although technically the reverse is also true, the Markovian assumptions do not
                explicitly state all independence relationships.</p>
            </div>
            <hr/>
            
            
            <br/>
            <div id='inference' class='scrollspy-element' scrollspy-title='Inference'></div>
            <h1>Inference</h1>
            <div>
              <p>So now that we have our Bayesian networks defined and we know what independence relationships they claim... what do we do with them?</p>
              <p>Well, we can ask them questions of course!</p>
              <p>For our networks, those questions will be of the form: &quot;What's the probability of witnessing events Q given that I've seen evidence e?&quot; (where Q is a set of variables and e is an
                instantiation of evidence)</p>
              <p class='definition'><strong>Inference queries</strong>, for query sentence \(Q\) and evidence \(e\), compute the quantity \(Pr(Q | e)\) using the network CPTs.</p>
              <div class='toolkit'>
                <p>Every variable in the network can be classified under one of three categories while performing inference:</p>
                <ul class='indent-1'>
                  <li><p><strong>Query Variables (Q):</strong> variables in which we are interested in computing the probability mass.</p></li>
                  <li><p><strong>Evidence Variables (e):</strong> events that we have witnessed that will serve as our conditioned variables.</p></li>
                  <li><p><strong>"Hidden" Variables (V):</strong> variables in the network that are neither query nor evidence variables, but may still propagate information through the network. Unlike
                    evidence variables, we don't know the state of hidden variables, so we have to sum over all of their possible values.</p></li>
                </ul>
              </div>
              <br/>
              <p>A couple things to note about the inference problem:</p>
              <ul class='indent-1'>
                <li><p>This problem would be pretty easy if we had our joint distribution... but we don't with Bayesian networks! In fact, the whole point of Bayesian networks is to avoid
                  having to reconstruct our giant joint table. We just have CPTs.</p></li>
                <li><p>Accounting for evidence in terms of these CPTs is therefore less trivial because of the "information flow" we talked about in the last section, where conditioning on some evidence
                  might actually OPEN flow from one variable to another.</p></li>
                <li><p>We might have a lot of superfluous variables that don't contribute to our query and need a means of getting rid of them before beginning inference.</p></li>
              </ul>
              <br/>
              
              <p class='definition'><strong>Enumeration Inference</strong> is a strategy for computing an arbitrary Bayesian network query via a sum of products of CPT values from the network.</p>
              <p>Let's consider how we might compute an arbitrary query on a Bayesian network using Enumeration Inference.</p>
              <div class='well'>
                <p>Here's what we want:</p>
                <p>$$Pr(Q|e)$$</p>
                <p>Now, we don't have (nor want to recreate) the joint distribution, but we do have a set of our network's conditional probability tables.</p>
                <p>So, using Bayes' conditioning, we can phrase any query as:</p>
                <p>$$Pr(Q|e) = \alpha \sum_{v \in V} Pr(Q, e, v)$$</p>
                <p>Above, &alpha; is simply a normalizing constant that is equivalent to 1 / Pr(e).</p>
                <p>Finally, we know that the joint that we're summing over can be factorized into the Markovian family factors for all nodes X:</p>
                <p>$$Pr(Q, e, V) = \Pi_{x \in X} Pr(x | Parents(x))$$</p>
              </div>
              <br/>
              
              <p class='example'>Use the following Bayesian network and its CPTs to compute:<br/>$$Pr(Cancer = 1 | Smoking = 1)$$</p>
              <div class="fit-pres text-center">
                <img src="../../../assets/images/spring-2014/cs-161/week-8/bayesnet-7.PNG">
              </div>
              <br/>
              <p class='question' name='inf-q0'>What are our query, evidence, and hidden variables above?</p>
              <p class='answer' name='inf-q0'>Query: C = 1<br/>Evidence: A = 1<br/>Hidden: B</p>
              <p class='question' name='inf-q1'>Provide the query statement in terms of a sum of products over our network CPTs.</p>
              <p class='answer' name='inf-q1'>
                \begin{eqnarray}
                  Pr(C = 1 | A = 1) &=& \alpha * \sum_{b \in B} Pr(C = 1, A = 1, b)\\
                                    &=& \alpha * \sum_{b \in B} Pr(A = 1) * Pr(b | A = 1) * Pr(C = 1 | b)
                \end{eqnarray}
              </p>
              <br/>
              
              <p>Now, notice that above expression (the answer to the question above) says to "Sum over all of the possible values of B and multiply the family factors."</p>
              <p>This works, but we could improve it slightly, because the first factor Pr(A = 1) does not rely on the summation over B (see how B isn't mentioned in Pr(A = 1)?).</p>
              <p>So, we can simply move it outside of the summation to simplify, giving us:</p>
              <div class='well'>
                <p>
                \begin{eqnarray}
                  Pr(C = 1 | A = 1) &=& \alpha * Pr(A = 1) \sum_{b \in B} Pr(b | A = 1) * Pr(C = 1 | b)\\
                                    &=& \alpha * Pr(A = 1) [Pr(B = 0 | A = 1) * Pr(C = 1 | B = 0) + Pr(B = 1 | A = 1) * Pr(C = 1 | B = 1)]\\
                                    &=& \alpha * 0.6 * [0.1 * 0.5 + 0.9 * 0.3]\\
                                    &=& \alpha * 0.192
                \end{eqnarray}
                </p>
              </div>
              <br/>
              
              <p>So close to finishing our computation! Now we just need to find a value for $$\alpha = 1 / Pr(e) = 1 / Pr(A = 1)$$</p>
              <p>Easy enough! We happen to know this from our CPT for A. This gives us:</p>
              <div class='well'>
                \begin{eqnarray}
                  Pr(C = 1 | A = 1) &=& \alpha * 0.192\\
                                    &=& \frac{0.192}{Pr(A = 1)}\\
                                    &=& \frac{0.192}{0.6}\\
                                    &=& 0.32
                \end{eqnarray}
              </div>
              <br/>
              
              <p>And there you have it! So in summary, the steps of enumeration inference are:</p>
              <ol class='indent-1'>
                <li><p>Identify your query (Q), evidence (e), and hidden variables (V) amongst all variables in the network (X).</p></li>
                <li><p>Formulate your query expression in terms of the network CPTs, by the formula:<br/>$$Pr(Q|e) = \alpha \sum_{v \in V} Pr(Q, e, v)$$</p></li>
                <li><p>[Optional] (But time saving) Re-arrange summations and factors to reduce number of multiplications (like when we pulled out Pr(A) above)</p></li>
                <li><p>Solve for, and substitute into (2) above: $$\alpha = 1 / Pr(e)$$</p></li>
              </ol>
              <hr/>
              <br/>
              
              <h3>Additional Practice</h3>
              <p class='example'>Use the following Bayesian network to answer the queries that follow. Assume that each CPT denotes 0 as an absence of the variable and 1 as the presence of it (e.g. H = 1 means that you have hypertension). Use Enumeration Inference as described above.</p>
              <div class="fit-pres text-center">
                <img src="../../../assets/images/winter-2016/cs-161/b-net-ex-2.PNG">
              </div>
              <br/>
              <p class='question' name='inf-prac-q0'>What is the probability that an individual has hypertension given that they exercise? [Click for solution]</p>
              <div class='answer' name='inf-prac-q0'>
                <p>
                \begin{eqnarray}
                  Pr(H = 1 | E = 1) &=& \alpha * \sum_{w, d} Pr(E = 1) * Pr(d) * Pr(w | E = 1, d) * Pr(H = 1 | w, E = 1)\\
                                    &=& \alpha * Pr(E = 1) * \sum_{w, d} Pr(d) * Pr(w | E = 1, d) * Pr(H = 1 | w, E = 1)\\
                                    &=& \alpha * Pr(E = 1) * \sum_{d} Pr(d) * \sum_{w} Pr(w | E = 1, d) * Pr(H = 1 | w, E = 1)\\
                                    &=& \alpha * Pr(E = 1)\\
                                    &*& [Pr(D = 0) * [Pr(W = 0 | E = 1, D = 0) * Pr(H = 1 | W = 0, E = 1)\\
                                    &+& Pr(W = 1 | E = 1, D = 0) * Pr(H = 1 | W = 1, E = 1)]\\
                                    &+& Pr(D = 1) * [Pr(W = 0 | E = 1, D = 1) * Pr(H = 1 | W = 0, E = 1)\\
                                    &+& Pr(W = 1 | E = 1, D = 1) * Pr(H = 1 | W = 1, E = 1)]]\\
                                    &=& \alpha * 0.6 * [0.3 * [0.7 * 0.4 + 0.3 * 0.1] + 0.7 * [0.2 * 0.4 + 0.8 * 0.1]]\\
                                    &=& \alpha * 0.123
                \end{eqnarray}
                </p>
                <p>Whew! Almost there, now we just have to find $$\alpha = 1 / Pr(E = 1) = 1 / 0.6$$</p>
                <p>$$\therefore Pr(H = 1 | E = 1) = 0.123 / 0.6 = 0.205$$</p>
                <p>So, the chance that you have hypertension given that you exercise is about 20%. Now go do some pushups!</p>
              </div>
            </div>
            <hr/>
            
            
            <br/>
            <div id='homework4' class='scrollspy-element' scrollspy-title='Homework 4'></div>
            <h1>Homework 4</h1>
            <div>
              <p>Your last grueling exercise set! Let's go over it now and look at some hints...</p>
              <table class='table table-striped table-bordered'>
                <thead>
                  <tr>
                    <th><p>Problem</p></th>
                    <th><p>Tip</p></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><p>1. FR-TO-ENG</p></td>
                    <td><p>Deceptively intricate, but not impractical; consider breaking your solution down into several helper functions:</p>
                      <ul class='indent-1'>
                        <li><p>One that finds the pattern associated with the current top-level predicate.</p></li>
                        <li><p>One that performs the translation of the found pattern(s).</p></li>
                        <li><p>One that glues the replacement-patterns of a pattern with a decision tree together.</p></li>
                      </ul>
                    </td>
                  </tr>
                  <tr>
                    <td><p>2. EVAL-D-TREE</p></td>
                    <td><p>Trivial. Big hint: remember what the return value is for Clisp's logical AND!</p></td>
                  </tr>
                </tbody>
              </table>
              <br/>
              <p>Hope those tips help a bit! I think you'll find this assignment a vacation compared to the previous homeworks.</p>
            </div>
            <hr/>
            
            
            <a class='btn btn-default pull-right hidden-print' href='javascript:window.print();'>
              <span class='glyphicon glyphicon-print'></span>
              &nbsp; PDF / Print
            </a>
            
          </div>
          <!-- END PRESENTATION CONTENT -->
          
          <!-- MATERIALS FROM CLASS: -->
          
            
        </div>
      </div>
      <!-- END MAIN CONTENT -->
      
      
    </div>
    <!-- END WRAPPER -->
    
    <!-- BEGIN FOOTER -->
    <div id="footer">
      <div class="container">
        <div class="col-md-12 text-center">
          
        </div>
      </div>
    </div>
    <!-- END FOOTER -->
    
  </body>
</html>
